# Data {#data}

*This chapter is a draft.*


## Introduction

Start by loading the packages which we will need in this chapter.

```{r, message = FALSE}
library(tidyverse)
library(primer.data)
library(lubridate)
library(janitor)
library(skimr)
library(nycflights13)
library(gapminder)
library(fivethirtyeight)
```


## Absolute and relative file paths

When you load a data set into R, you first need to tell R where those files live. The file could live on your computer (*local*) or somewhere on the internet (*remote*). In this section, we will discuss the case where the file lives on your computer.

The place where the file lives on your computer is called the "path". You can think of the path as directions to the file. There are two kinds of paths: relative paths and absolute paths. A relative path is where the file is with respect to where you currently are on the computer (e.g., where the Jupyter notebook file that you're working in is). On the other hand, an absolute path is where the file is in respect to the base (or root) folder of the computer's filesystem.

Suppose our computer's filesystem looks like the picture below, and we are working in the Jupyter notebook titled `worksheetk_02.ipynb`. If we want to  read in the `.csv` file named `happiness_report.csv` into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show both choices below.

```{r, echo =FALSE, fig.cap = "Example file system"}
knitr::include_graphics("03-data/images/file-system-for-export-to-intro-datascience.svg")
```

**Reading `happiness_report.csv` using a relative path:**

```
happiness_data <- read_csv("data/happiness_report.csv")
```

**Reading `happiness_report.csv` using an absolute path:**

```
happiness_data <- read_csv("/home/jupyter/dsci-100/worksheet_02/data/happiness_report.csv")
```

So which one should you use? Generally speaking, to ensure your code can be run 
on a different computer, you should use relative paths. An added bonus is that 
it's also less typing! This is because the absolute path of a file (the names of 
folders between the computer's root `/` and the file) isn't usually the same 
across different computers. For example, suppose Fatima and Jayden are working on a 
project together on the `happiness_report.csv` data. Fatima's file is stored at 

`/home/Fatima/project/data/happiness_report.csv`, 

while Jayden's is stored at 

`/home/Jayden/project/data/happiness_report.csv`.
 
Even though Fatima and Jayden stored their files in the same place on their computers (in their home folders), the absolute paths are different due to their different usernames.
If Jayden has code that loads the `happiness_report.csv` data using an absolute path, the code won't work on Fatima's computer.
But the relative path from inside the `project` folder (`data/happiness_report.csv`) is the same on both computers; any code that uses relative paths will work on both!

<!--Why would the absolute path not work on a different computer? The reason for this is that the names and the folder structure of the path between a computer's root 
folder (named `/`) and any files and folders you are usually working with will be different depending on who owns the computer (usually there is a different user name 
on each computer) and where the files and folders happen to be located on that particular computer. -->

See this video for another explanation: 

<iframe width="840" height="473" src="https://www.youtube.com/embed/ephId3mYu9o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

*Source: [Udacity course "Linux Command Line Basics"](https://www.udacity.com/course/linux-command-line-basics--ud595)*

## Writing and Reading Files

Getting data into and out of R is a major part of any real world data science project. 


### CSV files

"CSV" stands for **c**omma **s**eparated **v**alue. In other words, csv files are files whose values are separated by commas. Each comma from the csv file corresponds to a column, and the column names are taken from the first line of the file. The function then "guesses" an appropriate data type for each of the columns.

In the real word, you often want to write a data frame that has changed (either through filtering, selecting, mutating or summarizing) to a file to share it with others. We use `write_csv()` to save a data frame to a .csv file. `write_csv()` has two main arguments: `x` and `path`. The `x` argument is the data set that you want to save. The `file` argument is the file path where you want to save the file. The end of the path argument is the name that you want to use for the file. 

<!-- Does there need to be an example here? -->

We use `read_csv()` from the **readr** package to load the data into R, and in that call we specify the relative path to the file

Consider the following csv file "test_2.csv". 

```{r}
file_1 <- "https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/test_2.csv"

read_csv(file_1)
```

As you can see, there is text at the top of this file. Often times information about how data was collected, or other relevant information, is included at the top of the data file. This does not allow us to correctly load the data into R, and it is not intended to be shown.

We can use the `skip` argument to skip the text lines.

```{r}
read_csv(file = file_1,
         skip = 2)
```

Do you see the column specification message? As of now, R "guesses" an appropriate data type for each of the columns. To get rid of this message, we use the `col_types()` argument and specify the data types.

```{r}
read_csv(file = file_1,
         skip = 1,
         col_types = cols(a = col_double(),
                          b = col_double(),
                          c = col_double()))

```


When our tabular data comes in a different format, we can use the `read_delim()` function instead. For example, a different version of "test_2.csv" could exist that has no column names and uses tabs as the delimiter instead of commas.

With `read_delim()`, we specify the first argument as the path to the file (as done with read_csv). Then we provide values to the `delim` argument (could be a tab, which we represent by "\t").

### Excel Files

Excel is a spreadsheet program that use tables to analyze, store, or manipulate data. The tables are composed of cells which include text, numbers, or formulas .Excel files have the filename extension .xlsx. They store additional things that you cannot store in a .csv file such as fonts, text formatting, graphics, etc.

In order to write excel files you have to install complex packages, and they are hard to create. Writing excel files is beyond the scope of Primer. 

Reading Excel files is easy. To do so, we use the `read_excel()` function from the **readxl** package.


```{r}
library(readxl)

# Unfortunately, it is not possible to read Excel files directly from the web.
# So we download the file by hand and then read it in from the current working
# directory. Note that the "proper" way of handling this would be to create a
# temp directory with tempdir(), download the file into that directory, read it,
# and then delete the temp directory. That way, you would not have random
# downloaed files hanging around.

download.file(url = "https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/excel_1.xlsx", 
              destfile = "example_excel.xlsx")


read_excel(path = "example_excel.xlsx")
```

```{r, include = FALSE}
# Just doing this to clean up the downlaoded file so that Git does not ask me
# about it each time I build the book. Maybe we should be transparent and
# include this code above, like we did with download.file(). (Or maybe we should
# hide download.file() as well!) But this seems OK for now.

file.remove("example_excel.xlsx")
```



If the .xlsx file has multiple sheets, you have to use the sheet argument to specify the sheet number or name. 


### RDS

rds files store a single R object within a file. 

When we save to a rds file, we use the function `write_rds()`. Just like `write_csv()`, this function has two main arguments: `x` and `file`.  The `x` argument is the data set that you want to save. The `file` argument is the file path where you want to save the file. The end of the path argument is the name that you want to use for the file. 

`read_rds()` reads the file back into R. Just like `read_csv()` `read_rds` has one main argument, which is the path to the file that you are wanting to read into R.

```{r}
# need example here
```


### JSON

An increasingly common format for sharing data is **J**avaScript **O**bject **N**otation or JSON. Because this format is very general, it is nothing like a spreadsheet. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and from which you can obtain data. 

The functions `fromJSON()` and `toJSON()` allow you to convert between R objects and JSON. Both functions come from the **jsonlite** package. 

The function `toJSON()` converts a tibble to JSON format.

```{r, echo = FALSE}
library(jsonlite)

example_1 <- tibble(name= c("Miguel", "Sofia", "Aya", "Cheng"), student_id = 1:4, exam_1 = c(85, 94, 87, 90), exam_2 = c(86, 93, 88, 91))

toJSON(example_1, pretty = TRUE) 

# The pretty argument adds indentation and whitespace when TRUE. 
```

The function `fromJSON()` converts JSON format to a tibble.

```{r, echo = FALSE}
library(jsonlite)

json_format_ex <-
'[
  {"Name" : "Mario", "Age" : 32, "Occupation" : "Plumber"}, 
  {"Name" : "Peach", "Age" : 21, "Occupation" : "Princess"},
  {},
  {"Name" : "Bowser", "Occupation" : "Koopa"}
]'

fromJSON(json_format_ex) 
```


## Reading data from a database

Another very common form of data storage is the relational database. There are many relational database management systems, such as
[SQLite](https://www.sqlite.org/index.html), [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/), [Oracle](https://www.oracle.com/ca-en/index.html), and many more. These different relational database management systems each have their own advantages and limitations. Almost all employ SQL (*structured query language*) to pull data from the database. Thankfully, you don't need to know SQL
to analyze data from a database; 
several packages have been written 
that allows R to connect to relational databases and use the R programming language as the front end (what the user types in) to pull data from them. These different relational database management systems have their own advantages, limitations, and excels in particular scenarios. In this book, we will 
give examples of how to do this using R with SQLite and PostgreSQL databases.

### Connecting to a database

#### Reading data from a SQLite database

SQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer. Data is usually stored in a file with a `.db` extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor. 

The first thing you need to do to read data into R from a database is to connect to the database. We do that using the `dbConnect` function from the `DBI` (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel.

```{r}
library(DBI)
con_lang_data <- dbConnect(RSQLite::SQLite(), "03-data/data/can_lang.db")
```

Often relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database, you need to know the table names. You can get the names of all the tables in the database using the `dbListTables` function:

```{r}
tables <- dbListTables(con_lang_data)
tables
```

We only get one table name returned from calling `dbListTables`, which tells us that there is only one table in this database. To reference a table in the database to do things like select columns and filter rows, we use the `tbl` function from the `dbplyr` package. The package `dbplyr` allows us to work with data stored in databases as if they were local data frames, which is useful because we can do a lot with big datasets without actually having to bring these vast amounts of data into your computer! 

```{r}
library(dbplyr)
lang_db <- tbl(con_lang_data, "lang")
lang_db
```

Although it looks like we just got a data frame from the database, we didn't! It's a *reference*, showing us data that is still in the SQLite database (note the first two lines of the output). 
It does this because databases are often more efficient at selecting, filtering and joining large data sets than R. And typically, the database will not even be 
stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell 
it to do so using the `collect` function from the `dbplyr` package. 

Here we will filter for only rows in the Aboriginal languages category according to the 2016 Canada Census, and then use `collect` to finally bring this data into R as a data frame. 

```{r}
aboriginal_lang_db <- filter(lang_db, category == "Aboriginal languages")
aboriginal_lang_db
```

```{r}
aboriginal_lang_data <- collect(aboriginal_lang_db)
aboriginal_lang_data
```

Why bother to use the `collect` function? The data looks pretty similar in both outputs shown above. And `dbplyr` provides lots of functions similar to `filter` that 
you can use to directly feed the database reference (what `tbl` gives you) into downstream analysis functions (e.g., `ggplot2` for data visualization and `lm` for 
linear regression modeling). However, this does not 
work in *every* case; look what happens when we try to use `nrow` to count rows in a data frame:

```{r}
nrow(aboriginal_lang_db)
```
 
or `tail` to preview the last 6 rows of a data frame:

```
tail(aboriginal_lang_db)
```
```
## Error: tail() is not supported by sql sources
```
Additionally, some operations will not work to extract columns or single values from the reference given by the `tbl` function. Thus, once you have finished your data wrangling of the `tbl` database reference object, it is advisable to bring it into your local machine's memory using `collect` as a data frame. 

> Warning: Usually, databases are very big! Reading the object into your local machine may give an error or take a lot of time to run so be careful if you plan to do this! 
 

## Webscraping

Web scraping, or web harvesting, is the process of extracting data from a website. The information used by a browser to render web pages is received as a text file from a server. The text is code written in **H**yper **T**ext **M**arkup **L**anguage (HTML). HTML is the standard language behind almost every document on the Internet. It provides the structure of the page such as images, text, and hyperlinks.

Consider the following webpage that contains interesting data about murders in the US in [this Wikipedia page](https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state): 


```{r, echo = FALSE}
knitr::include_graphics("03-data/images/murders-data-wiki-page.png")
```

Every browser has a way to show the html source code for a page. Here is the HTML for the webpage shown above. 

```{r, echo = FALSE}
knitr::include_graphics("03-data/images/html-code.png")
```

Here are a few lines of code from the Wikipedia page that provides the US murders data:

```
<table class="wikitable sortable">
<tr>
<th>State</th>
<th><a href="/wiki/List_of_U.S._states_and_territories_by_population" 
title="List of U.S. states and territories by population">Population</a><br />
<small>(total inhabitants)</small><br />
<small>(2015)</small> <sup id="cite_ref-1" class="reference">
<a href="#cite_note-1">[1]</a></sup></th>
<th>Murders and Nonnegligent
<p>Manslaughter<br />
<small>(total deaths)</small><br />
<small>(2015)</small> <sup id="cite_ref-2" class="reference">
<a href="#cite_note-2">[2]</a></sup></p>
</th>
<th>Murder and Nonnegligent
<p>Manslaughter Rate<br />
<small>(per 100,000 inhabitants)</small><br />
<small>(2015)</small></p>
</th>
</tr>
<tr>
<td><a href="/wiki/Alabama" title="Alabama">Alabama</a></td>
<td>4,853,875</td>
<td>348</td>
<td>7.2</td>
</tr>
<tr>
<td><a href="/wiki/Alaska" title="Alaska">Alaska</a></td>
<td>737,709</td>
<td>59</td>
<td>8.0</td>
</tr>
<tr>
```


<!-- ### The rvest package -->

<!-- **rvest**, maintained by [the team at RStudio](https://rvest.tidyverse.org/), is the best package for getting data from the web. Although there are a bewildering array of R packages for every task, you should always start with one maintained by high quality people/organizations. [RStudio maintained packages](https://www.tidyverse.org/packages/) are always good. A new version, 1.0, of **rvest** is imminent. Upgrade once it comes out.  -->

<!-- ```{r, message=FALSE, warning=FALSE} -->
<!-- library(rvest) -->



<!-- h <- read_html(url) -->
<!-- ``` -->


<!-- Note that the entire Murders in the US Wikipedia webpage is now contained in `h`. The class of this object is: -->

<!-- ```{r} -->
<!-- class(h) -->
<!-- ``` -->

<!-- The **rvest** package is actually more general; it handles XML documents. XML is a general markup language used to represent any kind of data. HTML is a specific type of XML developed for representing webpages. Here we focus on HTML documents. -->

<!-- Now, how do we extract the table from the object `h`? If we print `h`, we don't really see much: -->

<!-- ```{r} -->
<!-- h -->
<!-- ``` -->

<!-- We can see all the code that defines the downloaded webpage using the `html_text` function like this: -->

<!-- ```{r, eval=FALSE} -->
<!-- html_text(h) -->
<!-- ``` -->

<!-- We don't show the output here because it includes thousands of characters, but if we look at it, we can see that the data is stored in an HTML table -- note the line of the HTML code above `<table class="wikitable sortable">`. The different parts of an HTML document, often defined with a message in between  `<` and `>`  are referred to as nodes. The **rvest** package includes functions to extract nodes of an HTML document: `html_nodes` extracts all nodes of different types and `html_node` extracts the first one. To extract the tables from the html code we use: -->

<!-- ```{r}  -->
<!-- tab <- h %>% html_nodes("table") -->
<!-- ``` -->

<!-- Now, instead of the entire webpage, we just have the html code for the tables in the page: -->

<!-- ```{r} -->
<!-- tab -->
<!-- ``` -->

<!-- The table we are interested is the first one: -->

<!-- ```{r} -->
<!-- tab[[1]] -->
<!-- ``` -->

<!-- This is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, **rvest** includes a function just for converting HTML tables into data frames: -->


<!-- ```{r} -->
<!-- tab <- tab[[1]] %>% html_table -->
<!-- class(tab) -->
<!-- ``` -->

<!-- We are now much closer to having a usable data table: -->

<!-- ```{r} -->
<!-- tab <- tab %>% setNames(c("state", "population",  -->
<!--                           "total", "murder_rate"))  -->

<!-- head(tab) -->
<!-- ``` -->

<!-- We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites. -->


### CSS selectors

The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is `table`, but there are many, many more. 

If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the `html_nodes` function. However, knowing which selector can be quite complicated. 
In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.

SelectorGadget^[http://selectorgadget.com/] is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including **rvest** author Hadley Wickham's
vignette^[https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html] and other tutorials based on the vignette^[https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/] ^[https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/].

 

## Working with APIs

## Working with Databases


## Summary
