---
title: "The Four Cardinal Virtues for Bayesian Data Science"
author: "David Kane"
output: html_document
---


## Courage




### Show the Model

First, we can just print it. Chapter 7 walks the reader through all the parts of a stan_glm() model in detail. Later chapters will also show the printed model, but can move more quickly. Second, we create a summary table of the model using **gtsummary**. Note that this is just a different view of the same model. We don't show some things --- like sigma --- that we did show when just printing. We do show other things, like the 95% confidence interval which we did not show before. Neither is better! We use the one which is most helpful to our audience. Third, we use **tidybayes** to show histograms of the posterior distributions. The posterior is the underlying reality, the closest to the "truth" which we are going to get. The printed and table outputs are just summaries of the posterior. We might not show all three things every time, but we certainly always show the posterior. Graphics are pretty!

What is the best way to create these graphics? Do we just use tidybayes? There are built-in plots with stan_glm() objects which give an ugly version of the posterior distributions for the coefficients, but not for sigma. 

A parameter is something which does not exist in the real world. (If it did, or could, then it would be data.) Instead, a parameter is a mental abstraction, a building block which we will use to to help us accomplish our true goal: To replace at least some of the questions marks in the Preceptor Table. But, since parameters are mental abstractions, we will always be uncertain as to their value, however much data we might collect.



### Residuals


$$outcome = fitted\ value + unmodeled\ variation$$

After noting this formula, each example should create a plot with three histograms in a row --- left-to-right, the outcome (i.e., a histogram or density of Y), the fitted values (which is sometimes a spike, sometimes two spikes and so on) and, finally, the residuals. This highlights how we have *decomposed* the outcome into two parts: the model and the unmodeled variation. This belongs in the Courage chapter because it is a way of understanding the model we have made.




## Temperance

We need a "machine" which generates these predictions, which is the same thing as a machine which fills in all the question marks in the Actual Preceptor Table, which is the same thing as a machine which produces "fake data" which looks a lot like our actual data.  


Recall the Actual Preceptor Table with all those question marks. Now we have a tool for filling in those question marks. Then we have the ability to answer the questions which we started the chapter.

This also leads directly to the concept of *posterior predictive checks*, which is just fancy terminology for helping to see if your model makes sense. If your model is reasonable, then you would expect to see Z (a feature of the real data) in either new data or in fake data generated by your model. If you see Z, then you should have more faith in your model. If you don't, then something is wrong. In what chapter should we start discussing this?

We have a model which we have built and understood. Now we get to use it! This almost always involves `posterior_predict()` and/or `posterior_epred()`. (I am hazy about the differences.)




### Realism










