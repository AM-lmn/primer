[{"path":"index.html","id":"section","chapter":"","heading":"","text":"","code":""},{"path":"preamble.html","id":"preamble","chapter":"Preamble","heading":"Preamble","text":"","code":""},{"path":"preamble.html","id":"dedication","chapter":"Preamble","heading":"Dedication","text":"romantic, Kay —\nlove?\nNeed ask anyone tell us things?","code":""},{"path":"preamble.html","id":"acknowledgements","chapter":"Preamble","heading":"Acknowledgements","text":"work builds contributions many people R Open Source communities. particular, like acknowledge extensive material taken Introduction Data Science: Data Analysis Prediction Algorithms R Rafael . Irizarry, ModernDive: Statistical Inference via Data Science Chester Ismay Albert Y. Kim,\nSTAT 545: Data wrangling, exploration, analysis R Jenny Bryan, Intro Stat Randomization Simulation David M. Diez, Christopher D. Barr Mine Cetinkaya-Rundel, Think Bayes: Bayesian Statistics Made Simple Allen B. Downey, R Data Science Garrett Grolemund Hadley Wickham, Tidy Modeling R Max Kuhn Julia Silge, Broadening Statistical Horizons: Generalized Linear Models Multilevel Models Julie Legler Paul Roback. See Diez, Barr, Çetinkaya-Rundel (2014) Grolemund Wickham (2017).Alboukadel Kassambara, Andrew Tran, Thomas Mock others kindly allowed re-use /modification work.Thanks contributions Harvard students, colleagues random people met internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, Becca Gill, Stephanie Yao Tyler Simko.like gratefully acknowledge funding Derek Bok Center Teaching Learning Harvard University, via Digital Teaching Fellows Learning Lab Undergraduate Fellows programs.","code":""},{"path":"preamble.html","id":"license","chapter":"Preamble","heading":"License","text":"work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"shopping-week.html","id":"shopping-week","chapter":"Shopping Week","heading":"Shopping Week","text":" usual touchstone whether someone asserts mere persuasion least subjective conviction, .e., firm belief, betting. Often someone pronounces propositions confident inflexible defiance seems entirely laid aside concern error. bet disconcerts . Sometimes reveals persuaded enough one ducat ten. happily bet one, 10 suddenly becomes aware previously noticed, namely quite possible erred. -— Immanuel Kant, Critique Pure ReasonThe world confronts us. Make decisions must.","code":""},{"path":"shopping-week.html","id":"warning","chapter":"Shopping Week","heading":"Warning","text":"isn’t book ’re looking .First, book students Gov 1005: Big Data, course offered Government Department Harvard University. Everything book designed make experience students better. material may useful students outside class, don’t really care .Second, book changes time. --date possible.Third, highly opinionated matters . unlikely share views.","code":""},{"path":"shopping-week.html","id":"install-r-and-rstudio","chapter":"Shopping Week","heading":"Install R and RStudio","text":"\nFIGURE 0.1: Analogy difference R RStudio.\nThroughout book, assume using R via RStudio. R like car’s engine RStudio like car’s dashboard.precisely, R programming language runs computations, RStudio integrated development environment (IDE) provides interface adding many convenient features tools. just way access speedometer, rearview mirrors, navigation system makes driving much easier, using RStudio’s interface makes using R much easier well.first need download install R RStudio (Desktop version) computer. Install R first install RStudio.must first: Download install R.must first: Download install R.must second: Download install RStudio Desktop (free version).must second: Download install RStudio Desktop (free version).","code":""},{"path":"shopping-week.html","id":"using-r-via-rstudio","chapter":"Shopping Week","heading":"Using R via RStudio","text":"\nFIGURE 0.2: Icons R versus RStudio computer.\nMuch don’t drive car interacting directly engine rather interacting elements car’s dashboard, won’t using R directly rather use RStudio’s interface. install R RStudio computer, ’ll two new programs (also called applications) can open. ’ll always work RStudio directly R application.Although experimental, allow two modes working R RStudio. First, computer. Second, using FAS OnDemand, available Canvas page. try highlight differences two approaches , use R RStudio, many. recommend students try course semester.FAS OnDemand provides similar experience RStudio Cloud, paid service. data science today done machine, data science future done cloud.Let’s begin getting familiar RStudio, whether machine FAS OnDemand. Begin opening RStudio. open RStudio, see three panes, panels, dividing screen: console pane, files pane, environment pane. see something like :workspace! can see three main windows right now. Let’s focus big one left:three tabs window, ’ll focusing Console Terminal. first start R, Console gives information version R. time written, 4.0.1 latest version R! Console can type run R code. example, type 1 + 1 hit return, Console returns 2.Next, let’s move top right:, main two tabs ’ll using Environment Git (yet available). Environment tab shows datasets variables currently loaded RStudio. case, loaded dataset 3407 rows 5 columns variable x equal 5. , Environment empty, let’s change !create first variable RStudio, go Console type:Now, hit return/enter see variable x equal 5 Environment! must always hit return/enter typing command, otherwise RStudio realize want R execute command. bottom right window:, Files tab allow see computer’s file system. create project later, tab automatically show contents project’s folder. plots tab show preview plots make RStudio. Packages shows packages installed RStudio far. Help explained later chapter.","code":"x <- 5"},{"path":"shopping-week.html","id":"initial-set-up","chapter":"Shopping Week","heading":"Initial Set Up","text":"Whether working FAS OnDemand computer, next step copy paste following R Console:rstudioapi commands set sensible defaults working RStudio. execute commands one time.Note R occasionally ask want install packages. Almost time want , otherwise R asking ! One tricky aspect process occasionally asked R:Unless good reason , always answer “” question.","code":"\nlibrary(rstudioapi)\nrstudioapi::writeRStudioPreference(name = \"load_workspace\", value = FALSE)\nrstudioapi::writeRStudioPreference(name = \"save_workspace\", value = \"never\")Do you want to install from sources the packages which \nneed compilation? (Yes/no/cancel)"},{"path":"shopping-week.html","id":"package-installation","chapter":"Shopping Week","heading":"Package installation","text":"Another point confusion many new R users idea R package. R packages, also known R libraries, extend functionality R providing additional functions, data, documentation. written worldwide community R users can downloaded free.example, among many packages use book remotes package.\nFIGURE 0.3: Analogy R versus R packages.\nR like new mobile phone: certain amount features use first time, doesn’t everything. R packages like apps can download onto phone Apple’s App Store Android’s Google Play.Let’s continue analogy considering Instagram app editing sharing pictures. Say purchased new phone like share photo just taken friends Instagram. need :Install app: Since phone new include Instagram app, need download app either App Store Google Play. ’re set time . might need future update app.Open app: ’ve installed Instagram, need open .Instagram open phone, can proceed share photo friends family. process similar using R package. need :\nFIGURE 0.4: Installing versus loading R package\nInstall package: like installing app phone. packages installed default install R RStudio. Thus want use package first time, need install first. ’ve installed package, likely won’t install unless want update newer version.“Load” package: “Loading” package like opening app phone. Packages “loaded” default start RStudio computer; need “load” package want use every time start RStudio.Let’s perform two steps remotes package.Type install.packages(\"remotes\") console pane RStudio press Return/Enter keyboard. Note must include quotation marks around name package.R packages generally live one two places: CRAN (rhymes “clan”) mature, popular packages Github experimental, less stable packages. install.packages() gets packages CRAN. end section, also install one package Github.","code":""},{"path":"shopping-week.html","id":"package-loading","chapter":"Shopping Week","heading":"Package loading","text":"Recall ’ve installed package, need “load .” words, need “open .” using library() command.example, load remotes package, run following code console pane. mean “run following code?” Either type copy--paste following code console pane hit Enter key.running earlier code, blinking cursor returns next > “prompt” sign, means successful remotes package now loaded ready use. , however, get red “error message” reads:haven’t successfully install . example “error message.” get error message, go back subsection R package installation make sure install remotes package proceeding.historical reasons “packages” also known “libraries,” relevant command loading library().","code":"\nlibrary(remotes)Error in library(remotes) : there is no package called ‘remotes’"},{"path":"shopping-week.html","id":"package-use","chapter":"Shopping Week","heading":"Package use","text":"One common mistake new R users make wanting use particular packages forget load first using library() command just saw. Remember: load package want use every time start RStudio. don’t load package attempting use one features, ’ll see error message similar :different error message one just saw package installed yet. R telling trying use function package yet loaded. R doesn’t know find function using. Almost new users forget starting .Now, installing package available CRAN: PPBDS.data. Copy paste following R Console:Depending computer/browser/locale, might fail, especially quotation marks paste turn curly. case, type commands .result many new packages installed. may take minutes. something gets messed , often useful remove.packages() problematic package install .","code":"Error: could not find function\nlibrary(remotes)\nremotes::install_github(\"davidkane9/PPBDS.data\")"},{"path":"shopping-week.html","id":"tutorials","chapter":"Shopping Week","heading":"Tutorials","text":"chapter textbook, corresponding tutorial available PPBDS.data package. order access tutorials, follow steps:Run library(PPBDS.data) R Console.can access tutorials via Tutorial pane top right tab RStudio. Click “Start tutorial” “Shopping Week” tutorial. don’t see tutorials, try clicking “Home” button – little house symbol thin red roof upper right.order expand window, can drag enlarge tutorial pane inside RStudio. order open popup window, click “Show New Window” icon next home icon.may notice Jobs tab create output tutorial starting . RStudio running code create tutorial. accidentally clicked “Start Tutorial” like stop job running, can click back arrow Jobs tab, press red stop sign icon.work saved sessions, can complete tutorial multiple sittings. order complete tutorial successfully, make sure enter name beginning answer questions. completed tutorial, follow instructions tutorial Submit page upload resulting tutorial_responses.rds file Canvas.Tutorials graded pass/fail. hard fail. long make honest attempt, pass easily.’ve finished tutorial! Now ? ways can close tutorial safely can quit RStudio session.clicked “Show new window” working tutorial pop-window, simply X pop-windowIf working tutorial inside Tutorial pane RStudio, simply press red stop sign icon","code":""},{"path":"visualization.html","id":"visualization","chapter":"1 Visualization","heading":"1 Visualization","text":"People love visualizations. chapter focuses ggplot2, one core packages tidyverse. access datasets, help pages, functions use chapter, load tidyverse running code:one line code loads packages associated tidyverse, packages use almost every data analysis. also tells functions tidyverse conflict functions base R packages might loaded. (future, hide messages ugly.)run code get error message “package called ‘tidyverse’,” ’ll need first install , run library() .Recall, need install package . need load every time use . also use two packages chapter: gapminder nycflights13.packages contain datasets use creating visualizations. may need install packages already done .need explicit function (dataset) comes , ’ll use special form: package::function(). example, ggplot2::ggplot() tells explicitly ’re using ggplot() function ggplot2 package.Now ’re set , let’s create first data visualization R using iris dataset! famous iris data set gives measurements centimeters variables sepal length width petal length width, respectively, 50 flowers 3 species iris (setosa, versicolor, virginica). can learn dataset running ?iris access help page.Wow! Just running lines code created nice visualization compare measured petal lengths 3 species iris! ’ll learn lines code mean write others like throughout chapter, now, let’s learn coding R.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4\n## ✓ tibble  3.0.4     ✓ dplyr   1.0.2\n## ✓ tidyr   1.1.2     ✓ stringr 1.4.0\n## ✓ readr   1.4.0     ✓ forcats 0.5.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x readr::col_factor() masks scales::col_factor()\n## x purrr::discard()    masks scales::discard()\n## x dplyr::filter()     masks stats::filter()\n## x dplyr::lag()        masks stats::lag()\ninstall.packages(\"tidyverse\")\nlibrary(gapminder)\nlibrary(nycflights13)\nggplot(data = iris, \n       mapping = aes(x = Petal.Length, fill = Species)) +\n  geom_histogram(binwidth = 0.1) +\n  labs(title = \"Petal Length Among Three Species of Iris\")"},{"path":"visualization.html","id":"getting-started","chapter":"1 Visualization","heading":"1.1 Getting Started","text":"","code":""},{"path":"visualization.html","id":"how-do-i-code-in-r","chapter":"1 Visualization","heading":"1.1.1 How do I code in R?","text":"Unlike statistical software programs like Excel, SPSS, Minitab provide point--click interfaces, R interpreted language. means type commands written R code. words, code/program R. Note ’ll use terms “coding” “programming” interchangeably book.new world coding, R, RStudio feel benefit detailed introduction, suggest check short book, Getting Used R, RStudio, R Markdown. Ismay Kennedy (2016) includes screencast recordings can follow along pause learn. book also contains introduction R Markdown, tool used reproducible research R.required seasoned coder/computer programmer use R, still set basic programming concepts new R users need understand.","code":""},{"path":"visualization.html","id":"tips","chapter":"1 Visualization","heading":"1.1.2 Tips","text":"Learning code/program quite similar learning foreign language. can daunting frustrating first. frustrations common normal feel disCouraged learn. However, just learning foreign language, put effort afraid make mistakes, anybody can learn improve.useful tips keep mind learn program:Remember computers actually smart: may think computer smartphone “smart,” really people spent lot time energy designing appear “smart.” reality, tell computer everything needs . Furthermore, instructions give computer can’t mistakes , can ambiguous way.Take “copy, paste, tweak” approach: Especially learn first programming language need understand particularly complicated code, often much easier take existing code know works modify suit ends. opposed trying type code scratch. call “copy, paste, tweak” approach. early , suggest trying write code memory, rather take existing examples provided , copy, paste, tweak suit goals. start feeling confident, can slowly move away approach write code scratch. Think “copy, paste, tweak” approach training wheels learning ride bike. getting comfortable, won’t need anymore.best way learn code : Rather learning code sake, find learning code goes much smoother goal mind working particular project, like analyzing data interested important .Practice key: Just method improve foreign language skills lots practice speaking, method improving coding skills lots practice. Write R code every day.","code":""},{"path":"visualization.html","id":"basic-programming","chapter":"1 Visualization","heading":"1.1.3 Basic programming","text":"now introduce basic programming concepts terminology. Instead asking memorize concepts terminology right now, ’ll guide ’ll “learn .” help learn, always use different font distinguish regular text computer_code. best way master topics , opinions, deliberate practice R lots repetition.Console pane: enter commands.Running code: act telling R perform act giving commands console.Objects: values saved R. ’ll show assign values objects display contents objects.Data types: integers, doubles/numerics, logicals, characters. Integers values like -1, 0, 2, 4092. Doubles numerics larger set values containing integers also fractions decimal values like -24.932 0.8. Logicals either TRUE FALSE characters text “cabbage,” “Hamilton,” “Wire greatest TV show ever,” “ramen delicious.” Note characters often denoted quotation marks around .Vectors: series values. created using c() function, c() stands “combine” “concatenate.” example, c(6, 11, 13, 31, 90, 92) creates six element series positive integer values.Factors: categorical data commonly represented R factors. Categorical data can also represented strings. go detail variable types Chapter 2.Data frames: rectangular spreadsheets. representations datasets R rows correspond observations columns correspond variables describe observations. Modern data frames called tibbles.Boolean algebra: TRUE/FALSE statements mathematical operators < (less ), <= (less equal), != (equal ). example, 4 + 2 >= 3 return TRUE, 3 + 5 <= 1 return FALSE. Testing inclusion %% operator. example, \"B\" %% c(\"\", \"B\") returns TRUE \"C\" %% c(\"\", \"B\") returns FALSE. test equality R using == (=, typically used assignment). example, 2 + 1 == 3 compares 2 + 1 3 correct R code, 2 + 1 = 3 return error.Logical operators: & representing “” well | representing “.” example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since clauses TRUE (first clause TRUE). hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since least one two clauses TRUE.Functions, also called commands: perform tasks R. take inputs called arguments return outputs. can either manually specify function’s arguments use function’s default values. example, function seq() R generates sequence numbers. just run seq() return value 1. doesn’t seem useful! default arguments set seq(= 1, = 1). Thus, don’t pass different values change behavior, R just assumes want number 1. can change argument values updating values = sign. try seq(= 2, = 5) get result 2 3 4 5, expect.Help files: provide documentation various functions datasets. can bring help files adding ? name function data frame run console. presented page showing corresponding documentation.","code":""},{"path":"visualization.html","id":"errors-warnings-and-messages","chapter":"1 Visualization","heading":"1.1.4 Errors, warnings, and messages","text":"R reports errors, warnings, messages glaring red font, makes seem like scolding . However, seeing red text console always bad.R show red text console pane three different situations:Errors: red text legitimate error, prefaced “Error …” try explain went wrong. Generally ’s error, code run. example, see Error ggplot(...) : find function \"ggplot\", means ggplot() function accessible package contains function, ggplot2, loaded library(ggplot2). use ggplot() function without ggplot2 package loaded first.Warnings: red text warning, prefaced “Warning:” R try explain ’s warning. Generally code still work, caveats. create scatterplot based dataset two rows data missing entries, see warning: Warning: Removed 2 rows containing missing values (geom_point). R still produce scatterplot remaining non-missing values, warning two points aren’t .Messages: red text doesn’t start either “Error” “Warning,” ’s just friendly message. ’ll see messages load R packages read data saved spreadsheet files read_csv() function ’ll see Chapter 2. helpful diagnostic messages. don’t stop code working. Additionally, ’ll see messages install packages using install.packages().Remember, see red text console, don’t panic. doesn’t necessarily mean anything wrong. Rather:text starts “Error,” figure ’s causing . Think errors red traffic light: something wrong!text starts “Warning,” figure ’s something worry . instance, get warning missing values scatterplot know missing values, ’re fine. ’s surprising, look data see ’s missing. Think warnings yellow traffic light: everything working fine, watch /pay attention.Otherwise, text just message. Read , wave back R, thank talking . Think messages green traffic light: everything working fine keep going!","code":""},{"path":"visualization.html","id":"examining-trains","chapter":"1 Visualization","heading":"1.1.5 Examining trains","text":"Let’s put everything ’ve learned far practice start exploring real data! Data comes us variety formats, pictures text numbers. Throughout book, ’ll focus datasets saved “spreadsheet”-type format. probably common way data collected saved many fields. “spreadsheet”-type datasets called data frames R. ’ll focus working data saved data frames throughout book. , “tibble” modern term “data frame,” use interchangeably.See “Causal effect intergroup contact attitudes,” Ryan D. Enos, Proceedings National Academy Sciences, Mar 2014, 111 (10) background details trains dataset.’ll begin exploring trains data frame PPBDS.data package get idea structure. dataset includes data attitudes toward immigration-related policies, experiment randomly exposed commuters Spanish-speakers Boston train platform. Individuals treatment value “Treated” exposed two Spanish-speakers regular commute. “Control” individuals .Run following code console, either typing cutting--pasting . displays contents trains data frame console. Note depending size monitor, output may vary slightly.Let’s unpack output:tibble: 115 x 8: tibble specific kind data frame R. particular data frame 115 rows corresponding different observations. , observation person. tibble also 8 columns corresponding 8 variables describing observation.gender, liberal, party, age, income, att_start, treatment, att_end different variables dataset.see, dy default, top 10 rows, ten followed ... 105 rows, indicating us 105 rows data fit screen. R showing first 10 rows, since probably want see first. can see (fewer) rows print() command, .e.,","code":"\nlibrary(PPBDS.data)\ntrains## # A tibble: 115 x 8\n##    gender liberal party        age income att_start treatment att_end\n##    <chr>  <lgl>   <chr>      <dbl>  <dbl>     <dbl> <fct>       <dbl>\n##  1 Female FALSE   Democrat      31 135000        11 Treated        11\n##  2 Female FALSE   Republican    34 105000         9 Treated        10\n##  3 Male   TRUE    Democrat      63 135000         3 Treated         5\n##  4 Male   FALSE   Democrat      45 300000        11 Treated        11\n##  5 Male   TRUE    Democrat      55 135000         8 Control         5\n##  6 Female FALSE   Democrat      37  87500        13 Treated        13\n##  7 Female FALSE   Republican    53  87500        13 Control        13\n##  8 Male   FALSE   Democrat      36 135000        10 Treated        11\n##  9 Female FALSE   Democrat      54 105000        12 Control        12\n## 10 Male   FALSE   Republican    42 135000         9 Treated        10\n## # … with 105 more rows\nprint(trains, n = 15)## # A tibble: 115 x 8\n##    gender liberal party        age income att_start treatment att_end\n##    <chr>  <lgl>   <chr>      <dbl>  <dbl>     <dbl> <fct>       <dbl>\n##  1 Female FALSE   Democrat      31 135000        11 Treated        11\n##  2 Female FALSE   Republican    34 105000         9 Treated        10\n##  3 Male   TRUE    Democrat      63 135000         3 Treated         5\n##  4 Male   FALSE   Democrat      45 300000        11 Treated        11\n##  5 Male   TRUE    Democrat      55 135000         8 Control         5\n##  6 Female FALSE   Democrat      37  87500        13 Treated        13\n##  7 Female FALSE   Republican    53  87500        13 Control        13\n##  8 Male   FALSE   Democrat      36 135000        10 Treated        11\n##  9 Female FALSE   Democrat      54 105000        12 Control        12\n## 10 Male   FALSE   Republican    42 135000         9 Treated        10\n## 11 Female FALSE   Democrat      33 105000        10 Control         9\n## 12 Male   FALSE   Democrat      50 250000        11 Treated         9\n## 13 Male   FALSE   Republican    24 105000        13 Treated        13\n## 14 Male   TRUE    Democrat      40  62500         6 Control         7\n## 15 Male   TRUE    Democrat      53 300000         8 Control         8\n## # … with 100 more rows"},{"path":"visualization.html","id":"exploring-data-frames","chapter":"1 Visualization","heading":"1.1.6 Exploring data frames","text":"many ways get feel data contained data frame trains. present two functions take “argument” (input) data frame question. also include third method exploring one particular column data frame:Using View() function, brings RStudio’s built-data viewer.Using glimpse() function, included dplyr package.Using $ “extraction operator,” used view single variable/column data frame.1. View():Run View(trains) console RStudio, either typing cutting--pasting console pane. Explore data frame resulting pop viewer. get habit viewing data frames encounter. Note uppercase V View(). R case-sensitive, ’ll get error message run view(trains) instead View(trains).running View(trains), can explore different variables listed columns. Observe many different types variables. variables including age, income, att_start, att_end quantitative variables. variables numerical nature. variables , including gender, liberal, party, treatment, categorical.Note look leftmost column View(trains) output, see column numbers. row numbers dataset. glance across row number, say row 5, can get idea row representing. allow identify object described given row taking note values columns specific row. often called observational unit. observational unit example individual participating experiment Boston commuter train platform.can identify observational unit determining “thing” measured described variables.2. glimpse():second way ’ll cover explore data frame using glimpse() provides us alternative perspective exploring data frame View() function:Observe glimpse() give first entries variable row variable name. addition, data type variable given immediately variable’s name inside < >. , dbl refers “double,” computer coding terminology quantitative/numerical variables. data type trains, int refers “integer” another data type also represents quantitative/numerical variables. “Doubles” take twice size store computer compared integers.contrast, chr refers “character,” computer terminology text data. forms, text data, gender party person, categorical variables. liberal variable another data type: lgl. types variables represent logical data (True/False). Finally, trains dataset also includes data type fct. fct refers “factor” describes variable nominal, case treatment variable.3. $ operatorLastly, $ operator allows us extract explore single variable within data frame. example, run following consoleWe used $ operator extract age variable return vector. ’ll occasionally exploring data frames using $ operator, instead favoring View() glimpse() functions.","code":"\nglimpse(trains)## Rows: 115\n## Columns: 8\n## $ gender    <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Fema…\n## $ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE…\n## $ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat…\n## $ age       <dbl> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53,…\n## $ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 13500…\n## $ att_start <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 13, 7…\n## $ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contr…\n## $ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8…\ntrains$age##   [1] 31 34 63 45 55 37 53 36 54 42 33 50 24 40 53 50 33 33 32 57 41 36 43 25 41\n##  [26] 33 44 46 41 28 36 37 38 48 20 52 38 45 55 38 45 44 36 29 42 43 54 39 31 50\n##  [51] 60 67 54 44 50 20 57 25 60 44 35 54 52 47 60 47 22 56 50 21 29 45 46 42 23\n##  [76] 29 60 41 30 61 21 46 53 45 46 63 21 31 35 22 68 27 22 30 59 56 32 35 23 60\n## [101] 50 31 43 30 54 52 52 50 37 27 55 42 68 52 50"},{"path":"visualization.html","id":"basic-plots","chapter":"1 Visualization","heading":"1.2 Basic Plots","text":"begin development data science toolbox data visualization. visualizing data, gain valuable insights couldn’t initially obtain just looking raw data values. ’ll use ggplot2 package, provides easy way customize plots.basic, graphics/plots/charts (use terms interchangeably book) provide nice way explore patterns data, presence outliers, distributions individual variables, relationships groups variables. Graphics designed emphasize findings insights want audience understand. , however, require balancing act. one hand, want highlight many interesting findings possible. hand, don’t want include much information overwhelms audience.can break graphic following three essential components:data: dataset containing variables interest.geom: geometric object question. refers type object can observe plot. example: points, lines, bars.aes: aesthetic attributes geometric object. example, x/y position, color, shape, size. Aesthetic attributes mapped variables dataset.\nFIGURE 1.1: Artwork Allison Horst\nthree components specified ggplot() function included ggplot2 package. purposes book, ’ll always provide ggplot() function following arguments (.e., inputs) minimum:data frame variables exist: data argument.mapping variables aesthetic attributes: mapping argument specifies aesthetic attributes involved.’ve specified components, add layers plot using + sign. essential layer add plot layer specifies type geometric object want plot involve: points, lines, bars, others. layers can add plot include plot title, axes labels, visual themes plots, facets.February 2006, Swedish physician data advocate named Hans Rosling gave TED talk titled “best stats ’ve ever seen” presented global economic, health, development data website gapminder.org. example, data 142 countries 2007, let’s consider countries following table peak data.row table corresponds country 2007. row, 5 columns:Country: Name country.Continent: five continents country part . Note “Americas” includes countries North South America Antarctica excluded.Life Expectancy: Life expectancy years.Population: Number people living country.GDP per Capita: Gross domestic product (US dollars).Now consider following scatterplot, plots 142 data’s countries.Let’s view plot grammar graphics:data variable GDP per Capita gets mapped x-position aesthetic points.data variable Life Expectancy gets mapped y-position aesthetic points.data variable Population gets mapped size aesthetic points.data variable Continent gets mapped color aesthetic points.’ll see shortly data corresponds particular data frame data saved “data variables” correspond particular columns data frame. Furthermore, type geometric object considered plot points. said, example considering points, graphics limited just points. can also use lines, bars, geometric objects.Let’s take tour useful geoms.","code":"## # A tibble: 142 x 5\n##   Country     Continent `Life Expectancy` Population `GDP per Capita`\n##   <fct>       <fct>                 <dbl>      <int>            <dbl>\n## 1 Afghanistan Asia                   43.8   31889923             975.\n## 2 Albania     Europe                 76.4    3600523            5937.\n## 3 Algeria     Africa                 72.3   33333216            6223.\n## # … with 139 more rows"},{"path":"visualization.html","id":"geom_point","chapter":"1 Visualization","heading":"1.2.1 geom_point()","text":"Scatterplots, also called bivariate plots, allow visualize relationship two numerical variables. Specifically, visualize relationship following two numerical variables flights data frame included nycflights13 package:dep_delay: departure delay horizontal “x” axis andarr_delay: arrival delay vertical “y” axisfor Alaska Airlines flights leaving NYC 2013. requires paring data 336,776 flights left NYC 2013, 714 Alaska Airlines flights left NYC 2013. scatterplot involve manageable 714 points, overwhelmingly large number like 336,776. achieve , ’ll take flights data frame, filter rows 714 rows corresponding Alaska Airlines flights kept, save new data frame called alaska_flights using <- assignment operator:now, suggest don’t worry don’t fully understand code. ’ll see later code uses dplyr package tidyverse achieve goal: takes flights data frame filters return rows carrier equal \"\", Alaska Airlines’ carrier code. Testing equality specified == =. Convince code achieves supposed exploring resulting data frame running View(alaska_flights). ’ll see 714 rows, consisting 714 Alaska Airlines flights.Let’s now go code create desired scatterplot break piece--piece.Within ggplot() function, specify two plot’s components arguments (.e., inputs):data alaska_flights data frame via data = alaska_flights.aesthetic mapping setting mapping = aes(x = dep_delay, y = arr_delay). Specifically, variable dep_delay maps x position aesthetic, variable arr_delay maps y position.add layer ggplot() function call using + sign. added layer question specifies third component: geometric object. case, geometric object set points specifying geom_point(). running two lines code console, ’ll notice two outputs: warning message following graphic shown.Let’s first unpack graphic. Observe positive relationship exists dep_delay arr_delay: departure delays increase, arrival delays tend also increase. Observe also large mass points clustered near (0, 0), point indicating flights neither departed arrived late.Let’s turn attention warning message. R alerting us fact five rows ignored due missing. 5 rows, either value dep_delay arr_delay missing (recorded R NA), thus rows ignored plot.continue, let’s make observations code created scatterplot. Note + sign comes end lines, beginning. ’ll get error R put beginning line. adding layers plot, enCouraged start new line + (pressing Return/Enter button keyboard) code layer new line. add layers plots, ’ll see greatly improve legibility code.stress importance adding layer specifying geometric object, consider figure layers added. geometric object specified, blank plot useful!","code":"\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()## Warning: Removed 5 rows containing missing values (geom_point).\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay))"},{"path":"visualization.html","id":"geom_jitter","chapter":"1 Visualization","heading":"1.2.2 geom_jitter()","text":"large mass points near (0, 0) scatterplot just plotted can cause confusion since hard tell true number points plotted. result phenomenon called overplotting. one may guess, corresponds points plotted top . overplotting occurs, difficult know number points plotted. two methods address issue overplotting. Either byAdjusting transparency points orAdding little random “jitter,” random “nudges,” points.Method 1: Changing transparencyThe first way addressing overplotting change transparency/opacity points setting alpha argument geom_point(). can change alpha argument value 0 1, 0 sets points 100% transparent 1 sets points 100% opaque. default, alpha set 1. words, don’t explicitly set alpha value, R use alpha = 1.Note following code identical code created scatterplot overplotting, alpha = 0.2 added geom_point() function:key feature note plot transparency points cumulative: areas high-degree overplotting darker, whereas areas lower degree less dark. Note furthermore aes() surrounding alpha = 0.2. mapping variable aesthetic attribute, rather merely changing default setting alpha. fact, ’ll receive error try change second line read geom_point(aes(alpha = 0.2)).Method 2: Jittering pointsThe second way addressing overplotting jittering points. means giving point small “nudge” random direction. can think “jittering” shaking points around bit plot. Let’s illustrate using simple example first. Say data frame 4 identical rows x y values: (0,0), (0,0), (0,0), (0,0). present regular scatterplot 4 points (left) jittered counterpart (right).left-hand regular scatterplot, observe 4 points superimposed top . know 4 values plotted, fact might apparent others. right-hand jittered scatterplot, now plainly evident plot involves four points since point given random “nudge.”Keep mind, however, jittering strictly visualization tool; even creating jittered scatterplot, original values saved data frame remain unchanged.create jittered scatterplot, instead using geom_point(), use geom_jitter(). Observe following code similar code created scatterplot overplotting, geom_point() replaced geom_jitter().order specify much jitter add, adjusted width height arguments geom_jitter(). corresponds hard ’d like shake plot horizontal x-axis units vertical y-axis units, respectively. case, axes minutes. much jitter add using width height arguments? one hand, important add just enough jitter break overlap points, hand, much completely alter original pattern points.can seen resulting plot, case jittering doesn’t really provide much new insight. particular case, can argued changing transparency points setting alpha proved effective. better use jittered scatterplot? better alter points’ transparency? single right answer applies situations. need make subjective choice choice. least confronted overplotting, however, suggest make types plots see one better emphasizes point trying make.","code":"\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point(alpha = 0.2)\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_jitter(width = 30, height = 30)"},{"path":"visualization.html","id":"geom_line","chapter":"1 Visualization","heading":"1.2.3 geom_line()","text":"Linegraphs show relationship two numerical variables variable x-axis, also called explanatory variable, sequential nature. words, inherent ordering variable.common examples linegraphs notion time x-axis: hours, days, weeks, years, etc. Since time sequential, connect consecutive observations variable y-axis line. Linegraphs notion time x-axis also called time series plots. Let’s illustrate linegraphs using another dataset nycflights13 package: weather data frame.Let’s explore weather data frame running View(weather) glimpse(weather). Furthermore let’s read associated help file running ?weather bring help file.Observe variable called temp hourly temperature recordings Fahrenheit weather stations near three major airports New York City: Newark (origin code EWR), John F. Kennedy International (JFK), LaGuardia (LGA). However, instead considering hourly temperatures days 2013 three airports, simplicity let’s consider hourly temperatures Newark airport first 15 days January.Recall section scatterplots, used filter() function choose subset rows flights corresponding Alaska Airlines flights. similarly use filter() , using & operator choose subset rows weather origin \"EWR\", month January, day 1 15. Recall performed similar task section scatterplots creating alaska_flights data frame Alaska Airlines flights, topic ’ll explore next chapter data wrangling.Let’s create time series plot hourly temperatures saved early_january_weather data frame using geom_line() create linegraph, instead using geom_point() like used previously create scatterplots:Much ggplot() code created scatterplot departure arrival delays Alaska Airlines flights, let’s break code piece--piece terms grammar graphics:Within ggplot() function call, specify two components grammar graphics arguments:data early_january_weather data frame setting data = early_january_weather.aesthetic mapping setting mapping = aes(x = time_hour, y = temp). Specifically, variable time_hour maps x position aesthetic, variable temp maps y position aesthetic.add layer ggplot() function call using + sign. layer question specifies third component grammar: geometric object question. case, geometric object line set specifying geom_line().","code":"\nearly_january_weather <- weather %>% \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\nggplot(data = early_january_weather, \n       mapping = aes(x = time_hour, y = temp)) +\n  geom_line()"},{"path":"visualization.html","id":"geom_histogram","chapter":"1 Visualization","heading":"1.2.4 geom_histogram()","text":"Let’s consider temp variable weather data frame , unlike linegraphs, let’s say don’t care relationship time, rather care values temp distribute. words:smallest largest values?“center” “typical” value?values spread ?frequent infrequent values?One way visualize distribution single variable temp plot horizontal line:gives us general idea values temp distribute: observe temperatures vary around\n11°F (-11°C) 100°F (38°C). Furthermore, appear recorded temperatures 40°F 60°F outside range. However, high degree overplotting points, ’s hard get sense exactly many values say 50°F 55°F.commonly produced instead horizontal line plot known histogram. histogram plot visualizes distribution numerical value follows:first cut x-axis series bins, bin represents range values.bin, count number observations fall range corresponding bin.bin, draw bar whose height marks corresponding count.Let’s drill-example histogram.Let’s focus temperatures 30°F (-1°C) 60°F (15°C) now. Observe three bins equal width 30°F 60°F. Thus three bins width 10°F : one bin 30-40°F range, another bin 40-50°F range, another bin 50-60°F range. Since:bin 30-40°F range height around 5000. words, around 5000 hourly temperature recordings 30°F 40°F.bin 40-50°F range height around 4300. words, around 4300 hourly temperature recordings 40°F 50°F.bin 50-60°F range height around 3500. words, around 3500 hourly temperature recordings 50°F 60°F.nine bins spanning 10°F 100°F x-axis interpretation.Let’s now present ggplot() code plot first histogram! Unlike scatterplots linegraphs, now one variable mapped aes(): single numerical variable temp. y-aesthetic histogram, count observations bin, gets computed automatically. Furthermore, geometric object layer now geom_histogram(). running following code create histogram hourly temperatures three NYC airports, ’ll see histogram well warning messages. ’ll discuss warning messages first.first message telling us histogram constructed using bins = 30 30 equally spaced bins. known computer programming default value; unless override default number bins number specify, R choose 30 default. ’ll see next section change number bins another value default.second message telling us something similar warning message received ran code create scatterplot departure arrival delays Alaska Airlines flights: one row missing NA value temp, omitted histogram. R just giving us friendly heads case.Now let’s unpack resulting histogram. Observe values less 25°F well values 80°F rather rare. However, large number bins, ’s hard get sense range temperatures spanned bin; everything one giant amorphous blob. let’s add white vertical borders demarcating bins adding color = \"white\" argument geom_histogram() ignore warning setting number bins better value:now easier time associating ranges temperatures bins. can also vary color bars setting fill argument. example, can set bin colors “blue steel” setting fill = \"steelblue\":’re curious, run colors() see 657 possible choice colors R!Observe last histogram created 50-75°F range appear roughly 8 bins. Thus bin width 25 divided 8, 3.125°F, easily interpretable range work . Let’s improve adjusting number bins histogram one two ways:adjusting number bins via bins argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().Using first method, power specify many bins like cut x-axis . mentioned previous section, default number bins 30. can override default, say 40 bins, follows:Using second method, instead specifying number bins, specify width bins using binwidth argument geom_histogram() layer. example, let’s set width bin 10°F.compare resulting histograms side--side.","code":"## Warning: Removed 1 rows containing missing values (geom_point).\nggplot(data = weather, mapping = aes(x = temp)) + geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 1 rows containing non-finite values (stat_bin).\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\")\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(bins = 40, color = \"white\")\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 10, color = \"white\")"},{"path":"visualization.html","id":"geom_boxplot","chapter":"1 Visualization","heading":"1.2.5 geom_boxplot()","text":"faceted histograms one type visualization used compare distribution numerical variable split values another variable, another type visualization achieves goal side--side boxplot. boxplot constructed information provided five-number summary numerical variable.keep things simple now, let’s consider 2141 hourly temperature recordings month November, represented jittered point.2141 observations following five-number summary:Minimum: 21°FFirst quartile (25th percentile): 36°FMedian (second quartile, 50th percentile): 45°FThird quartile (75th percentile): 52°FMaximum: 71°FIn leftmost plot, let’s mark 5 values dashed horizontal lines top 2141 points. middle plot, let’s add boxplot. rightmost plot, let’s remove points dashed horizontal lines clarity’s sake.boxplot visually summarize \n2141 points cutting 2141 temperature recordings quartiles dashed lines, quartile contains\nroughly 2141 \\(\\div\\) 4 \\(\\approx\\)\n535 observations. Thus25% points fall bottom edge box, first quartile 36°F. words, 25% observations 36°F.25% points fall bottom edge box solid middle line, median 45°F. Thus, 25% observations 36°F 45°F 50% observations 45°F.25% points fall solid middle line top edge box, third quartile 52°F. follows 25% observations 45°F 52°F 75% observations 52°F.25% points fall top edge box. words, 25% observations 52°F.middle 50% points lie within interquartile range (IQR) first third quartile. Thus, IQR example 52 - 36 = 16°F. interquartile range measure numerical variable’s spread.Furthermore, rightmost plot, see whiskers boxplot. whiskers stick either end box way minimum maximum observed temperatures 21°F 71°F, respectively. However, whiskers don’t always extend smallest largest observed values . fact extend 1.5 \\(\\times\\) interquartile range either end box. case November temperatures, 1.5 \\(\\times\\) 16°F = 24°F either end box. observed values outside range get marked points called outliers, ’ll see next section.Let’s now create side--side boxplot hourly temperatures split 12 months previously faceted histograms. mapping month variable x-position aesthetic, temp variable y-position aesthetic, adding geom_boxplot() layer:Observe plot provide information temperature separated month. first warning message clues us . telling us “continuous,” numerical variable, x-position aesthetic. Boxplots, however, require categorical variable mapped x-position aesthetic. second warning message identical warning message plotting histogram hourly temperatures: one values recorded NA missing.can convert numerical variable month factor categorical variable using factor() function. applying factor(month), month goes numerical values 1, 2, …, 12 associated ordering. ordering, ggplot() now knows work variable produce needed plot.resulting plot shows 12 separate “box whiskers” plots similar rightmost plot figure November temperatures. Thus different boxplots shown “side--side.”“box” portions visualization represent 1st quartile, median (2nd quartile), 3rd quartile.height box (value 3rd quartile minus value 1st quartile) interquartile range (IQR). measure spread middle 50% values, longer boxes indicating variability.“whisker” portions plots extend bottoms tops boxes represent points less 25th percentile greater 75th percentiles, respectively. ’re set extend \\(1.5 \\times IQR\\) units away either end boxes. say “” ends whiskers correspond observed temperatures. length whiskers show data outside middle 50% values vary, longer whiskers indicating variability.dots representing values falling outside whiskers called outliers. can thought anomalous (“---ordinary”) values.important keep mind definition outlier somewhat arbitrary absolute. case, defined length whiskers, \n\\(1.5 \\times IQR\\) units long boxplot. Looking side--side plot can see, expected, summer months (6 8) higher median temperatures evidenced higher solid lines middle boxes. can easily compare temperatures across months drawing imaginary horizontal lines across plot. Furthermore, heights 12 boxes quantified interquartile ranges informative ; tell us variability, spread, temperatures recorded given month.","code":"\nggplot(data = weather, mapping = aes(x = month, y = temp)) +\n  geom_boxplot()## Warning: Continuous x aesthetic -- did you forget aes(group=...)?## Warning: Removed 1 rows containing non-finite values (stat_boxplot).\nggplot(data = weather, mapping = aes(x = factor(month), y = temp)) +\n  geom_boxplot()## Warning: Removed 1 rows containing non-finite values (stat_boxplot)."},{"path":"visualization.html","id":"geom_bar","chapter":"1 Visualization","heading":"1.2.6 geom_bar()","text":"histograms boxplots tools visualize distribution numerical variables. Another commonly desired task visualize distribution categorical variable. simpler task, simply counting different categories within categorical variable, also known levels categorical variable. Often best way visualize different counts, also known frequencies, barplots (also called barcharts).Run following code manually creates data frame representing collection fruit: 3 apples 2 oranges. Notice fruits lists fruit individually.Using fruits data frame 5 fruits listed individually 5 rows, map fruit variable x-position aesthetic add geom_bar() layer:Let’s now look flights data frame nycflights13 package visualize distribution categorical variable carrier. words, let’s visualize number domestic flights New York City airline company flew 2013. Since row flights data frame corresponds flight, flights data frame like fruits data frame flights pre-counted carrier. Thus use geom_bar() create barplot. Much like geom_histogram(), one variable aes() aesthetic mapping: variable carrier gets mapped x-position. difference though, histograms bars touch whereas bar graphs white space bars going left right.\nFIGURE 1.2: Number flights departing NYC 2013 airline using geom_bar().\nObserve United Airlines (UA), JetBlue Airways (B6), ExpressJet Airlines (EV) flights depart NYC 2013. don’t know airlines correspond carrier codes, run View(airlines) see directory airlines. example, B6 JetBlue Airways.","code":"\nfruits <- tibble(fruit = c(\"apple\", \"apple\", \"orange\", \n                           \"apple\", \"orange\"))\n\nfruits## # A tibble: 5 x 1\n##   fruit \n##   <chr> \n## 1 apple \n## 2 apple \n## 3 orange\n## 4 apple \n## 5 orange\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()"},{"path":"visualization.html","id":"no-pie-charts","chapter":"1 Visualization","heading":"1.2.6.1 No pie charts!","text":"One common plots used visualize distribution categorical data pie chart. may seem harmless enough, pie charts actually present problem humans unable judge angles well. Robbins (2013) argues overestimate angles greater 90 degrees underestimate angles less 90 degrees. words, difficult us determine relative size one piece pie compared another.pie charts present information way comparisons must made comparing angles, barplots effective present information way comparisons categories can made single horizontal lines.","code":""},{"path":"visualization.html","id":"two-categorical-variables","chapter":"1 Visualization","heading":"1.2.6.2 Two categorical variables","text":"Barplots common way visualize frequency different categories, levels, single categorical variable. Another use barplots visualize joint distribution two categorical variables time. Let’s examine joint distribution outgoing domestic flights NYC carrier well origin. words, number flights carrier origin combination.example, number WestJet flights JFK, number WestJet flights LGA, number WestJet flights EWR, number American Airlines flights JFK, . Recall ggplot() code created barplot carrier frequency:can now map additional variable origin adding fill = origin inside aes() aesthetic mapping.\nFIGURE 1.3: Stacked barplot flight amount carrier origin.\nexample stacked barplot. simple make, certain aspects ideal. example, difficult compare heights different colors bars, corresponding comparing number flights origin airport carriers.continue, let’s address common points confusion among new R users. First, fill aesthetic corresponds color used fill bars, color aesthetic corresponds color outline bars. identical added color histogram geom_histogram section: set outline bars white setting color = \"white\" colors bars blue steel setting fill = \"steelblue\". Observe mapping origin color fill yields grey bars different colored outlines.\nFIGURE 1.4: Stacked barplot color aesthetic used instead fill.\nSecond, note fill another aesthetic mapping much like x-position; thus careful include within parentheses aes() mapping. following code, fill aesthetic specified outside aes() mapping yield error. fairly common error new ggplot users make:alternative stacked barplots side--side barplots, also known dodged barplots. code create side--side barplot identical code create stacked barplot, position = \"dodge\" argument added geom_bar(). words, overriding default barplot type, stacked barplot, specifying side--side barplot instead.\nFIGURE 1.5: Side--side barplot comparing number flights carrier origin.\nNote width bars , F9, FL, HA YV different others. can make one tweak position argument get size terms width bars using robust position_dodge() function.\nFIGURE 1.6: Side--side barplot comparing number flights carrier origin (formatting tweak).\n","code":"\nggplot(data = flights, mapping = aes(x = carrier)) + \n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, color = origin)) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier), fill = origin) +\n  geom_bar()\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = \"dodge\")\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = position_dodge(preserve = \"single\"))"},{"path":"visualization.html","id":"geom_smooth","chapter":"1 Visualization","heading":"1.2.7 geom_smooth()","text":"Now let’s go back geom_point() section add smooth line using geom_smooth() function.’s scatterplot created geom_point() section:Let’s try adding regression line scatterplot using geom_smooth() function combination argument method = lm, lm stands linear model. can add geom_smooth(method = lm) another layer plot.Another method fit line scatter plot called loess method, computes smooth local regression default value small number observations. can read loess using R code ?loess console.’s graph looks like using Loess method local regression fitting:want remove gray area, confidence interval, can set se = FALSE within geom_smooth() function.","code":"## Warning: Removed 5 rows containing missing values (geom_point).## `geom_smooth()` using formula 'y ~ x'## Warning: Removed 5 rows containing non-finite values (stat_smooth).## Warning: Removed 5 rows containing missing values (geom_point).## `geom_smooth()` using formula 'y ~ x'## Warning: Removed 5 rows containing non-finite values (stat_smooth).## Warning: Removed 5 rows containing missing values (geom_point).## `geom_smooth()` using formula 'y ~ x'## Warning: Removed 5 rows containing non-finite values (stat_smooth).## Warning: Removed 5 rows containing missing values (geom_point)."},{"path":"visualization.html","id":"geom_density","chapter":"1 Visualization","heading":"1.2.8 geom_density()","text":"Recall histogram plotted geom_histogram() section.\ncan change geom_histogram() geom_density() make density plot, smoothed version histogram. useful alternative histogram displays continuous data smooth distribution.","code":"\nggplot(data = weather, mapping = aes(x = temp)) + \n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 1 rows containing non-finite values (stat_bin).\nggplot(data = weather, mapping = aes(x = temp)) + \n  geom_density()## Warning: Removed 1 rows containing non-finite values (stat_density)."},{"path":"visualization.html","id":"advanced-plots","chapter":"1 Visualization","heading":"1.3 Advanced Plots","text":"\nFIGURE 1.7: Data Visualization ggplot2 Cheat Sheet\nprevious section seen three components every plot must include: data, data mappings, geom. may found graphics look bit boring. Luckily, Grammar Graphics allows us add layers order customize plots. go additional layers , Data Visualization ggplot2 Cheat Sheet great resource refer ggplot visualizations get complicated.keep simple, change Gapminder plot beginning chapter layer layer. begin creating subset data plotting subset.moment consists three necessary elements:subset gapminder datasetGDP per capita x-axis, life expectancy y-axis (mappings)geom_point(), creates scatterplot","code":"\ngapminder_07 <- gapminder %>% \n                    filter(year == 2007, continent != \"Oceania\")\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()"},{"path":"visualization.html","id":"faceting","chapter":"1 Visualization","heading":"1.3.1 Faceting","text":"Let’s start introducing new concept called faceting. Faceting used ’d like split particular visualization values another variable. create multiple copies type plot matching x y axes, whose content differ.look plot , quite difficult compare continents despite colors. much easier “split” scatterplot 5 continents dataset. words, create plots gdpPercap lifeExp continent separately. adding facet_wrap(~ continent) layer. Note ~ “tilde” can generally found key next “1” key US keyboards. tilde required ’ll receive error Error .quoted(facets) : object 'month' found don’t include .way better! However, R chooses default 2 plots per row, Asia Europe two continents. can specify number rows columns grid using nrow ncol arguments inside facet_wrap(). Let’s get continents row setting nrow = 1:Note , expected, can see positive correlation economic development life expectancy continents. Now also clearer Asia average level Americas, countries Asia extremes.","code":"\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent)\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1)"},{"path":"visualization.html","id":"stats","chapter":"1 Visualization","heading":"1.3.2 Stats","text":"next layer stats, statistical transformations. ggplot provides us many different ones, suitable certain types geoms. Take example stat_boxplot(), applicable scatterplots adds boxplots plot:Now recognize differences continents, also within continents better. However, interpretation box plots intuitive, least first glance. easier understand alternative line best fit, can add stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE):two stats, many different purposes. type “stat” console, get suggestions different options ggplot offers.","code":"\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_boxplot()\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE)"},{"path":"visualization.html","id":"coordinate-systems","chapter":"1 Visualization","heading":"1.3.3 Coordinate Systems","text":"Next, can specify type coordinate system. cases use Cartesian coordinate systems, set coord_cartesian(). R draws every plot default method, don’t need determine specifically. Depending data working , exotic variants like coord_polar() coord_map() may helpful.coordinate system used often coord_flip(). actually just Cartesian coordinate system, name suggests, simply swaps axes:can seen, lifeExp now x-axis gdpPercap y-axis. Compared previous plot now easier observe distribution life expectancy respective continents. example, can see many countries Africa 55 years, Americas Asia 75 years, Europe 80 years. However, think makes sense consider lifeExp dependent variable, don’t use coord_flip() subsequent plots.","code":"\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  coord_flip()"},{"path":"visualization.html","id":"positions-axis-limits-and-scales","chapter":"1 Visualization","heading":"1.3.4 Positions, Axis Limits and Scales","text":"can also use ggplot change position plot content. Positions rather “cosmetic” elements, make plots easier understand. , many different options , often work certain geoms. example, work barplots, can use position_dodge() position_stack() specify whether want arrange bars plot side side top . scatterplots like , position_jitter() can used:Notice several things code. First, positions placed geom refer (case geom_point()). Second, positions always start position = followed position type (case position_jitter()). width height optional can freely defined. might wonder difference position_jitter() geom_jitter() . answer : Nothing. R often several functions can lead result. However, practical start geom_point() first, still choice changing position shaking dots - geom_jitter() plot constructed like beginning.Besides position can also manipulate limits axes using xlim() ylim(). example, assume interested countries GDP per capita 0 30000. can tell R follows want see range. Note , data first argument mapping second ggplot(), don’t actually name arguments. can just provide , long correct order.can see GDP per capita y-axis now shown 0 30000.Finally can change scaling axes. example, might useful display axes scale_x_log10() scale_y_log10() logarithmic scale. Let’s try GDP per capita. Also, note can (lazily!) provide explicit x y argument names aes() long provide values right order: x comes y.Notice scale GDP per capita changed (now, don’t worry overlapping labels).","code":"\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(position = position_jitter(width = 10, height = 15)) +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\nggplot(gapminder_07, \n       aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  xlim(0, 30000)## Warning: Removed 19 rows containing non-finite values (stat_smooth).## Warning: Removed 19 rows containing missing values (geom_point).\nggplot(gapminder_07, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10()"},{"path":"visualization.html","id":"labels-and-text","chapter":"1 Visualization","heading":"1.3.5 Labels and Text","text":"last plot made looks quite good, perfect yet. noticed default R simply uses names variables axes legends. Also, plot title yet. can easily change using labs():title labs() argument long, can insert newline character — “\\n” — middle, cause title take two lines rather one. simplest way deal titles axis labels long.us, many arguments labs() use. arguments probably self-explanatory, makes sense look last one. determines title legend, always named aesthetic legend refers. Since legend created color argument density plot, can refer color =.can also change labels within plots. Wouldn’t great knew country point refers ? can help geom_text():Great! Notice need determine aesthetic called label. defines character variable used basis labels.","code":"\nggplot(gapminder_07, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") \nggplot(gapminder_07, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") +\n  geom_text(aes(label = country), size = 2, \n            color = \"black\", check_overlap = TRUE)"},{"path":"visualization.html","id":"themes","chapter":"1 Visualization","heading":"1.3.6 Themes","text":"almost finished, plot still boring default design. ggplot provides called themes, can used change overall appearance plot without much effort. example, theme_linedraw() uses white background black text:can find overview different themes ggplot . ggthemes package even adds additional themes.addition ready--use themes, theme() function also offers wide selection functions manually changing individual elements. Let’s remove legend, change font plot enlarge axis labels bit:Now take moment compare plot one started :can see things necessary create plot, can make big difference much information can derive . way, remember mentioned Grammar Graphics said build plots layer layer? exactly .","code":"\nggplot(gapminder_07, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\",\n       color = \"Continent\") +\n  geom_text(aes(label = country), size = 2, \n            color = \"black\", check_overlap = TRUE) +\n  theme_linedraw()\nggplot(gapminder_07, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  stat_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\",\n       color = \"Continent\") +\n  geom_text(aes(label = country), size = 2, \n            color = \"black\", check_overlap = TRUE) +\n  theme_linedraw() +\n  theme(legend.position = \"none\", \n        text = element_text(family = \"Palatino\"),\n        axis.text.x = element_text(size = 11),\n        axis.text.y = element_text(size = 10))\nggplot(data = gapminder_07, \n       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()"},{"path":"visualization.html","id":"the-tidyverse","chapter":"1 Visualization","heading":"1.4 The Tidyverse","text":"Let’s go important points specifying arguments (.e., inputs) functions. Run following two segments code:’ll notice code segments create barplot, even though second segment omitted data = mapping = code argument names. ggplot() function default assumes data argument comes first mapping argument comes second. long specify data frame question first aes() mapping second, can omit explicit statement argument names data = mapping =.Going forward rest book, ggplot() code like second segment: data = mapping = explicit naming argument omitted default ordering arguments respected. ’ll brevity’s sake; ’s common see style reviewing R users’ code.Data “wild” never ready visualization. can’t use beautiful plots learned previous chapter “wrangled” data convenient shape. chapter, ’ll introduce series functions tidyverse collection packages help wrangling, everything else need work data. functions include:filter() data frame’s existing rows pick subset . example, alaska_flights data frame.select() specific variable columns data set. example, choose dep_delay arr_delay variables easily view relationship two. Additional functions like slice() can subset data.arrange() rows. example, sort rows weather ascending descending order temp.group_by() rows. words, assign different rows part group. can combine group_by() summarize() report summary statistics group separately. example, say don’t want single overall average departure delay dep_delay three origin airports combined, rather three separate average departure delays, one computed three origin airports.mutate() existing columns/variables create new ones. example, convert hourly temperature recordings degrees Fahrenheit degrees Celsius.Notice used computer_code font describe actions want take data frames. dplyr package, one packages tidyverse, intuitively verb-named functions easy remember.benefit learning use dplyr package data wrangling: similarity database querying language SQL (pronounced “sequel” spelled “S,” “Q,” “L”). SQL (stands “Structured Query Language”) used manage large databases quickly efficiently widely used many institutions lot data. SQL topic left book course database management, keep mind learn dplyr, can learn SQL easily.","code":"\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\nggplot(flights, aes(x = carrier)) +\n  geom_bar()"},{"path":"visualization.html","id":"the-pipe-operator","chapter":"1 Visualization","heading":"1.4.1 The pipe operator: %>%","text":"start data wrangling, let’s first introduce nifty tool gets loaded dplyr package included tidyverse: pipe operator %>%. pipe operator allows us combine multiple operations R single sequential chain actions.Recall chapter 1 add geom layer ggplot format code :Without + end first row, computer know continue onto second. occur without pipe operator. instance, take look following code. can run Rstudio console. happens?Without pipe operator, filter() function work computer know use flights dataset operation within parentheses. become clearer practice using dplyr functions.result transformed/modified data frame want. example, ’ll save result new data frame using <- assignment operator name alaska_flights via alaska_flights <-. assigned modified data frame alaska_flights, separate entity initial flights data frame. , however, written code flights <- flights overwritten previous data frame, original flights data **nycflights13* package re-installed access .Much like adding layers ggplot() using + sign, form single chain data wrangling operations combining verb-named functions single sequence using pipe operator %>%. Furthermore, much like + sign come end lines constructing plots, pipe operator %>% come end lines well. Note also pipe operator can used multiple times sequentially. Simply include end line, following function immediately linked output previous line containing operator. call text within parentheses argument(s) function.worth noting dplyr verbs, well functions larger tidyverse, achieve effect always first argument input tibble. example, look ?dplyr::filter see , example, first argument filter() tibble named .data. , can rewrite code snippet :“.” serves special role using pipes. represents tibble “passed ” previous step pipe. , telling R “.” — flights case — first argument filter(). Since argument names used, can rewrite :almost never write code looks like , least simple dplyr verbs like filter(). , behind scenes, going . , advanced cases, need use “.” refer passed-tibble.Keep mind, many advanced data wrangling functions just six listed introduction chapter; ’ll see examples Section ??. However, just six verb-named functions ’ll able perform broad array data wrangling tasks rest book.","code":"\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = \"dodge\")\nalaska_flights <- flights \n  filter(carrier == \"AS\")\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\nalaska_flights <- flights %>% \n  filter(.data = ., carrier == \"AS\")\nalaska_flights <- flights %>% \n  filter(., carrier == \"AS\")"},{"path":"visualization.html","id":"filter-rows","chapter":"1 Visualization","heading":"1.4.2 filter() rows","text":"\nFIGURE 1.8: Diagram filter() rows operation.\nfilter() function works much like “Filter” option Microsoft Excel; allows specify criteria values variable dataset filters rows match criteria.begin focusing flights New York City Portland, Oregon. dest destination code (airport code) Portland, Oregon \"PDX\". Run following look results RStudio’s spreadsheet viewer ensure flights heading Portland chosen :Note order code. First, take flights data frame flights filter() data frame dest equals \"PDX\" included. test equality using double equal sign == single equal sign =. words filter(dest = \"PDX\") yield error. convention across many programming languages. new coding, ’ll probably forget use double equal sign == times get hang .can use operators beyond just == operator tests equality:> corresponds “greater ”< corresponds “less ”>= corresponds “greater equal ”<= corresponds “less equal ”!= corresponds “equal .” ! used many programming languages indicate “.”Furthermore, can combine multiple criteria using operators make comparisons:| corresponds “”& corresponds “”see many action, let’s filter flights rows departed JFK heading Burlington, Vermont (\"BTV\") Seattle, Washington (\"SEA\") departed months October, November, December. Run following:Note even though colloquially speaking one might say “flights leaving Burlington, Vermont Seattle, Washington,” terms computer operations, really mean “flights leaving Burlington, Vermont leaving Seattle, Washington.” given row data, dest can \"BTV\", \"SEA\", something else, \"BTV\" \"SEA\" time. Furthermore, note careful use parentheses around dest == \"BTV\" | dest == \"SEA\".can often skip use & just separate conditions comma. previous code return identical output btv_sea_flights_fall following code:Let’s present another example uses ! “” operator pick rows don’t match criteria. mentioned earlier, ! can read “.” filtering rows corresponding flights didn’t go Burlington, VT Seattle, WA., note careful use parentheses around (dest == \"BTV\" | dest == \"SEA\"). didn’t use parentheses follows:returning flights headed \"BTV\" headed \"SEA\", entirely different resulting data frame.Now say larger number airports want filter , say \"SEA\", \"SFO\", \"PDX\", \"BTV\", \"BDL\". continue use | () operator:progressively include airports, get unwieldy write. slightly shorter approach uses %% operator along c() function. Recall Subsection ?? c() function “combines” “concatenates” values single vector values.One common mistakes use == filter() rather %%. diabolical since won’t fail , certain situations, might even issue warning. Beware!code filtering flights flights dest vector airports c(\"BTV\", \"SEA\", \"PDX\", \"SFO\", \"BDL\"). outputs many_airports , can see latter takes much less energy code. %% operator useful looking matches commonly one vector/variable compared another.final note, recommend filter() often among first verbs consider applying data. cleans dataset rows care , put differently, narrows scope data frame just observations care .","code":"\nportland_flights <- flights %>% \n  filter(dest == \"PDX\")\nView(portland_flights)\nbtv_sea_flights_fall <- flights %>% \n  filter(origin == \"JFK\" & \n           (dest == \"BTV\" | dest == \"SEA\") & \n           month >= 10)\nView(btv_sea_flights_fall)\nbtv_sea_flights_fall <- flights %>% \n  filter(origin == \"JFK\", \n         (dest == \"BTV\" | dest == \"SEA\"), \n         month >= 10)         \nView(btv_sea_flights_fall)\nnot_BTV_SEA <- flights %>% \n  filter(!(dest == \"BTV\" | dest == \"SEA\"))\nView(not_BTV_SEA)\nflights %>% filter(!dest == \"BTV\" | dest == \"SEA\")\nmany_airports <- flights %>% \n  filter(dest == \"SEA\" | dest == \"SFO\" | dest == \"PDX\" | \n         dest == \"BTV\" | dest == \"BDL\")\nmany_airports <- flights %>% \n  filter(dest %in% c(\"SEA\", \"SFO\", \"PDX\", \"BTV\", \"BDL\"))\nView(many_airports)"},{"path":"visualization.html","id":"select-variables","chapter":"1 Visualization","heading":"1.4.3 select variables","text":"\nFIGURE 1.9: Diagram select() columns.\nUsing filter() function able pick specific rows dataset. select() function allows R users pick specific columns/variables instead.’ve seen flights data frame nycflights13 package contains 19 different variables. can identify names 19 variables running glimpse() function dplyr package:However, say need two 19 variables, say carrier flight. can select() two variables:function makes easier explore large datasets since allows us limit scope variables care . example, select() smaller number variables shown Figure ??, make viewing dataset RStudio’s spreadsheet viewer digestible. Using select() can also useful creating ggplot2 visualizations need variables.Let’s say instead want drop, de-select, certain variables. example, consider variable year flights data frame. variable isn’t quite “variable” always 2013 hence doesn’t change. Say want remove variable data frame. can deselect year using - sign:Another way selecting columns/variables specifying range columns:select() columns month day (including two specified columns), well arr_time sched_arr_time, drop rest.select() function can also used reorder columns used everything() helper function. example, suppose want hour, minute, time_hour variables appear immediately year, month, day variables, discarding rest variables. following code, everything() pick remaining variables:Lastly, helper functions starts_with(), ends_with(), contains() can used select variables/columns match conditions. examples,","code":"\nglimpse(flights)\nflights %>% \n  select(carrier, flight)\nflights_no_year <- flights %>% select(-year)\nflight_arr_times <- flights %>% select(month:day, arr_time:sched_arr_time)\nflight_arr_times\nflights_reorder <- flights %>% \n  select(year, month, day, hour, minute, time_hour, everything())\nglimpse(flights_reorder)\nflights %>% select(starts_with(\"a\"))\nflights %>% select(ends_with(\"delay\"))\nflights %>% select(contains(\"time\"))"},{"path":"visualization.html","id":"slice-and-pull-and","chapter":"1 Visualization","heading":"1.4.4 slice() and pull() and []","text":"slice() pull() additional functions can use pick specific rows columns within data frame.Using slice() gives us specific rows flights tibble:Unlike filter(), slice() relies numeric order data.pull() grabs variable vector, rather leaving within tibble, select() :often handy want feed data function, like mean() requires vector input:common way subset vectors use “bracket” operator []. Example:","code":"\nslice(flights, 2:5)## # A tibble: 4 x 19\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      533            529         4      850            830\n## 2  2013     1     1      542            540         2      923            850\n## 3  2013     1     1      544            545        -1     1004           1022\n## 4  2013     1     1      554            600        -6      812            837\n## # … with 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>\nslice(flights, 2:5) %>% \n  pull(dep_time)## [1] 533 542 544 554\nslice(flights, 2:5) %>% \n  pull(dep_time) %>% \n  mean()## [1] 543\nflights$dep_time[2:5]## [1] 533 542 544 554"},{"path":"visualization.html","id":"arrange-rows","chapter":"1 Visualization","heading":"1.4.5 arrange() rows","text":"One commonly performed data wrangling tasks sort data frame’s rows alphanumeric order one variables. Unlike filter() select(), arrange() remove rows columns data frame. Instead, dplyr package’s arrange() function allows us sort/reorder data frame’s rows according values specified variable.Suppose interested determining flight covers distance domestic flights departing New York City 2013:First, let’s select() pertinent variables make data easy read.order data appears maintained original flights data set. Say instead like see data, sorted distance flight (farthest shortest distance)., however, opposite want. rows sorted flights covering least distance displayed first. arrange() always returns rows sorted ascending order default. switch ordering “descending” order instead, use desc() function like :Let’s try one time character variable. happens try sort destination (dest) variable?can see, character variables sorted alphabetically. Using desc() helper function character variable, sort destinations reverse alphabetically.","code":"\nflights_dist <- flights %>% \n  select(origin, dest, air_time, distance)\nflights_dist## # A tibble: 336,776 x 4\n##    origin dest  air_time distance\n##    <chr>  <chr>    <dbl>    <dbl>\n##  1 EWR    IAH        227     1400\n##  2 LGA    IAH        227     1416\n##  3 JFK    MIA        160     1089\n##  4 JFK    BQN        183     1576\n##  5 LGA    ATL        116      762\n##  6 EWR    ORD        150      719\n##  7 EWR    FLL        158     1065\n##  8 LGA    IAD         53      229\n##  9 JFK    MCO        140      944\n## 10 LGA    ORD        138      733\n## # … with 336,766 more rows\nflights_dist %>% \n  arrange(distance)## # A tibble: 336,776 x 4\n##    origin dest  air_time distance\n##    <chr>  <chr>    <dbl>    <dbl>\n##  1 EWR    LGA         NA       17\n##  2 EWR    PHL         30       80\n##  3 EWR    PHL         30       80\n##  4 EWR    PHL         28       80\n##  5 EWR    PHL         32       80\n##  6 EWR    PHL         29       80\n##  7 EWR    PHL         22       80\n##  8 EWR    PHL         25       80\n##  9 EWR    PHL         30       80\n## 10 EWR    PHL         27       80\n## # … with 336,766 more rows\nflights_dist %>% \n  arrange(desc(distance))## # A tibble: 336,776 x 4\n##    origin dest  air_time distance\n##    <chr>  <chr>    <dbl>    <dbl>\n##  1 JFK    HNL        659     4983\n##  2 JFK    HNL        638     4983\n##  3 JFK    HNL        616     4983\n##  4 JFK    HNL        639     4983\n##  5 JFK    HNL        635     4983\n##  6 JFK    HNL        611     4983\n##  7 JFK    HNL        612     4983\n##  8 JFK    HNL        645     4983\n##  9 JFK    HNL        640     4983\n## 10 JFK    HNL        633     4983\n## # … with 336,766 more rows\nflights_dist %>%\n  arrange(dest)## # A tibble: 336,776 x 4\n##    origin dest  air_time distance\n##    <chr>  <chr>    <dbl>    <dbl>\n##  1 JFK    ABQ        230     1826\n##  2 JFK    ABQ        238     1826\n##  3 JFK    ABQ        251     1826\n##  4 JFK    ABQ        257     1826\n##  5 JFK    ABQ        242     1826\n##  6 JFK    ABQ        240     1826\n##  7 JFK    ABQ        246     1826\n##  8 JFK    ABQ        233     1826\n##  9 JFK    ABQ        236     1826\n## 10 JFK    ABQ        245     1826\n## # … with 336,766 more rows"},{"path":"visualization.html","id":"mutate","chapter":"1 Visualization","heading":"1.4.6 mutate()","text":"\nFIGURE 1.10: Diagram mutate() columns.\nAnother common transformation data create/compute new variables based existing ones. example, say comfortable thinking temperature degrees Celsius (°C) instead degrees Fahrenheit (°F). formula convert temperatures °F °C \\[\n\\text{temp C} = \\frac{\\text{temp F} - 32}{1.8}\n\\]can apply formula temp variable using mutate() function dplyr package, takes existing variables mutates create new ones.code, mutate() weather data frame creating new variable temp_in_C = (temp - 32) / 1.8 overwrite original weather data frame. overwrite data frame weather, instead assigning result new data frame like weather_new? rough rule thumb, long losing original information might need later, ’s acceptable practice overwrite existing data frames updated ones, . hand, overwrite variable temp, instead created new variable called temp_in_C? , erased original information contained temp temperatures Fahrenheit may still valuable us.Let’s now compute monthly average temperatures °F °C using group_by() summarize() code saw Section ??:Let’s consider another example. Passengers often frustrated flight departs late, aren’t annoyed , end, pilots can make time flight. known airline industry gain, create variable using mutate() function:Let’s take look dep_delay, arr_delay, resulting gain variables first 5 rows updated flights data frame Table ??.flight first row departed 2 minutes late arrived 11 minutes late, “gained time air” loss 9 minutes, hence gain 2 - 11 = -9. hand, flight fourth row departed minute early (dep_delay -1) arrived 18 minutes early (arr_delay -18), “gained time air” \\(-1 - (-18) = -1 + 18 = 17\\) minutes, hence gain +17.Recall Section ?? since gain numerical variable, can visualize distribution using histogram.\nFIGURE 1.11: Histogram gain variable.\nresulting histogram Figure 1.11 provides different perspective gain variable summary statistics computed earlier. example, note values gain right around 0.close discussion mutate() function create new variables, note can create multiple new variables mutate() code. Furthermore, within mutate() code can refer new variables just created. example, consider code Grolemund Wickham (2017):","code":"\nweather <- weather %>% \n  mutate(temp_in_C = (temp - 32) / 1.8)\nsummary_monthly_temp <- weather %>% \n  group_by(month) %>% \n  summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), \n            mean_temp_in_C = mean(temp_in_C, na.rm = TRUE))## `summarise()` ungrouping output (override with `.groups` argument)\nsummary_monthly_temp## # A tibble: 12 x 3\n##    month mean_temp_in_F mean_temp_in_C\n##    <int>          <dbl>          <dbl>\n##  1     1           35.6           2.02\n##  2     2           34.3           1.26\n##  3     3           39.9           4.38\n##  4     4           51.7          11.0 \n##  5     5           61.8          16.6 \n##  6     6           72.2          22.3 \n##  7     7           80.1          26.7 \n##  8     8           74.5          23.6 \n##  9     9           67.4          19.7 \n## 10    10           60.1          15.6 \n## 11    11           45.0           7.22\n## 12    12           38.4           3.58\nflights <- flights %>% \n  mutate(gain = dep_delay - arr_delay)## # A tibble: 5 x 3\n##   dep_delay arr_delay  gain\n##       <dbl>     <dbl> <dbl>\n## 1         2        11    -9\n## 2         4        20   -16\n## 3         2        33   -31\n## 4        -1       -18    17\n## 5        -6       -25    19\nggplot(data = flights, mapping = aes(x = gain)) +\n  geom_histogram(color = \"white\", bins = 20)## Warning: Removed 9430 rows containing non-finite values (stat_bin).\nflights <- flights %>% \n  mutate(gain = dep_delay - arr_delay,\n         hours = air_time / 60,\n         gain_per_hour = gain / hours)"},{"path":"visualization.html","id":"ifelse","chapter":"1 Visualization","heading":"1.4.6.1 ifelse()","text":"ifelse() three arguments. first argument test logical vector. result contain value second argument, yes, test TRUE, value third argument, , FALSE. Imagine want create new variable E, TRUE color diamond “E” FALSE otherwise.Alternatively ifelse(), use dplyr::case_when(). case_when() particularly useful inside mutate want create new variable relies complex combination existing variables. Note robust version ifelse() dplyr: if_else(). works exactly standard version somewhat robust.\nFIGURE 1.12: (ref:groupby)\n","code":"\ndiamonds %>% \n  select(carat, color, price) %>% \n  mutate(E = ifelse(color == \"E\", TRUE, FALSE))## # A tibble: 53,940 x 4\n##    carat color price E    \n##    <dbl> <ord> <int> <lgl>\n##  1 0.23  E       326 TRUE \n##  2 0.21  E       326 TRUE \n##  3 0.23  E       327 TRUE \n##  4 0.290 I       334 FALSE\n##  5 0.31  J       335 FALSE\n##  6 0.24  J       336 FALSE\n##  7 0.24  I       336 FALSE\n##  8 0.26  H       337 FALSE\n##  9 0.22  E       337 TRUE \n## 10 0.23  H       338 FALSE\n## # … with 53,930 more rows"},{"path":"visualization.html","id":"summarize","chapter":"1 Visualization","heading":"1.4.7 summarize()","text":"next common task working data frames compute summary statistics. Summary statistics single numerical values summarize large number values. Commonly known examples summary statistics include mean (also called average) median (middle value). examples summary statistics might immediately come mind include sum, smallest value also called minimum, largest value also called maximum, standard deviation.Return familiar dataset. Let’s calculate two summary statistics temp temperature variable weather data frame: mean standard deviation. compute summary statistics, need mean() sd() summary functions R. Summary functions R take many values return single value.Recall output summary() function.function offers array summary statistics columns dataset, summarize() (alternatively summarise()) allows us calculate statistics individual columns dataset.precisely, ’ll use mean() sd() summary functions within summarize() function dplyr package. Note can also use British English spelling summarise(). summarize() function takes data frame returns data frame one row corresponding summary statistics.’ll save results new data frame called summary_temp two columns/variables: mean std_dev:values returned NA?`NA R encodes missing values NA indicates “available” “applicable.” value particular row particular column exist, NA stored instead. Values can missing many reasons. Perhaps data collected someone forgot enter ? Perhaps data collected difficult ? Perhaps erroneous value someone entered corrected read missing? ’ll often encounter issues missing values working real data.Going back summary_temp output, default time try calculate summary statistic variable one NA missing values R, NA returned. work around fact, can set na.rm argument TRUE, rm short “remove”; ignore NA missing values return summary value non-missing values.code follows computes mean standard deviation non-missing values temp:Notice na.rm = TRUE used arguments mean() sd() summary functions individually, summarize() function.However, one needs cautious whenever ignoring missing values ’ve just done. possible ramifications blindly sweeping rows missing values “rug.” fact na.rm argument summary statistic function R set FALSE default. words, R ignore rows missing values default. R alerting presence missing data mindful missingness potential causes missingness throughout analysis.","code":"##     origin               year          month           day            hour     \n##  Length:26115       Min.   :2013   Min.   : 1.0   Min.   : 1.0   Min.   : 0.0  \n##  Class :character   1st Qu.:2013   1st Qu.: 4.0   1st Qu.: 8.0   1st Qu.: 6.0  \n##  Mode  :character   Median :2013   Median : 7.0   Median :16.0   Median :11.0  \n##                     Mean   :2013   Mean   : 6.5   Mean   :15.7   Mean   :11.5  \n##                     3rd Qu.:2013   3rd Qu.: 9.0   3rd Qu.:23.0   3rd Qu.:17.0  \n##                     Max.   :2013   Max.   :12.0   Max.   :31.0   Max.   :23.0  \n##                                                                                \n##       temp          dewp         humid        wind_dir     wind_speed  \n##  Min.   : 11   Min.   :-10   Min.   : 13   Min.   :  0   Min.   :   0  \n##  1st Qu.: 40   1st Qu.: 26   1st Qu.: 47   1st Qu.:120   1st Qu.:   7  \n##  Median : 55   Median : 42   Median : 62   Median :220   Median :  10  \n##  Mean   : 55   Mean   : 41   Mean   : 63   Mean   :200   Mean   :  11  \n##  3rd Qu.: 70   3rd Qu.: 58   3rd Qu.: 79   3rd Qu.:290   3rd Qu.:  14  \n##  Max.   :100   Max.   : 78   Max.   :100   Max.   :360   Max.   :1048  \n##  NA's   :1     NA's   :1     NA's   :1     NA's   :460   NA's   :4     \n##    wind_gust         precip        pressure        visib     \n##  Min.   :16      Min.   :0.00   Min.   : 984   Min.   : 0.0  \n##  1st Qu.:21      1st Qu.:0.00   1st Qu.:1013   1st Qu.:10.0  \n##  Median :24      Median :0.00   Median :1018   Median :10.0  \n##  Mean   :25      Mean   :0.00   Mean   :1018   Mean   : 9.3  \n##  3rd Qu.:29      3rd Qu.:0.00   3rd Qu.:1023   3rd Qu.:10.0  \n##  Max.   :67      Max.   :1.21   Max.   :1042   Max.   :10.0  \n##  NA's   :20778                  NA's   :2729                 \n##    time_hour                     temp_in_C  \n##  Min.   :2013-01-01 01:00:00   Min.   :-12  \n##  1st Qu.:2013-04-01 21:30:00   1st Qu.:  4  \n##  Median :2013-07-01 14:00:00   Median : 13  \n##  Mean   :2013-07-01 18:26:37   Mean   : 13  \n##  3rd Qu.:2013-09-30 13:00:00   3rd Qu.: 21  \n##  Max.   :2013-12-30 18:00:00   Max.   : 38  \n##                                NA's   :1\nsummary_temp <- weather %>% \n  summarize(mean = mean(temp), std_dev = sd(temp))\nsummary_temp## # A tibble: 1 x 2\n##    mean std_dev\n##   <dbl>   <dbl>\n## 1    NA      NA\nsummary_temp <- weather %>% \n  summarize(mean = mean(temp, na.rm = TRUE), \n            std_dev = sd(temp, na.rm = TRUE))\nsummary_temp## # A tibble: 1 x 2\n##    mean std_dev\n##   <dbl>   <dbl>\n## 1  55.3    17.8"},{"path":"visualization.html","id":"basic-statistical-terms","chapter":"1 Visualization","heading":"1.4.8 Basic statistical terms","text":"summary functions can use inside summarize() verb compute summary statistics? can use function R takes many values returns just one. just :mean(): averagemin() max(): minimum maximum values, respectivelysd(): standard deviation, measure spreadsum(): total amount adding multiple numbersn(): count number rowsn_distinct(): number distinct valuesmean()mean commonly reported measure center. commonly called average though term can little ambiguous. mean sum data elements divided many elements . \\(n\\) data points, mean given :\\[Mean = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\]median()median calculated first sorting variable’s data smallest largest. sorting data, middle element list median. middle falls two values, median mean two middle values.sd()next discuss standard deviation (\\(sd\\)) variable. formula can little intimidating first important remember essentially measure far expect given data value mean:\\[sd = \\sqrt{\\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}}\\]Let’s return gain variable previous chapter look summary statistics considering multiple summary functions summarize() code:see example average gain +5 minutes, largest +109 minutes! However, code take time type practice.can also run summary statistics across() multiple columns time. get better understanding across() helper function, run ?across console see arguments takes. Suppose wanted take mean() temp dewp variables., encounter NA values missing values, set na.rm = TRUE argument mean() function.","code":"\nflights %>% \n  mutate(gain = arr_delay - dep_delay) %>% \n  summarize(min = min(gain, na.rm = TRUE),\n            q1 = quantile(gain, 0.25, na.rm = TRUE),\n            median = quantile(gain, 0.5, na.rm = TRUE),\n            q3 = quantile(gain, 0.75, na.rm = TRUE),\n            max = max(gain, na.rm = TRUE),\n            mean = mean(gain, na.rm = TRUE),\n            sd = sd(gain, na.rm = TRUE),\n            missing = sum(is.na(gain)))## # A tibble: 1 x 8\n##     min    q1 median    q3   max  mean    sd missing\n##   <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>   <int>\n## 1  -109   -17     -7     3   196 -5.66  18.0    9430\nweather %>%\n  summarize(across(c(temp, dewp), mean))## # A tibble: 1 x 2\n##    temp  dewp\n##   <dbl> <dbl>\n## 1    NA    NA\nweather %>%\n  summarize(across(c(temp, dewp), ~mean(., na.rm = TRUE)))## # A tibble: 1 x 2\n##    temp  dewp\n##   <dbl> <dbl>\n## 1  55.3  41.4\nweather %>%\n  summarize(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))## # A tibble: 1 x 14\n##    year month   day  hour  temp  dewp humid wind_dir wind_speed wind_gust\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>      <dbl>     <dbl>\n## 1  2013  6.50  15.7  11.5  55.3  41.4  62.5     200.       10.5      25.5\n## # … with 4 more variables: precip <dbl>, pressure <dbl>, visib <dbl>,\n## #   temp_in_C <dbl>"},{"path":"visualization.html","id":"group_by","chapter":"1 Visualization","heading":"1.4.9 group_by()","text":"Say instead single mean temperature whole year, like 12 mean temperatures, one 12 months separately. words, like compute mean temperature split month. can “grouping” temperature observations values another variable, case 12 values variable month. Run following code:Notice warning. R trying save us . warning means tibble issues forth end pipe ungrouped, meaning longer group attribute. probably want happen, default behavior. many cases want keep group attribute, .e., want resulting tibble still grouped month. warning urging us sure want. proper way handle situation, everywhere else use group_by() summarize(), specify .groups argument.code thing first version, issue warning, since made affirmative decision drop grouping variables. See ?summarize discussion possible values .groups.code identical previous code created summary_temp, extra group_by(month) added summarize(). Grouping weather dataset month applying summarize() functions yields data frame displays mean standard deviation temperature split 12 months year.important note group_by() function doesn’t change data frames . Rather changes meta-data, data data, specifically grouping structure. apply summarize() function data frame changes.Run code (forget load package nycflights13 console already):Observe first line output reads # tibble: 336,776 x 20. example meta-data, case number observations/rows variables/columns flights. actual data subsequent table values. Now let’s pipe flights data frame group_by(origin):Observe now additional meta-data: # Groups: origin [3] indicating grouping structure meta-data set based 3 possible levels categorical variable origin: \"EWR\", \"JFK\", \"LGA\". hand, observe data changed: still table 336,776 \\(\\times\\) 19 values.combining group_by() another data wrangling operation, case summarize(), data actually transformed.Let’s revisit n() counting summary function briefly introduced previously. Recall n() function counts rows. opposed sum() summary function returns sum numerical variable. example, suppose ’d like count many flights departed three airports New York City:see Newark (\"EWR\") flights departing 2013 followed \"JFK\" lastly LaGuardia (\"LGA\"). Note subtle important difference sum() n(); sum() returns sum numerical variable, n() returns count number rows/observations.like remove grouping structure meta-data, can pipe resulting data frame ungroup() function:Observe # Groups: origin [3] meta-data longer present.","code":"\nweather %>% \n  group_by(month) %>% \n  summarize(mean = mean(temp, na.rm = TRUE), \n            std_dev = sd(temp, na.rm = TRUE))## `summarise()` ungrouping output (override with `.groups` argument)## # A tibble: 12 x 3\n##    month  mean std_dev\n##    <int> <dbl>   <dbl>\n##  1     1  35.6   10.2 \n##  2     2  34.3    6.98\n##  3     3  39.9    6.25\n##  4     4  51.7    8.79\n##  5     5  61.8    9.68\n##  6     6  72.2    7.55\n##  7     7  80.1    7.12\n##  8     8  74.5    5.19\n##  9     9  67.4    8.47\n## 10    10  60.1    8.85\n## 11    11  45.0   10.4 \n## 12    12  38.4    9.98\nweather %>% \n  group_by(month) %>% \n  summarize(mean = mean(temp, na.rm = TRUE), \n            std_dev = sd(temp, na.rm = TRUE),\n            .groups = \"drop\")## # A tibble: 12 x 3\n##    month  mean std_dev\n##    <int> <dbl>   <dbl>\n##  1     1  35.6   10.2 \n##  2     2  34.3    6.98\n##  3     3  39.9    6.25\n##  4     4  51.7    8.79\n##  5     5  61.8    9.68\n##  6     6  72.2    7.55\n##  7     7  80.1    7.12\n##  8     8  74.5    5.19\n##  9     9  67.4    8.47\n## 10    10  60.1    8.85\n## 11    11  45.0   10.4 \n## 12    12  38.4    9.98\nflights## # A tibble: 336,776 x 22\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\nflights %>% \n  group_by(origin)## # A tibble: 336,776 x 22\n## # Groups:   origin [3]\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\nby_origin <- flights %>% \n  group_by(origin) %>% \n  summarize(count = n(), \n            .groups = \"drop\")\n\nby_origin## # A tibble: 3 x 2\n##   origin  count\n##   <chr>   <int>\n## 1 EWR    120835\n## 2 JFK    111279\n## 3 LGA    104662\nflights %>% \n  group_by(origin) %>% \n  ungroup()## # A tibble: 336,776 x 22\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>"},{"path":"visualization.html","id":"grouping-by-more-than-one-variable","chapter":"1 Visualization","heading":"1.4.9.1 Grouping by more than one variable","text":"limited grouping one variable. Say want know number flights leaving three New York City airports month. can also group second variable month using group_by(origin, month):Observe 36 rows by_origin_monthly 12 months 3 airports (EWR, JFK, LGA).group_by(origin, month) group_by(origin) group_by(month)? Let’s investigate:happened second group_by(month) overwrote grouping structure meta-data earlier group_by(origin), end grouping month. lesson want group_by() two variables, include variables time group_by() adding comma variable names.","code":"\nflights %>% \n  group_by(origin, month) %>% \n  summarize(count = n(),\n            .groups = \"drop\")## # A tibble: 36 x 3\n##    origin month count\n##    <chr>  <int> <int>\n##  1 EWR        1  9893\n##  2 EWR        2  9107\n##  3 EWR        3 10420\n##  4 EWR        4 10531\n##  5 EWR        5 10592\n##  6 EWR        6 10175\n##  7 EWR        7 10475\n##  8 EWR        8 10359\n##  9 EWR        9  9550\n## 10 EWR       10 10104\n## # … with 26 more rows\nflights %>% \n  group_by(origin) %>% \n  group_by(month) %>% \n  summarize(count = n(),\n            .groups = \"drop\")## # A tibble: 12 x 2\n##    month count\n##    <int> <int>\n##  1     1 27004\n##  2     2 24951\n##  3     3 28834\n##  4     4 28330\n##  5     5 28796\n##  6     6 28243\n##  7     7 29425\n##  8     8 29327\n##  9     9 27574\n## 10    10 28889\n## 11    11 27268\n## 12    12 28135"},{"path":"visualization.html","id":"summary","chapter":"1 Visualization","heading":"1.5 Summary","text":"first section looked basic concepts terms dealing programming R. section two, learned three basic components make plot: Data, mapping, one multiple geoms. ggplot2 package offers wide range geoms can use create different types plots. third section looked additional elements can use modify plots. include features axis scaling, labeling themes. Finally, fourth section, took look “super package” tidyverse, also includes ggplot2. Besides tools visualization, offers features importing manipulating data, main topic next chapter.Finally, mentioned seen small part R offers. Since R open source software, many independent developers constantly releasing new R packages new features functions. example, can use gganimate package bring gapminder plot life:rayshader package allows us create beautiful landscapes maps.Plotting cool! end course, know create things like (much ) .","code":"\nlibrary(gganimate)\n\ngapminder %>%\n  filter(continent != \"Oceania\") %>%\n  ggplot(aes(gdpPercap, lifeExp, size = pop, color = continent)) +\n    geom_point(show.legend = FALSE, alpha = 0.7) +\n    facet_wrap(~continent, nrow = 1) +\n    scale_size(range = c(2, 12)) +\n    scale_x_log10() +\n    labs(subtitle = \"Life Expectancy and GDP per Capita (1952-2007)\",\n         x = \"GDP per Capita, USD\",\n         y = \"Life Expectancy, Years\") +\n    theme_linedraw() +\n    transition_time(year) +\n    labs(title = \"Year: {frame_time}\") +\n    shadow_wake(wake_length = 0.1, alpha = FALSE)"},{"path":"wrangling.html","id":"wrangling","chapter":"2 Wrangling","heading":"2 Wrangling","text":"Start loading packages need chapter.tidyverse package used every chapter. PPBDS.data data package created specifically course. lubridate package working dates times. janitor offers functions cleaning dirty data. skimr contains functions useful providing summary statistics. nycflights includes data associated flights New York City’s three major airports. gapminder data countries across decades. fivethirtyeight cleans data FiveThirtyEight team.","code":"\nlibrary(tidyverse)\nlibrary(PPBDS.data)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(gapminder)\nlibrary(nycflights13)\nlibrary(fivethirtyeight)"},{"path":"wrangling.html","id":"data-gathering","chapter":"2 Wrangling","heading":"2.1 Data Gathering","text":"Recall read_csv() function introduced briefly chapter 1 textbook. Let’s import Comma Separated Values .csv file exists internet. .csv file dem_score.csv contains ratings level democracy different countries spanning 1952 1992 accessible https://moderndive.com/data/dem_score.csv. Let’s use read_csv() function readr package read web, import R, save data frame called dem_score.dem_score data frame, minimum value -10 corresponds highly autocratic nation, whereas value 10 corresponds highly democratic nation. Note also backticks surround different variable names. Variable names R default allowed start number include spaces, can get around fact surrounding column name backticks.Note read_csv() function included readr package different read.csv() function comes installed R. difference names might seem trivial (_ instead .), read_csv() function , opinion, easier use since can easily read data web generally imports data much faster speed. Furthermore, read_csv() function included readr saves data frames tibbles default.result code chunk pretty tame. tells us comma .csv file corresponds column, column names taken first line file. , function “guesses” appropriate data type columns creates. Sometimes .csv files lot dirtier require significant wrangling can explore data create usable graphics.Let’s try run read_csv() another dataset. link file containing faculty’s gender data across departments Harvard University. Note, file argument read_csv() function can take link dataset number forms. previous example one , file argument takes url. formats argument take full partial file paths .csv files saved locally computer.Now, call gender_data.can see, second row likely meant contain column names. can run ?read_csv() console see additional arguments read_csv() function may contain make new dataframe easier work . skip argument allows skip rows dataset.Now suppose want change data type one columns. col_type argument allows us . Without col_type argument, division column read character column. Instead, want read factor.Now new dataset read , explore using View() glimpse() functions $ operator previous chapter.Run summary() function gender_data dataframe.may notice something appears wrong dataset. department employed 644 male full professors? Let’s explore going wrong .seems one row table takes sum rows. likely last row dataframe. Use tail() print last rows dataframe check final row.Now, remove “Total” row affect dplyr functions try run dataframe. Check tail() dataframe make sure proper row removed.","code":"\ndem_score <- read_csv(file = \"https://moderndive.com/data/dem_score.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   country = col_character(),\n##   `1952` = col_double(),\n##   `1957` = col_double(),\n##   `1962` = col_double(),\n##   `1967` = col_double(),\n##   `1972` = col_double(),\n##   `1977` = col_double(),\n##   `1982` = col_double(),\n##   `1987` = col_double(),\n##   `1992` = col_double()\n## )\nurl <- \"https://raw.githubusercontent.com/davidkane9/PPBDS/master/02-wrangling/data/harvard-faculty-gender-final.csv\"\ngender_data <- read_csv(file = url)## Warning: Missing column names filled in: 'X1' [1], 'X2' [2], 'X3' [3], 'X4' [4],\n## 'X5' [5], 'X6' [6], 'X7' [7], 'X8' [8], 'X9' [9], 'X10' [10], 'X11' [11],\n## 'X12' [12]## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   X1 = col_character(),\n##   X2 = col_character(),\n##   X3 = col_character(),\n##   X4 = col_character(),\n##   X5 = col_character(),\n##   X6 = col_character(),\n##   X7 = col_character(),\n##   X8 = col_character(),\n##   X9 = col_character(),\n##   X10 = col_character(),\n##   X11 = col_character(),\n##   X12 = col_character()\n## )\ngender_data## # A tibble: 46 x 12\n##    X1      X2     X3     X4     X5     X6    X7    X8    X9    X10   X11   X12  \n##    <chr>   <chr>  <chr>  <chr>  <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n##  1 concen… full_… full_… assoc… assoc… asst… asst… lec_… lec_… prof… prof… divi…\n##  2 Theate… 0      2      0      0      1     0     0     0     0     1     Arts…\n##  3 Women,… 1      5      0      0      0     1     1     9     1     0     Soci…\n##  4 Histor… 5      8      2      1      3     1     2     5     0     0     Soci…\n##  5 Romanc… 6      9      1      1      1     2     2     11    0     0     Arts…\n##  6 Music   6      8      0      0      1     0     5     4     0     2     Arts…\n##  7 Africa… 19     15     1      0      1     0     0     0     1     0     Soci…\n##  8 History 23     18     0      1      0     0     4     2     0     0     Soci…\n##  9 Histor… 11     7      0      1      1     1     0     1     0     0     Arts…\n## 10 Psycho… 11     7      1      2      4     2     1     0     0     0     Soci…\n## # … with 36 more rows\ngender_data <- read_csv(file = url, skip = 1)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   concentration = col_character(),\n##   full_profs_m = col_double(),\n##   full_profs_f = col_double(),\n##   assoc_m = col_double(),\n##   assoc_f = col_double(),\n##   asst_m = col_double(),\n##   asst_f = col_double(),\n##   lec_precep_adj_m = col_double(),\n##   lec_precep_adj_f = col_double(),\n##   prof_of_practice_m = col_double(),\n##   prof_of_practic_f = col_double(),\n##   division = col_character()\n## )\ngender_data## # A tibble: 45 x 12\n##    concentration full_profs_m full_profs_f assoc_m assoc_f asst_m asst_f\n##    <chr>                <dbl>        <dbl>   <dbl>   <dbl>  <dbl>  <dbl>\n##  1 Theater, Dan…            0            2       0       0      1      0\n##  2 Women, Gende…            1            5       0       0      0      1\n##  3 History of S…            5            8       2       1      3      1\n##  4 Romance Lang…            6            9       1       1      1      2\n##  5 Music                    6            8       0       0      1      0\n##  6 African & Af…           19           15       1       0      1      0\n##  7 History                 23           18       0       1      0      0\n##  8 History of A…           11            7       0       1      1      1\n##  9 Psychology              11            7       1       2      4      2\n## 10 Sociology               11            7       2       0      0      1\n## # … with 35 more rows, and 5 more variables: lec_precep_adj_m <dbl>,\n## #   lec_precep_adj_f <dbl>, prof_of_practice_m <dbl>, prof_of_practic_f <dbl>,\n## #   division <chr>\ngender_data <- read_csv(file = url, skip = 1, col_type = cols(division = col_factor()))\nView(gender_data)\nglimpse(gender_data)## Rows: 45\n## Columns: 12\n## $ concentration      <chr> \"Theater, Dance & Media\", \"Women, Gender, & Sexual…\n## $ full_profs_m       <dbl> 0, 1, 5, 6, 6, 19, 23, 11, 11, 11, 13, 20, 33, 2, …\n## $ full_profs_f       <dbl> 2, 5, 8, 9, 8, 15, 18, 7, 7, 7, 8, 11, 17, 1, 2, 2…\n## $ assoc_m            <dbl> 0, 0, 2, 1, 0, 1, 0, 0, 1, 2, 1, 3, 2, 0, 1, 0, 1,…\n## $ assoc_f            <dbl> 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 2, 1, 0, 0, 0, 0,…\n## $ asst_m             <dbl> 1, 0, 3, 1, 1, 1, 0, 1, 4, 0, 1, 1, 1, 0, 0, 0, 2,…\n## $ asst_f             <dbl> 0, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 3, 2, 1, 1, 2, 0,…\n## $ lec_precep_adj_m   <dbl> 0, 1, 2, 2, 5, 0, 4, 0, 1, 0, 4, 4, 3, 0, 0, 2, 2,…\n## $ lec_precep_adj_f   <dbl> 0, 9, 5, 11, 4, 0, 2, 1, 0, 0, 10, 3, 2, 0, 0, 7, …\n## $ prof_of_practice_m <dbl> 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2,…\n## $ prof_of_practic_f  <dbl> 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n## $ division           <fct> Arts & Humanities, Social Sciences, Social Science…\ngender_data$division##  [1] Arts & Humanities Social Sciences   Social Sciences   Arts & Humanities\n##  [5] Arts & Humanities Social Sciences   Social Sciences   Arts & Humanities\n##  [9] Social Sciences   Social Sciences   Arts & Humanities Arts & Humanities\n## [13] Arts & Humanities Arts & Humanities Science           Arts & Humanities\n## [17] Science           Arts & Humanities SEAS              Arts & Humanities\n## [21] SEAS              Arts & Humanities Arts & Humanities Arts & Humanities\n## [25] Social Sciences   Science           Science           Social Sciences  \n## [29] Arts & Humanities Arts & Humanities Science           SEAS             \n## [33] SEAS              Social Sciences   Science           Science          \n## [37] Science           Science           SEAS              Arts & Humanities\n## [41] Social Sciences   SEAS              Arts & Humanities Science          \n## [45] <NA>             \n## Levels: Arts & Humanities Social Sciences Science SEAS\nsummary(gender_data)##  concentration       full_profs_m  full_profs_f    assoc_m      assoc_f    \n##  Length:45          Min.   :  0   Min.   :  0   Min.   : 0   Min.   : 0.0  \n##  Class :character   1st Qu.:  7   1st Qu.:  2   1st Qu.: 0   1st Qu.: 0.0  \n##  Mode  :character   Median : 13   Median :  4   Median : 1   Median : 0.0  \n##                     Mean   : 29   Mean   : 10   Mean   : 2   Mean   : 0.8  \n##                     3rd Qu.: 20   3rd Qu.:  7   3rd Qu.: 2   3rd Qu.: 1.0  \n##                     Max.   :644   Max.   :220   Max.   :40   Max.   :17.0  \n##      asst_m       asst_f   lec_precep_adj_m lec_precep_adj_f prof_of_practice_m\n##  Min.   : 0   Min.   : 0   Min.   :  0      Min.   :  0      Min.   : 0.0      \n##  1st Qu.: 0   1st Qu.: 0   1st Qu.:  0      1st Qu.:  0      1st Qu.: 0.0      \n##  Median : 1   Median : 1   Median :  1      Median :  1      Median : 0.0      \n##  Mean   : 2   Mean   : 2   Mean   :  5      Mean   :  5      Mean   : 0.6      \n##  3rd Qu.: 2   3rd Qu.: 2   3rd Qu.:  3      3rd Qu.:  3      3rd Qu.: 1.0      \n##  Max.   :50   Max.   :49   Max.   :102      Max.   :107      Max.   :14.0      \n##  prof_of_practic_f              division \n##  Min.   :0.0       Arts & Humanities:18  \n##  1st Qu.:0.0       Social Sciences  :10  \n##  Median :0.0       Science          :10  \n##  Mean   :0.4       SEAS             : 6  \n##  3rd Qu.:0.0       NA's             : 1  \n##  Max.   :8.0\ngender_data %>% filter(full_profs_m == 644)## # A tibble: 1 x 12\n##   concentration full_profs_m full_profs_f assoc_m assoc_f asst_m asst_f\n##   <chr>                <dbl>        <dbl>   <dbl>   <dbl>  <dbl>  <dbl>\n## 1 Total                  644          220      40      17     50     49\n## # … with 5 more variables: lec_precep_adj_m <dbl>, lec_precep_adj_f <dbl>,\n## #   prof_of_practice_m <dbl>, prof_of_practic_f <dbl>, division <fct>\ntail(gender_data)## # A tibble: 6 x 12\n##   concentration full_profs_m full_profs_f assoc_m assoc_f asst_m asst_f\n##   <chr>                <dbl>        <dbl>   <dbl>   <dbl>  <dbl>  <dbl>\n## 1 Near Eastern…           11            1       0       0      2      1\n## 2 Economics               40            3       1       0      2      2\n## 3 Environmenta…           13            0       1       1      1      0\n## 4 Linguistics              5            0       1       1      0      1\n## 5 Math                    19            0       0       0      0      0\n## 6 Total                  644          220      40      17     50     49\n## # … with 5 more variables: lec_precep_adj_m <dbl>, lec_precep_adj_f <dbl>,\n## #   prof_of_practice_m <dbl>, prof_of_practic_f <dbl>, division <fct>\ngender_data <- gender_data %>%\n  filter(concentration != \"Total\")\n\ntail(gender_data)## # A tibble: 6 x 12\n##   concentration full_profs_m full_profs_f assoc_m assoc_f asst_m asst_f\n##   <chr>                <dbl>        <dbl>   <dbl>   <dbl>  <dbl>  <dbl>\n## 1 Biomedical E…           15            2       2       0      1      0\n## 2 Near Eastern…           11            1       0       0      2      1\n## 3 Economics               40            3       1       0      2      2\n## 4 Environmenta…           13            0       1       1      1      0\n## 5 Linguistics              5            0       1       1      0      1\n## 6 Math                    19            0       0       0      0      0\n## # … with 5 more variables: lec_precep_adj_m <dbl>, lec_precep_adj_f <dbl>,\n## #   prof_of_practice_m <dbl>, prof_of_practic_f <dbl>, division <fct>"},{"path":"wrangling.html","id":"html","chapter":"2 Wrangling","heading":"2.1.1 HTML","text":"data need answer question always spreadsheet ready us read. example, can find interesting data murders US Wikipedia page:can see data table visit webpage:get data, need web scraping.Web scraping, web harvesting, term use describe process extracting data website. reason can information used browser render webpages received text file server. text code written hyper text markup language (HTML). Every browser way show html source code page, one different. Chrome, can use Control-U PC command+option+U Mac. see something like :code accessible, can download HTML file, import R, write programs extract information need page. However, look HTML code, might seem like daunting task. show convenient tools facilitate process. get idea works, lines code Wikipedia page provides US murders data:can actually see data, except data values surrounded html code <td>. can also see pattern stored. know HTML, can write programs leverage knowledge patterns extract want. also take advantage language widely used make webpages look “pretty” called Cascading Style Sheets (CSS).Although provide tools make possible scrape data without knowing HTML, data scientist quite useful learn HTML CSS. improve scraping skills, might come handy creating webpage showcase work.","code":"\nurl <- paste0(\"https://en.wikipedia.org/w/index.php?title=\",\n              \"Gun_violence_in_the_United_States_by_state\",\n              \"&direction=prev&oldid=810166167\")<table class=\"wikitable sortable\">\n<tr>\n<th>State<\/th>\n<th><a href=\"/wiki/List_of_U.S._states_and_territories_by_population\" \ntitle=\"List of U.S. states and territories by population\">Population<\/a><br />\n<small>(total inhabitants)<\/small><br />\n<small>(2015)<\/small> <sup id=\"cite_ref-1\" class=\"reference\">\n<a href=\"#cite_note-1\">[1]<\/a><\/sup><\/th>\n<th>Murders and Nonnegligent\n<p>Manslaughter<br />\n<small>(total deaths)<\/small><br />\n<small>(2015)<\/small> <sup id=\"cite_ref-2\" class=\"reference\">\n<a href=\"#cite_note-2\">[2]<\/a><\/sup><\/p>\n<\/th>\n<th>Murder and Nonnegligent\n<p>Manslaughter Rate<br />\n<small>(per 100,000 inhabitants)<\/small><br />\n<small>(2015)<\/small><\/p>\n<\/th>\n<\/tr>\n<tr>\n<td><a href=\"/wiki/Alabama\" title=\"Alabama\">Alabama<\/a><\/td>\n<td>4,853,875<\/td>\n<td>348<\/td>\n<td>7.2<\/td>\n<\/tr>\n<tr>\n<td><a href=\"/wiki/Alaska\" title=\"Alaska\">Alaska<\/a><\/td>\n<td>737,709<\/td>\n<td>59<\/td>\n<td>8.0<\/td>\n<\/tr>\n<tr>"},{"path":"wrangling.html","id":"the-rvest-package","chapter":"2 Wrangling","heading":"2.1.2 The rvest package","text":"tidyverse contains web harvesting package called rvest. first step using package import webpage R. package makes quite simple:Note entire Murders US Wikipedia webpage now contained h. class object :rvest package actually general; handles XML documents. XML general markup language (’s ML stands ) can used represent kind data. HTML specific type XML specifically developed representing webpages. focus HTML documents.Now, extract table object h? print h, don’t really see much:can see code defines downloaded webpage using html_text function like :don’t show output includes thousands characters, look , can see data stored HTML table: can see line HTML code <table class=\"wikitable sortable\">. different parts HTML document, often defined message < > referred nodes. rvest package includes functions extract nodes HTML document: html_nodes extracts nodes different types html_node extracts first one. extract tables html code use:Now, instead entire webpage, just html code tables page:table interested first one:clearly tidy dataset, even data frame. code , can definitely see pattern writing code extract just data doable. fact, rvest includes function just converting HTML tables data frames:now much closer usable data table:still wrangling . example, need remove commas turn characters numbers. continuing , learn general approach extracting information web sites.","code":"\nlibrary(tidyverse)\nlibrary(rvest)\nh <- read_html(url)\nclass(h)## [1] \"xml_document\" \"xml_node\"\nh## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nhtml_text(h)\ntab <- h %>% html_nodes(\"table\")\ntab## {xml_nodeset (2)}\n## [1] <table class=\"wikitable sortable\"><tbody>\\n<tr>\\n<th>State\\n<\/th>\\n<th>\\n ...\n## [2] <table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\ntab[[1]]## {html_node}\n## <table class=\"wikitable sortable\">\n## [1] <tbody>\\n<tr>\\n<th>State\\n<\/th>\\n<th>\\n<a href=\"/wiki/List_of_U.S._states ...\ntab <- tab[[1]] %>% html_table\nclass(tab)## [1] \"data.frame\"\ntab <- tab %>% setNames(c(\"state\", \"population\", \"total\", \"murder_rate\")) \nhead(tab)##        state population total murder_rate\n## 1    Alabama  4,853,875   348         7.2\n## 2     Alaska    737,709    59         8.0\n## 3    Arizona  6,817,565   309         4.5\n## 4   Arkansas  2,977,853   181         6.1\n## 5 California 38,993,940 1,861         4.8\n## 6   Colorado  5,448,819   176         3.2"},{"path":"wrangling.html","id":"css-selectors","chapter":"2 Wrangling","heading":"2.1.3 CSS selectors","text":"default look webpage made basic HTML quite unattractive. aesthetically pleasing pages see today made using CSS define look style webpages. fact pages company style usually results use CSS file define style. general way CSS files work defining elements webpage look. title, headings, itemized lists, tables, links, example, receive style including font, color, size, distance margin. CSS leveraging patterns used define elements, referred selectors. example pattern, used , table, many, many .want grab data webpage happen know selector unique part page containing data, can use html_nodes function. However, knowing selector can quite complicated.\nfact, complexity webpages increasing become sophisticated. advanced ones, seems almost impossible find nodes define particular piece data. However, selector gadgets actually make possible.SelectorGadget1 piece software allows interactively determine CSS selector need extract specific components webpage. plan scraping data tables html pages, highly recommend install . Chrome extension available permits turn gadget , click page, highlights parts shows selector need extract parts. various demos including rvest author Hadley Wickham’s\nvignette2 tutorials based vignette3.4","code":""},{"path":"wrangling.html","id":"json","chapter":"2 Wrangling","heading":"2.1.4 JSON","text":"Sharing data internet become common. Unfortunately, providers use different formats, makes harder data scientists wrangle data R. Yet standards also becoming common. Currently, format widely adopted JavaScript Object Notation JSON. format general, nothing like spreadsheet. JSON file looks like code use define list. example information stored JSON format:file actually represents data frame. read , can use function fromJSON jsonlite package. Note JSON files often made available via internet. Several organizations provide JSON API web service can connect directly obtain data.can learn much examining tutorials help files jsonlite package. package intended relatively simple tasks converging data tables. flexibility, recommend rjson.","code":"## \n## Attaching package: 'jsonlite'## The following object is masked from 'package:purrr':\n## \n##     flatten## [\n##   {\n##     \"name\": \"Miguel\",\n##     \"student_id\": 1,\n##     \"exam_1\": 85,\n##     \"exam_2\": 86\n##   },\n##   {\n##     \"name\": \"Sofia\",\n##     \"student_id\": 2,\n##     \"exam_1\": 94,\n##     \"exam_2\": 93\n##   },\n##   {\n##     \"name\": \"Aya\",\n##     \"student_id\": 3,\n##     \"exam_1\": 87,\n##     \"exam_2\": 88\n##   },\n##   {\n##     \"name\": \"Cheng\",\n##     \"student_id\": 4,\n##     \"exam_1\": 90,\n##     \"exam_2\": 91\n##   }\n## ]"},{"path":"wrangling.html","id":"characters","chapter":"2 Wrangling","heading":"2.2 Characters","text":"’ve spent lot time working big, beautiful data frames clean wholesome, like gapminder nycflights13 data.real life much nastier. bring data R outside world discover problems. might think: hard can deal character data? answer : can hard!discuss common remedial tasks cleaning transforming character data, also known “strings.” data frame tibble consist one atomic vectors certain class. lesson deals things can vectors class character.beginning chapter, loaded tidyverse, includes stringr. package allows users manipulate strings functions str_.basic string manipulation tasks:Study single character vector\nlong strings?\nPresence/absence literal string\nlong strings?Presence/absence literal stringOperate single character vector\nKeep/discard elements contain literal string\nSplit two character vectors using fixed delimiter\nSnip pieces strings based character position\nCollapse single string\nKeep/discard elements contain literal stringSplit two character vectors using fixed delimiterSnip pieces strings based character positionCollapse single stringOperate two character vectors\nGlue together element-wise get new character vector.\nGlue together element-wise get new character vector.fruit, words, sentences character vectors ship stringr practicing.*","code":""},{"path":"wrangling.html","id":"studying-a-single-character-vector","chapter":"2 Wrangling","heading":"2.2.1 Studying a single character vector","text":"Determine presence/absence literal string str_detect(). Spoiler: later see str_detect() also detects regular expressions.fruits actually use word “fruit?”’s easiest way get actual fruits match? Use str_subset() keep matching elements. Note storing new vector my_fruit use later examples!Use stringr::str_view() create window viewer highlights specified pattern instances within list. look subset fruit highlight pattern “berry” within items list.can helpful want check highlighting correct pattern (especially using regex). think following regex means? (explained depth later)","code":"\nstr_detect(fruit, pattern = \"fruit\")##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n## [37] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [73] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n(my_fruit <- str_subset(fruit, pattern = \"fruit\"))## [1] \"breadfruit\"   \"dragonfruit\"  \"grapefruit\"   \"jackfruit\"    \"kiwi fruit\"  \n## [6] \"passionfruit\" \"star fruit\"   \"ugli fruit\"\nstr_view(fruit[5:10], pattern = \"berry\")\nstr_view(fruit[1:5], pattern = \"^a.*o\")"},{"path":"wrangling.html","id":"operating-on-a-single-character-vector","chapter":"2 Wrangling","heading":"2.2.2 Operating on a single character vector","text":"Use stringr::str_split() split strings delimiter. fruits compound words, like “grapefruit,” two words, like “ugli fruit.” split single space \" \", show use regular expression later.’s bummer get list back. must ! full generality, split strings must return list, knows many pieces ?willing commit number pieces, can use str_split_fixed() get character matrix. ’re welcome!--split variable lives data frame, tidyr::separate() split 2 variables.Count characters strings str_length(). Note different length character vector .can snip substrings based character position str_sub().start end arguments vectorised. Example: sliding 3-character window.Finally, str_sub() also works assignment, .e. left hand side <-.can collapse character vector length n > 1 single string str_c(), also uses (see next section).can replace pattern str_replace(). use explicit string--replace, later revisit regular expression.special case comes lot replacing NA, str_replace_na().NA-afflicted variable lives data frame, can use tidyr::replace_na().concludes treatment regex-free manipulations character data!","code":"\nstr_split(my_fruit, pattern = \" \")## [[1]]\n## [1] \"breadfruit\"\n## \n## [[2]]\n## [1] \"dragonfruit\"\n## \n## [[3]]\n## [1] \"grapefruit\"\n## \n## [[4]]\n## [1] \"jackfruit\"\n## \n## [[5]]\n## [1] \"kiwi\"  \"fruit\"\n## \n## [[6]]\n## [1] \"passionfruit\"\n## \n## [[7]]\n## [1] \"star\"  \"fruit\"\n## \n## [[8]]\n## [1] \"ugli\"  \"fruit\"\nstr_split_fixed(my_fruit, pattern = \" \", n = 2)##      [,1]           [,2]   \n## [1,] \"breadfruit\"   \"\"     \n## [2,] \"dragonfruit\"  \"\"     \n## [3,] \"grapefruit\"   \"\"     \n## [4,] \"jackfruit\"    \"\"     \n## [5,] \"kiwi\"         \"fruit\"\n## [6,] \"passionfruit\" \"\"     \n## [7,] \"star\"         \"fruit\"\n## [8,] \"ugli\"         \"fruit\"\nmy_fruit_df <- tibble(my_fruit)\nmy_fruit_df %>% \n  separate(my_fruit, into = c(\"pre\", \"post\"), sep = \" \")## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 5 rows [1, 2, 3,\n## 4, 6].## # A tibble: 8 x 2\n##   pre          post \n##   <chr>        <chr>\n## 1 breadfruit   <NA> \n## 2 dragonfruit  <NA> \n## 3 grapefruit   <NA> \n## 4 jackfruit    <NA> \n## 5 kiwi         fruit\n## 6 passionfruit <NA> \n## 7 star         fruit\n## 8 ugli         fruit\nlength(my_fruit)## [1] 8\nstr_length(my_fruit)## [1] 10 11 10  9 10 12 10 10\nhead(fruit) %>% \n  str_sub(1, 3)## [1] \"app\" \"apr\" \"avo\" \"ban\" \"bel\" \"bil\"\ntibble(fruit) %>% \n  head() %>% \n  mutate(snip = str_sub(fruit, 1:6, 3:8))## # A tibble: 6 x 2\n##   fruit       snip \n##   <chr>       <chr>\n## 1 apple       \"app\"\n## 2 apricot     \"pri\"\n## 3 avocado     \"oca\"\n## 4 banana      \"ana\"\n## 5 bell pepper \" pe\"\n## 6 bilberry    \"rry\"\n(x <- head(fruit, 3))## [1] \"apple\"   \"apricot\" \"avocado\"\nstr_sub(x, 1, 3) <- \"AAA\"\nx## [1] \"AAAle\"   \"AAAicot\" \"AAAcado\"\nhead(fruit) %>% \n  str_c(collapse = \", \")## [1] \"apple, apricot, avocado, banana, bell pepper, bilberry\"\nstr_replace(my_fruit, pattern = \"fruit\", replacement = \"THINGY\")## [1] \"breadTHINGY\"   \"dragonTHINGY\"  \"grapeTHINGY\"   \"jackTHINGY\"   \n## [5] \"kiwi THINGY\"   \"passionTHINGY\" \"star THINGY\"   \"ugli THINGY\"\nmelons <- str_subset(fruit, pattern = \"melon\")\nmelons[2] <- NA\nmelons## [1] \"canary melon\" NA             \"watermelon\"\nstr_replace_na(melons, \"UNKNOWN MELON\")## [1] \"canary melon\"  \"UNKNOWN MELON\" \"watermelon\"\ntibble(melons) %>% \n  replace_na(replace = list(melons = \"UNKNOWN MELON\"))## # A tibble: 3 x 1\n##   melons       \n##   <chr>        \n## 1 canary melon \n## 2 UNKNOWN MELON\n## 3 watermelon"},{"path":"wrangling.html","id":"operating-on-two-or-more-character-vectors","chapter":"2 Wrangling","heading":"2.2.3 Operating on two or more character vectors","text":"two character vectors length, can glue together element-wise, get new vector length. … awful smoothie flavors?Element-wise catenation can combined collapsing.--combined vectors variables data frame, can use tidyr::unite() make single new variable .","code":"\nstr_c(fruit[1:4], fruit[5:8], sep = \" & \")## [1] \"apple & bell pepper\"   \"apricot & bilberry\"    \"avocado & blackberry\" \n## [4] \"banana & blackcurrant\"\nstr_c(fruit[1:4], fruit[5:8], sep = \" & \", collapse = \", \")## [1] \"apple & bell pepper, apricot & bilberry, avocado & blackberry, banana & blackcurrant\"\nfruit_df <- tibble(\n  fruit1 = fruit[1:4],\n  fruit2 = fruit[5:8]\n)\nfruit_df %>% \n  unite(\"flavor_combo\", fruit1, fruit2, sep = \" & \")## # A tibble: 4 x 1\n##   flavor_combo         \n##   <chr>                \n## 1 apple & bell pepper  \n## 2 apricot & bilberry   \n## 3 avocado & blackberry \n## 4 banana & blackcurrant"},{"path":"wrangling.html","id":"regular-expressions-with-stringr","chapter":"2 Wrangling","heading":"2.2.4 Regular expressions with stringr","text":"\nFIGURE 2.1: @ThePracticalDev\nFrequently string tasks expressed terms fixed string, can described terms pattern. Regular expressions, aka “regexes,” standard way specify patterns. regexes, specific characters constructs take special meaning order match multiple strings.country names gapminder dataset convenient examples. Load now store\n142 unique country names object countries.first metacharacter period ., stands single character, except newline (way, represented \\n). regex .match countries , followed single character, followed . Yes, regexes case sensitive, .e. “Italy” match.Notice .matches “ina,” “ica,” “ita,” .Anchors can included express expression must occur within string. ^ indicates beginning string $ indicates end.Note regex .$ matches many fewer countries .alone. Likewise, elements my_fruit match d ^d, requires “d” string start.metacharacter \\b indicates word boundary \\B indicates word boundary. first encounter something called “escaping” right now just want accept need prepend second backslash use sequences regexes R. ’ll come back tedious point later.Characters can specified via classes. can make explicitly “hand” use pre-existing ones. Character classes usually given inside square brackets, [] come often metacharacter , \\d single digit.match ia end country name, preceded one characters class. , negated class, preceded anything one characters.revisit splitting my_fruit two general ways match whitespace: \\s metacharacter POSIX class [:space:]. Notice must prepend extra backslash \\ escape \\s POSIX class surrounded two sets square brackets.Let’s see country names contain punctuation.","code":"\ncountries <- levels(gapminder$country)\nstr_subset(countries, pattern = \"i.a\")##  [1] \"Argentina\"                \"Bosnia and Herzegovina\"  \n##  [3] \"Burkina Faso\"             \"Central African Republic\"\n##  [5] \"China\"                    \"Costa Rica\"              \n##  [7] \"Dominican Republic\"       \"Hong Kong, China\"        \n##  [9] \"Jamaica\"                  \"Mauritania\"              \n## [11] \"Nicaragua\"                \"South Africa\"            \n## [13] \"Swaziland\"                \"Taiwan\"                  \n## [15] \"Thailand\"                 \"Trinidad and Tobago\"\nstr_subset(countries, pattern = \"i.a$\")## [1] \"Argentina\"              \"Bosnia and Herzegovina\" \"China\"                 \n## [4] \"Costa Rica\"             \"Hong Kong, China\"       \"Jamaica\"               \n## [7] \"South Africa\"\nstr_subset(my_fruit, pattern = \"d\")## [1] \"breadfruit\"  \"dragonfruit\"\nstr_subset(my_fruit, pattern = \"^d\")## [1] \"dragonfruit\"\nstr_subset(fruit, pattern = \"melon\")## [1] \"canary melon\" \"rock melon\"   \"watermelon\"\nstr_subset(fruit, pattern = \"\\\\bmelon\")## [1] \"canary melon\" \"rock melon\"\nstr_subset(fruit, pattern = \"\\\\Bmelon\")## [1] \"watermelon\"\n# Make a class \"by hand\"\n\nstr_subset(countries, pattern = \"[nls]ia$\")##  [1] \"Albania\"    \"Australia\"  \"Indonesia\"  \"Malaysia\"   \"Mauritania\"\n##  [6] \"Mongolia\"   \"Romania\"    \"Slovenia\"   \"Somalia\"    \"Tanzania\"  \n## [11] \"Tunisia\"\n# Use ^ to negate the class\n\nstr_subset(countries, pattern = \"[^nls]ia$\")##  [1] \"Algeria\"      \"Austria\"      \"Bolivia\"      \"Bulgaria\"     \"Cambodia\"    \n##  [6] \"Colombia\"     \"Croatia\"      \"Ethiopia\"     \"Gambia\"       \"India\"       \n## [11] \"Liberia\"      \"Namibia\"      \"Nigeria\"      \"Saudi Arabia\" \"Serbia\"      \n## [16] \"Syria\"        \"Zambia\"\n# Remember this?\n# str_split_fixed(fruit, pattern = \" \", n = 2)\n# Alternatives:\n\nstr_split_fixed(my_fruit, pattern = \"\\\\s\", n = 2)##      [,1]           [,2]   \n## [1,] \"breadfruit\"   \"\"     \n## [2,] \"dragonfruit\"  \"\"     \n## [3,] \"grapefruit\"   \"\"     \n## [4,] \"jackfruit\"    \"\"     \n## [5,] \"kiwi\"         \"fruit\"\n## [6,] \"passionfruit\" \"\"     \n## [7,] \"star\"         \"fruit\"\n## [8,] \"ugli\"         \"fruit\"\nstr_split_fixed(my_fruit, pattern = \"[[:space:]]\", n = 2)##      [,1]           [,2]   \n## [1,] \"breadfruit\"   \"\"     \n## [2,] \"dragonfruit\"  \"\"     \n## [3,] \"grapefruit\"   \"\"     \n## [4,] \"jackfruit\"    \"\"     \n## [5,] \"kiwi\"         \"fruit\"\n## [6,] \"passionfruit\" \"\"     \n## [7,] \"star\"         \"fruit\"\n## [8,] \"ugli\"         \"fruit\"\nstr_subset(countries, \"[[:punct:]]\")## [1] \"Congo, Dem. Rep.\" \"Congo, Rep.\"      \"Cote d'Ivoire\"    \"Guinea-Bissau\"   \n## [5] \"Hong Kong, China\" \"Korea, Dem. Rep.\" \"Korea, Rep.\"      \"Yemen, Rep.\""},{"path":"wrangling.html","id":"quantifiers","chapter":"2 Wrangling","heading":"2.2.5 Quantifiers","text":"can decorate characters (constructs, like metacharacters classes) information many characters allowed match.Explore inspecting matches l followed e, allowing various numbers characters .l.*e match strings 0 characters , .e. string l eventually followed e. inclusive regex example, store result matches use baseline comparison.Change quantifier * + require least one intervening character. strings longer match: literal le preceding l following e.Change quantifier * ? require one intervening character. strings longer match, shortest gap l following e least two characters.Finally, remove quantifier allow intervening characters. strings longer match lack literal le.","code":"\n(matches <- str_subset(fruit, pattern = \"l.*e\"))##  [1] \"apple\"             \"bell pepper\"       \"bilberry\"         \n##  [4] \"blackberry\"        \"blood orange\"      \"blueberry\"        \n##  [7] \"cantaloupe\"        \"chili pepper\"      \"clementine\"       \n## [10] \"cloudberry\"        \"elderberry\"        \"huckleberry\"      \n## [13] \"lemon\"             \"lime\"              \"lychee\"           \n## [16] \"mulberry\"          \"olive\"             \"pineapple\"        \n## [19] \"purple mangosteen\" \"salal berry\"\nlist(match = intersect(matches, str_subset(fruit, pattern = \"l.+e\")),\n     no_match = setdiff(matches, str_subset(fruit, pattern = \"l.+e\")))## $match\n##  [1] \"bell pepper\"       \"bilberry\"          \"blackberry\"       \n##  [4] \"blood orange\"      \"blueberry\"         \"cantaloupe\"       \n##  [7] \"chili pepper\"      \"clementine\"        \"cloudberry\"       \n## [10] \"elderberry\"        \"huckleberry\"       \"lime\"             \n## [13] \"lychee\"            \"mulberry\"          \"olive\"            \n## [16] \"purple mangosteen\" \"salal berry\"      \n## \n## $no_match\n## [1] \"apple\"     \"lemon\"     \"pineapple\"\nlist(match = intersect(matches, str_subset(fruit, pattern = \"l.?e\")),\n     no_match = setdiff(matches, str_subset(fruit, pattern = \"l.?e\")))## $match\n##  [1] \"apple\"             \"bilberry\"          \"blueberry\"        \n##  [4] \"clementine\"        \"elderberry\"        \"huckleberry\"      \n##  [7] \"lemon\"             \"mulberry\"          \"pineapple\"        \n## [10] \"purple mangosteen\"\n## \n## $no_match\n##  [1] \"bell pepper\"  \"blackberry\"   \"blood orange\" \"cantaloupe\"   \"chili pepper\"\n##  [6] \"cloudberry\"   \"lime\"         \"lychee\"       \"olive\"        \"salal berry\"\nlist(match = intersect(matches, str_subset(fruit, pattern = \"le\")),\n     no_match = setdiff(matches, str_subset(fruit, pattern = \"le\")))## $match\n## [1] \"apple\"             \"clementine\"        \"huckleberry\"      \n## [4] \"lemon\"             \"pineapple\"         \"purple mangosteen\"\n## \n## $no_match\n##  [1] \"bell pepper\"  \"bilberry\"     \"blackberry\"   \"blood orange\" \"blueberry\"   \n##  [6] \"cantaloupe\"   \"chili pepper\" \"cloudberry\"   \"elderberry\"   \"lime\"        \n## [11] \"lychee\"       \"mulberry\"     \"olive\"        \"salal berry\""},{"path":"wrangling.html","id":"raw-strings","chapter":"2 Wrangling","heading":"2.2.6 Raw strings","text":"’ve probably caught now certain characters special meaning regexes,\nincluding $ * + . ? [ ] ^ { } | ( ) \\. makes things difficult want use characters within strings instead regexes. Previously, lot maneuvering create strings containing special characters, now can use r\"()\".\\ preceding quote known escaping. signals computer quotations part string. Now, happens backslash within string.can see, backslash used escape initial backslash.","code":"\nr\"(Do you use \"airquotes\" much?)\"## [1] \"Do you use \\\"airquotes\\\" much?\"\nr\"(\\)\"## [1] \"\\\\\""},{"path":"wrangling.html","id":"factors","chapter":"2 Wrangling","heading":"2.3 Factors","text":"Factors categorical variables may take specified set values. Thus, known categorical variables, can separated categories. manipulate factors use [forcats][forcats-web] package, core package tidyverse. Like stringr package whose functions begin str_, main functions forcats package start fct_.Get know factor start touching ! ’s polite. Let’s use gapminder$continent example.get frequency table tibble, tibble, use dplyr::count(). get similar result free-range factor, use forcats::fct_count().","code":"\nstr(gapminder$continent)##  Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\nlevels(gapminder$continent)## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\nnlevels(gapminder$continent)## [1] 5\nclass(gapminder$continent)## [1] \"factor\"\ngapminder %>% \n  count(continent)## # A tibble: 5 x 2\n##   continent     n\n##   <fct>     <int>\n## 1 Africa      624\n## 2 Americas    300\n## 3 Asia        396\n## 4 Europe      360\n## 5 Oceania      24\nfct_count(gapminder$continent)## # A tibble: 5 x 2\n##   f            n\n##   <fct>    <int>\n## 1 Africa     624\n## 2 Americas   300\n## 3 Asia       396\n## 4 Europe     360\n## 5 Oceania     24"},{"path":"wrangling.html","id":"dropping-unused-levels","chapter":"2 Wrangling","heading":"2.3.1 Dropping unused levels","text":"Just drop rows corresponding specific factor level, levels factor change. Sometimes unused levels can come back haunt later, e.g., figure legends.Watch happens levels country filter Gapminder handful countries.Even though h_gap data handful countries, still schlepping around levels original gapminder tibble.can get rid ? base function droplevels() operates factors data frame single factor. function forcats::fct_drop() operates factor.","code":"\nnlevels(gapminder$country)## [1] 142\nh_countries <- c(\"Egypt\", \"Haiti\", \"Romania\", \"Thailand\", \"Venezuela\")\nh_gap <- gapminder %>%\nfilter(country %in% h_countries)\nnlevels(h_gap$country)## [1] 142\nh_gap_dropped <- h_gap %>% \n  droplevels()\nnlevels(h_gap_dropped$country)## [1] 5\n# Use forcats::fct_drop() on a free-range factor\n\nh_gap$country %>%\n  fct_drop() %>%\n  levels()## [1] \"Egypt\"     \"Haiti\"     \"Romania\"   \"Thailand\"  \"Venezuela\""},{"path":"wrangling.html","id":"change-order-of-the-levels","chapter":"2 Wrangling","heading":"2.3.2 Change order of the levels","text":"default, factor levels ordered alphabetically. think , ordering might well random! preferable order levels according principle:Frequency. Make common level first .Another variable. Order factor levels according summary statistic another variable.First, let’s order continent frequency, forwards backwards. often great idea tables figures, esp. frequency barplots.two barcharts frequency continent differ order continents. prefer?Now order country another variable, forwards backwards. variable usually quantitative order factor according grouped summary. factor grouping variable default summarizing function median() can specify something else.reorder factor levels? often makes plots much better! factor mapped x y, almost always reordered quantitative variable mapping one.\nCompare interpretability two plots life expectancy Asian countries 2007. difference order country factor. one find easier learn ?Use fct_reorder2() line chart quantitative x another quantitative y factor provides color. way legend appears order data! Note, order taken right side plot (left). Contrast legend left one right.Sometimes just want hoist one levels front. ? said . resembles move variables front dplyr::select(special_var, everything()).might useful preparing report , say, Romanian government. reason always putting Romania first nothing data, important external reasons need way express .","code":"\n# Default order is alphabetical\n\ngapminder$continent %>%\n  levels()## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\n# Order by frequency\n\ngapminder$continent %>% \n  fct_infreq() %>%\n  levels()## [1] \"Africa\"   \"Asia\"     \"Europe\"   \"Americas\" \"Oceania\"\n# Backwards!\n\ngapminder$continent %>% \n  fct_infreq() %>%\n  fct_rev() %>% \n  levels()## [1] \"Oceania\"  \"Americas\" \"Europe\"   \"Asia\"     \"Africa\"\n# Order countries by median life expectancy\n\nfct_reorder(gapminder$country, gapminder$lifeExp) %>% \n  levels() %>% head()## [1] \"Sierra Leone\"  \"Guinea-Bissau\" \"Afghanistan\"   \"Angola\"       \n## [5] \"Somalia\"       \"Guinea\"\n# Order according to minimum life exp instead of median\n\nfct_reorder(gapminder$country, gapminder$lifeExp, min) %>% \n  levels() %>% head()## [1] \"Rwanda\"       \"Afghanistan\"  \"Gambia\"       \"Angola\"       \"Sierra Leone\"\n## [6] \"Cambodia\"\n# Backwards!\n\nfct_reorder(gapminder$country, gapminder$lifeExp, .desc = TRUE) %>% \n  levels() %>% head()## [1] \"Iceland\"     \"Japan\"       \"Sweden\"      \"Switzerland\" \"Netherlands\"\n## [6] \"Norway\"\ngap_asia_2007 <- gapminder %>% filter(year == 2007, continent == \"Asia\")\nggplot(gap_asia_2007, aes(x = lifeExp, y = country)) + geom_point()\nggplot(gap_asia_2007, aes(x = lifeExp, y = fct_reorder(country, lifeExp))) +\n  geom_point()\nh_countries <- c(\"Egypt\", \"Haiti\", \"Romania\", \"Thailand\", \"Venezuela\")\nh_gap <- gapminder %>%\n  filter(country %in% h_countries) %>% \n  droplevels()\nggplot(h_gap, aes(x = year, y = lifeExp, color = country)) +\n  geom_line()\nggplot(h_gap, aes(x = year, y = lifeExp,\n                  color = fct_reorder2(country, year, lifeExp))) +\n  geom_line() +\n  labs(color = \"country\")\nh_gap$country %>% \n  levels()## [1] \"Egypt\"     \"Haiti\"     \"Romania\"   \"Thailand\"  \"Venezuela\"\nh_gap$country %>% \n  fct_relevel(\"Romania\", \"Haiti\") %>% \n  levels()## [1] \"Romania\"   \"Haiti\"     \"Egypt\"     \"Thailand\"  \"Venezuela\""},{"path":"wrangling.html","id":"recode-the-levels","chapter":"2 Wrangling","heading":"2.3.3 Recode the levels","text":"Sometimes better ideas certain levels . called recoding.","code":"\ni_gap <- gapminder %>% \n  filter(country %in% c(\"United States\", \"Sweden\", \n                        \"Australia\")) %>% \n  droplevels()\n\ni_gap$country %>% \n  levels()## [1] \"Australia\"     \"Sweden\"        \"United States\"\ni_gap$country %>%\n  fct_recode(\"USA\" = \"United States\", \"Oz\" = \"Australia\") %>% \n  levels()## [1] \"Oz\"     \"Sweden\" \"USA\""},{"path":"wrangling.html","id":"grow-a-factor","chapter":"2 Wrangling","heading":"2.3.4 Grow a factor","text":"Let’s create two data frames, data two countries, dropping unused factor levels.country factors df1 df2 different levels.Can just combine ?Umm, . wrong many levels! Use fct_c() .","code":"\ndf1 <- gapminder %>%\n  filter(country %in% c(\"United States\", \"Mexico\"), year > 2000) %>%\n  droplevels()\ndf2 <- gapminder %>%\n  filter(country %in% c(\"France\", \"Germany\"), year > 2000) %>%\n  droplevels()\nlevels(df1$country)## [1] \"Mexico\"        \"United States\"\nlevels(df2$country)## [1] \"France\"  \"Germany\"\nc(df1$country, df2$country)## [1] 1 1 2 2 1 1 2 2\nfct_c(df1$country, df2$country)## [1] Mexico        Mexico        United States United States France       \n## [6] France        Germany       Germany      \n## Levels: Mexico United States France Germany"},{"path":"wrangling.html","id":"lists","chapter":"2 Wrangling","heading":"2.4 Lists","text":"Lists type vector step complexity atomic vectors, lists can contain lists. makes suitable representing hierarchical tree-like structures. create list list():useful tool working lists str() focuses structure, contents.Unlike atomic vectors, list() can contain mix objects:Lists can even contain lists!","code":"\nx <- list(1, 2, 3)\nx## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\nstr(x)## List of 3\n##  $ : num 1\n##  $ : num 2\n##  $ : num 3\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)## List of 3\n##  $ a: num 1\n##  $ b: num 2\n##  $ c: num 3\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)## List of 4\n##  $ : chr \"a\"\n##  $ : int 1\n##  $ : num 1.5\n##  $ : logi TRUE\nz <- list(list(1, 2), list(3, 4))\nstr(z)## List of 2\n##  $ :List of 2\n##   ..$ : num 1\n##   ..$ : num 2\n##  $ :List of 2\n##   ..$ : num 3\n##   ..$ : num 4"},{"path":"wrangling.html","id":"visualizing-lists","chapter":"2 Wrangling","heading":"2.4.1 Visualizing lists","text":"explain complicated list manipulation functions, ’s helpful visual representation lists. example, take three lists:’ll draw follows:three principles:Lists rounded corners. Atomic vectors square corners.Children drawn inside parent, slightly darker background make easier see hierarchy.orientation children (.e. rows columns) isn’t important, ’ll pick row column orientation either save space illustrate important property example.","code":"\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))"},{"path":"wrangling.html","id":"subsetting","chapter":"2 Wrangling","heading":"2.4.2 Subsetting","text":"three ways subset list, ’ll illustrate list named :[ extracts sub-list. result always list.Like vectors, can subset logical, integer, character vector.[[ extracts single component list. removes level hierarchy list.$ shorthand extracting named elements list. works similarly [[ except don’t need use quotes.distinction [ [[ really important lists, [[ drills list [ returns new, smaller list. Compare code output visual representation.\nFIGURE 2.2: Subsetting list, visually.\n","code":"\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\nstr(a[1:2])## List of 2\n##  $ a: int [1:3] 1 2 3\n##  $ b: chr \"a string\"\nstr(a[4])## List of 1\n##  $ d:List of 2\n##   ..$ : num -1\n##   ..$ : num -5\nstr(a[[1]])##  int [1:3] 1 2 3\nstr(a[[4]])## List of 2\n##  $ : num -1\n##  $ : num -5\na$a## [1] 1 2 3\na[[\"a\"]]## [1] 1 2 3"},{"path":"wrangling.html","id":"lists-of-condiments","chapter":"2 Wrangling","heading":"2.4.3 Lists of condiments","text":"difference [ [[ important, ’s easy get confused. help remember, let show unusual pepper shaker.pepper shaker list x, , x[1] pepper shaker containing single pepper packet:x[2] look , contain second packet. x[1:2] pepper shaker containing two pepper packets.x[[1]] :wanted get content pepper package, ’d need x[[1]][[1]]:","code":""},{"path":"wrangling.html","id":"date-times","chapter":"2 Wrangling","heading":"2.5 Date-Times","text":"manipulate date-times using lubridate package, makes easier work dates times R. lubridate part core tidyverse need ’re working dates/times.three types date/time data refer instant time:date. Tibbles print <date>.date. Tibbles print <date>.time within day. Tibbles print <time>.time within day. Tibbles print <time>.date-time date plus time: uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>. Elsewhere R called POSIXct, don’t think\n’s useful name.date-time date plus time: uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>. Elsewhere R called POSIXct, don’t think\n’s useful name.always use simplest possible data type works needs. means can use date instead date-time, . Date-times substantially complicated need handle time zones, ’ll come back end chapter.get current date date-time can use today() now():Otherwise, three ways ’re likely create date/time:string.individual date-time components.existing date/time object.work follows:","code":"\ntoday()## [1] \"2020-12-05\"\nnow()## [1] \"2020-12-05 19:15:30 EST\""},{"path":"wrangling.html","id":"from-strings","chapter":"2 Wrangling","heading":"2.5.1 From strings","text":"Date/time data often comes strings. lubridate functions automatically work format specify order component. use , identify order year, month, day appear dates, arrange “y,” “m,” “d” order. gives name lubridate function parse date. example:functions also take unquoted numbers. concise way create single date/time object, might need filtering date/time data. ymd() short unambiguous:ymd() friends create dates. create date-time, add underscore one “h,” “m,” “s” name parsing function:can also force creation date-time date supplying timezone:","code":"\nymd(\"2017-01-31\")## [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")## [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")## [1] \"2017-01-31\"\nymd(20170131)## [1] \"2017-01-31\"\nymd_hms(\"2017-01-31 20:11:59\")## [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")## [1] \"2017-01-31 08:01:00 UTC\"\nymd(20170131, tz = \"UTC\")## [1] \"2017-01-31 UTC\""},{"path":"wrangling.html","id":"from-individual-components","chapter":"2 Wrangling","heading":"2.5.2 From individual components","text":"Instead single string, sometimes ’ll individual components date-time spread across multiple columns. flights data:create date/time sort input, use make_date() dates, make_datetime() date-times:Let’s thing four time columns flights. times represented slightly odd format, use modulus arithmetic pull hour minute components. ’ve created date-time variables, focus variables ’ll explore rest chapter.data, can visualise distribution departure times across year:within single day:Note use date-times numeric context (like histogram), 1 means 1 second, binwidth 86400 means one day. dates, 1 means 1 day.","code":"\nflights %>% \n  select(year, month, day, hour, minute)## # A tibble: 336,776 x 5\n##     year month   day  hour minute\n##    <int> <int> <int> <dbl>  <dbl>\n##  1  2013     1     1     5     15\n##  2  2013     1     1     5     29\n##  3  2013     1     1     5     40\n##  4  2013     1     1     5     45\n##  5  2013     1     1     6      0\n##  6  2013     1     1     5     58\n##  7  2013     1     1     6      0\n##  8  2013     1     1     6      0\n##  9  2013     1     1     6      0\n## 10  2013     1     1     6      0\n## # … with 336,766 more rows\nflights %>% \n  select(year, month, day, hour, minute) %>% \n  mutate(departure = make_datetime(year, month, day, hour, minute))## # A tibble: 336,776 x 6\n##     year month   day  hour minute departure          \n##    <int> <int> <int> <dbl>  <dbl> <dttm>             \n##  1  2013     1     1     5     15 2013-01-01 05:15:00\n##  2  2013     1     1     5     29 2013-01-01 05:29:00\n##  3  2013     1     1     5     40 2013-01-01 05:40:00\n##  4  2013     1     1     5     45 2013-01-01 05:45:00\n##  5  2013     1     1     6      0 2013-01-01 06:00:00\n##  6  2013     1     1     5     58 2013-01-01 05:58:00\n##  7  2013     1     1     6      0 2013-01-01 06:00:00\n##  8  2013     1     1     6      0 2013-01-01 06:00:00\n##  9  2013     1     1     6      0 2013-01-01 06:00:00\n## 10  2013     1     1     6      0 2013-01-01 06:00:00\n## # … with 336,766 more rows\nmake_datetime_100 <- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\nflights_dt <- flights %>% \n  filter(!is.na(dep_time), !is.na(arr_time)) %>% \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) %>% \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\nflights_dt## # A tibble: 328,063 x 9\n##    origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n##    <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n##  1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n##  2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n##  3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n##  4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n##  5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n##  6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n##  7 EWR    FLL          -5        19 2013-01-01 05:55:00 2013-01-01 06:00:00\n##  8 LGA    IAD          -3       -14 2013-01-01 05:57:00 2013-01-01 06:00:00\n##  9 JFK    MCO          -3        -8 2013-01-01 05:57:00 2013-01-01 06:00:00\n## 10 LGA    ORD          -2         8 2013-01-01 05:58:00 2013-01-01 06:00:00\n## # … with 328,053 more rows, and 3 more variables: arr_time <dttm>,\n## #   sched_arr_time <dttm>, air_time <dbl>\nflights_dt %>% \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\nflights_dt %>% \n  filter(dep_time < ymd(20130102)) %>% \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes"},{"path":"wrangling.html","id":"from-other-types","chapter":"2 Wrangling","heading":"2.5.3 From other types","text":"may want switch date-time date. ’s job as_datetime() as_date():Sometimes ’ll get date/times numeric offsets “Unix Epoch,” 1970-01-01. offset seconds, use as_datetime(); ’s days, use as_date().","code":"\nas_datetime(today())## [1] \"2020-12-05 UTC\"\nas_date(now())## [1] \"2020-12-05\"\nas_datetime(60 * 60 * 10)## [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)## [1] \"1980-01-01\""},{"path":"wrangling.html","id":"date-time-components","chapter":"2 Wrangling","heading":"2.5.4 Date-time components","text":"Now know get date-time data R’s date-time data structures, let’s explore can . section focus accessor functions let get set individual components. next section look arithmetic works date-times.can pull individual parts date accessor functions year(), month(), mday() (day month), yday() (day year), wday() (day week), hour(), minute(), second().month() wday() can set label = TRUE return abbreviated name month day week. Set abbr = FALSE return full name.can use wday() see flights depart week weekend:’s interesting pattern look average departure delay minute within hour. looks like flights leaving minutes 20-30 50-60 much lower delays rest hour!Interestingly, look scheduled departure time don’t see strong pattern:see pattern actual departure times? Well, like much data collected humans, ’s strong bias towards flights leaving “nice” departure times. Always alert sort pattern whenever work data involves human judgement!","code":"\ndatetime <- ymd_hms(\"2016-07-08 12:34:56\")\nyear(datetime)## [1] 2016\nmonth(datetime)## [1] 7\nmday(datetime)## [1] 8\nyday(datetime)## [1] 190\nwday(datetime)## [1] 6\nmonth(datetime, label = TRUE)## [1] Jul\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nwday(datetime, label = TRUE, abbr = FALSE)## [1] Friday\n## 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\nflights_dt %>% \n  mutate(wday = wday(dep_time, label = TRUE)) %>% \n  ggplot(aes(x = wday)) +\n    geom_bar()\nflights_dt %>% \n  mutate(minute = minute(dep_time)) %>% \n  group_by(minute) %>% \n  summarise(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n()) %>% \n  ggplot(aes(minute, avg_delay)) +\n    geom_line()## `summarise()` ungrouping output (override with `.groups` argument)\nsched_dep <- flights_dt %>% \n  mutate(minute = minute(sched_dep_time)) %>% \n  group_by(minute) %>% \n  summarise(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n())## `summarise()` ungrouping output (override with `.groups` argument)\nggplot(sched_dep, aes(minute, avg_delay)) +\n  geom_line()\nggplot(sched_dep, aes(minute, n)) +\n  geom_line()"},{"path":"wrangling.html","id":"setting-components","chapter":"2 Wrangling","heading":"2.5.5 Setting components","text":"can create new date-time update().values big, roll-:can use update() show distribution flights across course day every day year:Setting larger components date constant powerful technique allows explore patterns smaller components.","code":"\nupdate(datetime, year = 2020, month = 2, mday = 2, hour = 2)## [1] \"2020-02-02 02:34:56 UTC\"\nymd(\"2015-02-01\") %>% \n  update(mday = 30)## [1] \"2015-03-02\"\nymd(\"2015-02-01\") %>% \n  update(hour = 400)## [1] \"2015-02-17 16:00:00 UTC\"\nflights_dt %>% \n  mutate(dep_hour = update(dep_time, yday = 1)) %>% \n  ggplot(aes(dep_hour)) +\n    geom_freqpoly(binwidth = 300)"},{"path":"wrangling.html","id":"time-zones","chapter":"2 Wrangling","heading":"2.5.6 Time zones","text":"Time zones enormously complicated topic interaction geopolitical entities. Fortunately don’t need dig details ’re important data analysis. can see complete list possible timezones function OlsonNames(). Unless otherwise specified, lubridate always uses UTC (Coordinated Universal Time).R, time zone attribute date-time controls printing. example, three objects represent instant time:","code":"\n(x1 <- ymd_hms(\"2015-06-01 12:00:00\", tz = \"America/New_York\"))## [1] \"2015-06-01 12:00:00 EDT\"\n(x2 <- ymd_hms(\"2015-06-01 18:00:00\", tz = \"Europe/Copenhagen\"))## [1] \"2015-06-01 18:00:00 CEST\"\n(x3 <- ymd_hms(\"2015-06-02 04:00:00\", tz = \"Pacific/Auckland\"))## [1] \"2015-06-02 04:00:00 NZST\""},{"path":"wrangling.html","id":"combining-data","chapter":"2 Wrangling","heading":"2.6 Combining Data","text":"many ways bring data together.Bind - basically smashing rocks tibbles together. can smash things together row-wise (“row binding”) column-wise (“column binding”). characterize rock-smashing? ’re often fairly crude operations, lots responsibility falling analyst making sure whole enterprise even makes sense.row binding, need consider variables two tibbles. variables exist ? type? Different approaches row binding different combinations flexibility vs rigidity around matters.column binding, onus entirely analyst make sure rows aligned. avoid column binding whenever possible. can introduce new variables , safer means, ! safer, mean: use mechanism row alignment correct definition. proper join gold standard.Join - designate variable (combination variables) key. row one data frame gets matched row another data frame key. can bring information variables secondary data frame primary data frame based key-based lookup. description incredibly oversimplified, ’s basic idea.variety row- column-wise operations fit framework, implies many different flavors join. concepts vocabulary around joins come database world. relevant functions dplyr follow convention mention join. relevant base R function merge().Let’s explore type operation examples.","code":""},{"path":"wrangling.html","id":"row-binding","chapter":"2 Wrangling","heading":"2.6.1 Row binding","text":"’s perfect row bind three (untidy!) data frames looks like using data Lord Rings trilogy.dplyr::bind_rows() works like charm row-bindable data frames. one data frames somehow missing variable? Let’s mangle one find .see dplyr::bind_rows() row bind puts NA missing values caused lack Female data Two Towers. Nonetheless, can problematic dissimilar datasets.","code":"\nfship <- tribble(\n                         ~Film,    ~Race, ~Female, ~Male,\n  \"The Fellowship Of The Ring\",    \"Elf\",    1229,   971,\n  \"The Fellowship Of The Ring\", \"Hobbit\",      14,  3644,\n  \"The Fellowship Of The Ring\",    \"Man\",       0,  1995\n)\nrking <- tribble(\n                         ~Film,    ~Race, ~Female, ~Male,\n      \"The Return Of The King\",    \"Elf\",     183,   510,\n      \"The Return Of The King\", \"Hobbit\",       2,  2673,\n      \"The Return Of The King\",    \"Man\",     268,  2459\n)\nttow <- tribble(\n                         ~Film,    ~Race, ~Female, ~Male,\n              \"The Two Towers\",    \"Elf\",     331,   513,\n              \"The Two Towers\", \"Hobbit\",       0,  2463,\n              \"The Two Towers\",    \"Man\",     401,  3589\n)\n(lotr_untidy <- bind_rows(fship, ttow, rking))## # A tibble: 9 x 4\n##   Film                       Race   Female  Male\n##   <chr>                      <chr>   <dbl> <dbl>\n## 1 The Fellowship Of The Ring Elf      1229   971\n## 2 The Fellowship Of The Ring Hobbit     14  3644\n## 3 The Fellowship Of The Ring Man         0  1995\n## 4 The Two Towers             Elf       331   513\n## 5 The Two Towers             Hobbit      0  2463\n## 6 The Two Towers             Man       401  3589\n## 7 The Return Of The King     Elf       183   510\n## 8 The Return Of The King     Hobbit      2  2673\n## 9 The Return Of The King     Man       268  2459\nttow_no_Female <- ttow %>% mutate(Female = NULL)\nbind_rows(fship, ttow_no_Female, rking)## # A tibble: 9 x 4\n##   Film                       Race   Female  Male\n##   <chr>                      <chr>   <dbl> <dbl>\n## 1 The Fellowship Of The Ring Elf      1229   971\n## 2 The Fellowship Of The Ring Hobbit     14  3644\n## 3 The Fellowship Of The Ring Man         0  1995\n## 4 The Two Towers             Elf        NA   513\n## 5 The Two Towers             Hobbit     NA  2463\n## 6 The Two Towers             Man        NA  3589\n## 7 The Return Of The King     Elf       183   510\n## 8 The Return Of The King     Hobbit      2  2673\n## 9 The Return Of The King     Man       268  2459\nrbind(fship, ttow_no_Female, rking)## Error in rbind(deparse.level, ...): numbers of columns of arguments do not match"},{"path":"wrangling.html","id":"column-binding","chapter":"2 Wrangling","heading":"2.6.2 Column binding","text":"\nFIGURE 2.3: Attempting bind columns correctly\nColumn binding much dangerous often “works” . ’s job make sure rows aligned ’s easy screw .data gapminder originally excavated 3 messy Excel spreadsheets: one life expectancy, population, GDP per capital. Let’s relive data wrangling joy show column bind gone wrong.create 3 separate data frames, evil row sorting, column bind. errors. result gapminder_garbage sort looks OK. Univariate summary statistics exploratory plots look OK. ’ve created complete nonsense!One last cautionary tale column binding. one requires use cbind() ’s tidyverse generally unwilling recycle combining things different length.create tibble gapminder columns. create another remainder, filtered just one country. able cbind() objects! ? 12 rows Canada divide evenly 1704 rows gapminder. Note dplyr::bind_cols() refuses column bind .data frame isn’t obviously wrong, wrong. See Canada’s population GDP per capita repeat country?Bottom line: Row bind need , inspect results re: coercion. Column bind must extremely paranoid.","code":"\nlife_exp <- gapminder %>%\n  select(country, year, lifeExp)\n\npop <- gapminder %>%\n  arrange(year) %>% \n  select(pop)\n  \ngdp_percap <- gapminder %>% \n  arrange(pop) %>% \n  select(gdpPercap)\n\n(gapminder_garbage <- bind_cols(life_exp, pop, gdp_percap))## # A tibble: 1,704 x 5\n##    country      year lifeExp      pop gdpPercap\n##    <fct>       <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan  1952    28.8  8425333      880.\n##  2 Afghanistan  1957    30.3  1282697      861.\n##  3 Afghanistan  1962    32.0  9279525     2670.\n##  4 Afghanistan  1967    34.0  4232095     1072.\n##  5 Afghanistan  1972    36.1 17876956     1385.\n##  6 Afghanistan  1977    38.4  8691212     2865.\n##  7 Afghanistan  1982    39.9  6927772     1533.\n##  8 Afghanistan  1987    40.8   120447     1738.\n##  9 Afghanistan  1992    41.7 46886859     3021.\n## 10 Afghanistan  1997    41.8  8730405     1890.\n## # … with 1,694 more rows\nsummary(gapminder$lifeExp)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##      24      48      61      59      71      83\nsummary(gapminder_garbage$lifeExp)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##      24      48      61      59      71      83\nrange(gapminder$gdpPercap)## [1]    241 113523\nrange(gapminder_garbage$gdpPercap)## [1]    241 113523\ngapminder_mostly <- gapminder %>% select(-pop, -gdpPercap)\ngapminder_leftovers_filtered <- gapminder %>% \n  filter(country == \"Canada\") %>% \n  select(pop, gdpPercap)\n\ngapminder_nonsense <- cbind(gapminder_mostly, gapminder_leftovers_filtered)\nhead(gapminder_nonsense, 14)##        country continent year lifeExp      pop gdpPercap\n## 1  Afghanistan      Asia 1952      29 14785584     11367\n## 2  Afghanistan      Asia 1957      30 17010154     12490\n## 3  Afghanistan      Asia 1962      32 18985849     13462\n## 4  Afghanistan      Asia 1967      34 20819767     16077\n## 5  Afghanistan      Asia 1972      36 22284500     18971\n## 6  Afghanistan      Asia 1977      38 23796400     22091\n## 7  Afghanistan      Asia 1982      40 25201900     22899\n## 8  Afghanistan      Asia 1987      41 26549700     26627\n## 9  Afghanistan      Asia 1992      42 28523502     26343\n## 10 Afghanistan      Asia 1997      42 30305843     28955\n## 11 Afghanistan      Asia 2002      42 31902268     33329\n## 12 Afghanistan      Asia 2007      44 33390141     36319\n## 13     Albania    Europe 1952      55 14785584     11367\n## 14     Albania    Europe 1957      59 17010154     12490"},{"path":"wrangling.html","id":"joins-in-dplyr","chapter":"2 Wrangling","heading":"2.6.3 Joins in dplyr","text":"recent release gapminder includes new data frame, country_codes, country names ISO codes. Therefore can also use practice joins.Join (.k.. merge) two tables: dplyr join cheatsheet comic characters publishers.Working two small data frames: superheroes publishers.Sorry, cheat sheet illustrate “multiple match” situations terribly well.Sub-plot: watch row variable order join results healthy reminder ’s dangerous rely analysis.","code":"\ngapminder %>% \n  select(country, continent) %>% \n  group_by(country) %>% \n  slice(1) %>% \n  left_join(country_codes)## Joining, by = \"country\"## # A tibble: 142 x 4\n## # Groups:   country [142]\n##    country     continent iso_alpha iso_num\n##    <chr>       <fct>     <chr>       <int>\n##  1 Afghanistan Asia      AFG             4\n##  2 Albania     Europe    ALB             8\n##  3 Algeria     Africa    DZA            12\n##  4 Angola      Africa    AGO            24\n##  5 Argentina   Americas  ARG            32\n##  6 Australia   Oceania   AUS            36\n##  7 Austria     Europe    AUT            40\n##  8 Bahrain     Asia      BHR            48\n##  9 Bangladesh  Asia      BGD            50\n## 10 Belgium     Europe    BEL            56\n## # … with 132 more rows\nsuperheroes <- tibble::tribble(\n       ~name, ~alignment,  ~gender,          ~publisher,\n   \"Magneto\",      \"bad\",   \"male\",            \"Marvel\",\n     \"Storm\",     \"good\", \"female\",            \"Marvel\",\n  \"Mystique\",      \"bad\", \"female\",            \"Marvel\",\n    \"Batman\",     \"good\",   \"male\",                \"DC\",\n     \"Joker\",      \"bad\",   \"male\",                \"DC\",\n  \"Catwoman\",      \"bad\", \"female\",                \"DC\",\n   \"Hellboy\",     \"good\",   \"male\", \"Dark Horse Comics\"\n  )\n\npublishers <- tibble::tribble(\n  ~publisher, ~yr_founded,\n        \"DC\",       1934L,\n    \"Marvel\",       1939L,\n     \"Image\",       1992L\n  )"},{"path":"wrangling.html","id":"inner_join","chapter":"2 Wrangling","heading":"2.6.3.1 inner_join()","text":"inner_join(x, y): Return rows x matching values y, columns x y. multiple matches x y, combination matches returned. mutating join.\nFIGURE 2.4: Inner join.\nlose Hellboy join , although appears x = superheroes, publisher Dark Horse Comics appear y = publishers. join result variables x = superheroes plus yr_founded, y.Now compare result using inner_join() two datasets opposite positions.way, illustrate multiple matches, think x = publishers direction. Every publisher match y = superheroes appears multiple times result, match. fact, ’re getting result inner_join(superheroes, publishers), variable order (also never rely analysis).inner_join() solve nearly problems ’ll encounter book.","code":"\n(ijsp <- inner_join(superheroes, publishers))## Joining, by = \"publisher\"## # A tibble: 6 x 5\n##   name     alignment gender publisher yr_founded\n##   <chr>    <chr>     <chr>  <chr>          <int>\n## 1 Magneto  bad       male   Marvel          1939\n## 2 Storm    good      female Marvel          1939\n## 3 Mystique bad       female Marvel          1939\n## 4 Batman   good      male   DC              1934\n## 5 Joker    bad       male   DC              1934\n## 6 Catwoman bad       female DC              1934\n(ijps <- inner_join(publishers, superheroes))## Joining, by = \"publisher\"## # A tibble: 6 x 5\n##   publisher yr_founded name     alignment gender\n##   <chr>          <int> <chr>    <chr>     <chr> \n## 1 DC              1934 Batman   good      male  \n## 2 DC              1934 Joker    bad       male  \n## 3 DC              1934 Catwoman bad       female\n## 4 Marvel          1939 Magneto  bad       male  \n## 5 Marvel          1939 Storm    good      female\n## 6 Marvel          1939 Mystique bad       female"},{"path":"wrangling.html","id":"full_join","chapter":"2 Wrangling","heading":"2.6.3.2 full_join()","text":"full_join(x, y): Return rows columns x y. matching values, returns NA one missing. mutating join.get rows x = superheroes plus new row y = publishers, containing publisher Image. get variables x = superheroes variables y = publishers. row derives solely one table carries NAs variables found table.full_join() returns rows columns x y result full_join(x = superheroes, y = publishers) match full_join(x = publishers, y = superheroes).","code":"\n(fjsp <- full_join(superheroes, publishers))## Joining, by = \"publisher\"## # A tibble: 8 x 5\n##   name     alignment gender publisher         yr_founded\n##   <chr>    <chr>     <chr>  <chr>                  <int>\n## 1 Magneto  bad       male   Marvel                  1939\n## 2 Storm    good      female Marvel                  1939\n## 3 Mystique bad       female Marvel                  1939\n## 4 Batman   good      male   DC                      1934\n## 5 Joker    bad       male   DC                      1934\n## 6 Catwoman bad       female DC                      1934\n## 7 Hellboy  good      male   Dark Horse Comics         NA\n## 8 <NA>     <NA>      <NA>   Image                   1992"},{"path":"wrangling.html","id":"left_join","chapter":"2 Wrangling","heading":"2.6.3.3 left_join()","text":"left_join(x, y): Return rows x, columns x y. multiple matches x y, combination matches returned. mutating join.basically get x = superheroes back, addition variable yr_founded, unique y = publishers. Hellboy, whose publisher appear y = publishers, NA yr_founded.Now compare result running left_join(x = publishers, y = superheroes). Unlike inner_join() full_join() order arguments significant effect resulting dataframe.get similar result inner_join() publisher Image survives join, even though superheroes Image appear y = superheroes. result, Image NAs name, alignment, gender.similar function, right_join(x, y) return rows y, columns x y. Like left_join(), mutating join.","code":"\n(ljsp <- left_join(superheroes, publishers))## Joining, by = \"publisher\"## # A tibble: 7 x 5\n##   name     alignment gender publisher         yr_founded\n##   <chr>    <chr>     <chr>  <chr>                  <int>\n## 1 Magneto  bad       male   Marvel                  1939\n## 2 Storm    good      female Marvel                  1939\n## 3 Mystique bad       female Marvel                  1939\n## 4 Batman   good      male   DC                      1934\n## 5 Joker    bad       male   DC                      1934\n## 6 Catwoman bad       female DC                      1934\n## 7 Hellboy  good      male   Dark Horse Comics         NA\n(ljps <- left_join(publishers, superheroes))## Joining, by = \"publisher\"## # A tibble: 7 x 5\n##   publisher yr_founded name     alignment gender\n##   <chr>          <int> <chr>    <chr>     <chr> \n## 1 DC              1934 Batman   good      male  \n## 2 DC              1934 Joker    bad       male  \n## 3 DC              1934 Catwoman bad       female\n## 4 Marvel          1939 Magneto  bad       male  \n## 5 Marvel          1939 Storm    good      female\n## 6 Marvel          1939 Mystique bad       female\n## 7 Image           1992 <NA>     <NA>      <NA>"},{"path":"wrangling.html","id":"semi_join","chapter":"2 Wrangling","heading":"2.6.3.4 semi_join()","text":"semi_join(x, y): Return rows x matching values y, keeping just columns x. semi join differs inner join inner join return one row x matching row y, semi join never duplicate rows x. filtering join.get similar result inner_join() join result contains variables originally found x = superheroes.Now compare result switching values arguments.Now effects switching x y roles clear. result resembles x = publishers, publisher Image lost, observations publisher == \"Image\" y = superheroes.","code":"\n(sjsp <- semi_join(superheroes, publishers))## Joining, by = \"publisher\"## # A tibble: 6 x 4\n##   name     alignment gender publisher\n##   <chr>    <chr>     <chr>  <chr>    \n## 1 Magneto  bad       male   Marvel   \n## 2 Storm    good      female Marvel   \n## 3 Mystique bad       female Marvel   \n## 4 Batman   good      male   DC       \n## 5 Joker    bad       male   DC       \n## 6 Catwoman bad       female DC\n(sjps <- semi_join(x = publishers, y = superheroes))## Joining, by = \"publisher\"## # A tibble: 2 x 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 DC              1934\n## 2 Marvel          1939"},{"path":"wrangling.html","id":"anti_joinsuperheroes-publishers","chapter":"2 Wrangling","heading":"2.6.3.5 anti_join(superheroes, publishers)","text":"anti_join(x, y): Return rows x matching values y, keeping just columns x. filtering join.keep Hellboy now (get yr_founded).Now switch arguments compare result.keep publisher Image now (variables found x = publishers).","code":"\n(ajsp <- anti_join(superheroes, publishers))## Joining, by = \"publisher\"## # A tibble: 1 x 4\n##   name    alignment gender publisher        \n##   <chr>   <chr>     <chr>  <chr>            \n## 1 Hellboy good      male   Dark Horse Comics\n(ajps <- anti_join(publishers, superheroes))## Joining, by = \"publisher\"## # A tibble: 1 x 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 Image           1992"},{"path":"wrangling.html","id":"join-data-frames-with-key-variables","chapter":"2 Wrangling","heading":"2.6.4 Join data frames with “key” variables","text":"“Joining” “merging” two different datasets tricky stuff. Let’s go examples reviewing basic concepts. flights data frame, variable carrier lists carrier code different flights. corresponding airline names \"UA\" \"AA\" might somewhat easy guess (United American Airlines), airlines codes \"VX\", \"HA\", \"B6\"? information provided separate data frame airlines.see airports, carrier carrier code, name full name airline company. Using table, can see \"VX\", \"HA\", \"B6\" correspond Virgin America, Hawaiian Airlines, JetBlue, respectively. However, wouldn’t nice information single data frame instead two separate data frames? can “joining” flights airlines data frames.Note values variable carrier flights data frame match values variable carrier airlines data frame. case, can use variable carrier key variable match rows two data frames. Key variables almost always identification variables uniquely identify observational units. ensures rows data frames appropriately matched join. Grolemund Wickham (2017) created following diagram help us understand different data frames nycflights13 package linked various key variables:\nFIGURE 2.5: Relationships among nycflights tables\n","code":"\nView(airlines)"},{"path":"wrangling.html","id":"matching-key-variable-names","chapter":"2 Wrangling","heading":"2.6.4.1 Matching “key” variable names","text":"flights airlines data frames, key variable want join/merge/match rows name: carrier. Let’s use inner_join() function join two data frames, rows matched variable carrier, compare resulting data frames:Observe flights flights_joined data frames identical except flights_joined additional variable name. values name correspond airline companies’ names indicated airlines data frame.Say instead interested destinations domestic flights departing NYC 2013, ask questions like: “cities airports ?” “\"ORD\" Orlando?” “\"FLL\"?”airports data frame contains airport codes airport:However, look airports flights data frames, ’ll find airport codes variables different names. airports airport code faa, whereas flights airport codes origin dest.order join two data frames airport code, inner_join() operation use = c(\"dest\" = \"faa\") argument modified code syntax allowing us join two data frames key variable different name:Let’s construct chain pipe operators %>% computes number flights NYC destination, also includes information destination airport:case didn’t know, \"ORD\" airport code Chicago O’Hare airport \"FLL\" main airport Fort Lauderdale, Florida, can seen airport_name variable.","code":"\nflights_joined <- flights %>% \n  inner_join(airlines, by = \"carrier\")\nView(flights)\nView(flights_joined)\nView(airports)\nflights_with_airport_names <- flights %>% \n  inner_join(airports, by = c(\"dest\" = \"faa\"))\nView(flights_with_airport_names)\nnamed_dests <- flights %>%\n  group_by(dest) %>%\n  summarize(num_flights = n(),\n            .groups = \"drop\") %>%\n  arrange(desc(num_flights)) %>%\n  inner_join(airports, by = c(\"dest\" = \"faa\")) %>%\n  rename(airport_name = name)\nnamed_dests## # A tibble: 101 x 9\n##    dest  num_flights airport_name          lat    lon   alt    tz dst   tzone   \n##    <chr>       <int> <chr>               <dbl>  <dbl> <dbl> <dbl> <chr> <chr>   \n##  1 ORD         17283 Chicago Ohare Intl   42.0  -87.9   668    -6 A     America…\n##  2 ATL         17215 Hartsfield Jackson…  33.6  -84.4  1026    -5 A     America…\n##  3 LAX         16174 Los Angeles Intl     33.9 -118.    126    -8 A     America…\n##  4 BOS         15508 General Edward Law…  42.4  -71.0    19    -5 A     America…\n##  5 MCO         14082 Orlando Intl         28.4  -81.3    96    -5 A     America…\n##  6 CLT         14064 Charlotte Douglas …  35.2  -80.9   748    -5 A     America…\n##  7 SFO         13331 San Francisco Intl   37.6 -122.     13    -8 A     America…\n##  8 FLL         12055 Fort Lauderdale Ho…  26.1  -80.2     9    -5 A     America…\n##  9 MIA         11728 Miami Intl           25.8  -80.3     8    -5 A     America…\n## 10 DCA          9705 Ronald Reagan Wash…  38.9  -77.0    15    -5 A     America…\n## # … with 91 more rows"},{"path":"wrangling.html","id":"multiple-key-variables","chapter":"2 Wrangling","heading":"2.6.4.2 Multiple “key” variables","text":"Say instead want join two data frames multiple key variables. example, see order join flights weather data frames, need one key variable: year, month, day, hour, origin. combination 5 variables act uniquely identify observational unit weather data frame: hourly weather recordings 3 NYC airports.achieve specifying vector key variables join using c() function. Recall c() short “combine” “concatenate.”","code":"\nflights_weather_joined <- flights %>%\n  inner_join(weather, by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\nView(flights_weather_joined)"},{"path":"wrangling.html","id":"tidy-data","chapter":"2 Wrangling","heading":"2.7 “Tidy” data","text":"Now, explore topic “tidy” data, manner data formatting particularly suited creation graphics manipulation dataframe. see data stored “tidy” format just everyday definition term “tidy” might suggest: data “neatly organized.” Instead, define term “tidy” ’s used data scientists use R, outlining set rules data saved.Knowledge type data formatting necessary treatment data visualization Chapter 1 previous data wrangling topics. data used already “tidy” format. chapter, ’ll now see format essential using tools covered now. Furthermore, also useful subsequent chapters book cover regression statistical inference.Let’s switch gears learn concept “tidy” data format motivating example fivethirtyeight package. fivethirtyeight package provides access datasets used many articles published data journalism website, FiveThirtyEight.com.Let’s focus attention drinks data frame look first 5 rows:reading help file running ?drinks, ’ll see drinks data frame containing results survey average number servings beer, spirits, wine consumed 193 countries. data originally reported FiveThirtyEight.com Mona Chalabi’s article: “Dear Mona Followup: People Drink Beer, Wine Spirits?”.Let’s apply data wrangling verbs drinks data frame:filter() drinks data frame consider 4 countries: United States, China, Italy, Saudi Arabia, thenselect() columns except total_litres_of_pure_alcohol using - sign, thenrename() variables beer_servings, spirit_servings, wine_servings beer, spirit, wine, respectively.save resulting data frame drinks_smaller:Let’s now ask question: \"Using drinks_smaller data frame, create side--side barplot .Let’s break graphic:categorical variable country four levels (China, Italy, Saudi Arabia, USA) mapped x-position bars.numerical variable servings mapped y-position bars (height bars).categorical variable type three levels (beer, spirit, wine) mapped fill color bars.Observe, however, drinks_smaller three separate variables beer, spirit, wine. order use ggplot() function recreate barplot need single variable type three possible values: beer, spirit, wine. map type variable fill aesthetic plot. words, recreate barplot, data frame look like :Observe drinks_smaller drinks_smaller_tidy rectangular shape contain 12 numerical values (3 alcohol types 4 countries), formatted differently. drinks_smaller formatted ’s known “wide” format, whereas drinks_smaller_tidy formatted ’s known “long/narrow” format.context data science R, long/narrow format also known “tidy” format. order use ggplot2 dplyr packages data visualization data wrangling, input data frames must “tidy” format. Thus, non-“tidy” data must converted “tidy” format first. convert non-“tidy” data frames like drinks_smaller “tidy” data frames like drinks_smaller_tidy, let’s define “tidy” data.","code":"## # A tibble: 5 x 5\n##   country    beer_servings spirit_servings wine_servings total_litres_of_pure_a…\n##   <chr>              <int>           <int>         <int>                   <dbl>\n## 1 Afghanist…             0               0             0                     0  \n## 2 Albania               89             132            54                     4.9\n## 3 Algeria               25               0            14                     0.7\n## 4 Andorra              245             138           312                    12.4\n## 5 Angola               217              57            45                     5.9\ndrinks_smaller <- drinks %>%\n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %>%\n  select(-total_litres_of_pure_alcohol) %>%\n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller## # A tibble: 4 x 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84\ndrinks_smaller_tidy <- drinks_smaller %>% \n  pivot_longer(cols = -country, names_to = \"type\", values_to = \"servings\")\n\ndrinks_smaller_tidy_plot <- ggplot(drinks_smaller_tidy, \n                                   aes(x = country, y = servings, fill = type)) + geom_col(position = \"dodge\") + \n                              labs(x = \"country\", y = \"servings\")\ndrinks_smaller_tidy## # A tibble: 12 x 3\n##    country      type   servings\n##    <chr>        <chr>     <int>\n##  1 China        beer         79\n##  2 China        spirit      192\n##  3 China        wine          8\n##  4 Italy        beer         85\n##  5 Italy        spirit       42\n##  6 Italy        wine        237\n##  7 Saudi Arabia beer          0\n##  8 Saudi Arabia spirit        5\n##  9 Saudi Arabia wine          0\n## 10 USA          beer        249\n## 11 USA          spirit      158\n## 12 USA          wine         84"},{"path":"wrangling.html","id":"definition-of-tidy-data","chapter":"2 Wrangling","heading":"2.7.1 Definition of “tidy” data","text":"mean data “tidy?” “tidy” clear English meaning “organized,” word “tidy” data science using R means data follows standardized format. follow Hadley Wickham’s definition “tidy” data:dataset collection values, usually either numbers (quantitative) strings AKA text data (qualitative/categorical). Values organised two ways. Every value belongs variable observation. variable contains values measure underlying attribute (like height, temperature, duration) across units. observation contains values measured unit (like person, day, city) across attributes.“Tidy” data standard way mapping meaning dataset structure. dataset messy tidy depending rows, columns tables matched observations, variables types. tidy data:variable forms column.observation forms row.type observational unit forms table.","code":""},{"path":"wrangling.html","id":"converting-to-tidy-data","chapter":"2 Wrangling","heading":"2.7.2 Converting to “tidy” data","text":"book far, ’ve seen data frames already “tidy” format. Furthermore, rest book, ’ll mostly see data frames already “tidy” format well. always case however datasets world. original data frame wide (non-“tidy”) format like use ggplot2 dplyr packages, first convert “tidy” format. , recommend using pivot_longer() function tidyr package (Wickham 2020d).Going back drinks_smaller data frame earlier:convert “tidy” format using pivot_longer() function tidyr package follows:set arguments pivot_longer() follows:names_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .values_to name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric values beer, wine, spirit columns drinks_smaller data corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols little nuanced, let’s consider code ’s written slightly differently produces output:Note third argument now specifies columns want “tidy” c(beer, spirit, wine), instead columns don’t want “tidy” using -country. use c() function create vector columns drinks_smaller ’d like “tidy.” Note since three columns appear one another drinks_smaller data frame, also following cols argument:drinks_smaller_tidy “tidy” formatted data frame, can now produce barplot saw previously using geom_col(). Recall previous chapter use geom_col() geom_bar(), since like map “pre-counted” servings variable y-aesthetic bars.\nFIGURE 2.6: Comparing alcohol consumption 4 countries using geom_col().\nConverting “wide” format data “tidy” format often confuses new R users. way learn get comfortable pivot_longer() function practice, practice, practice using different datasets. example, run ?pivot_longer look examples bottom help file.however want convert “tidy” data frame “wide” format, need use pivot_wider() function instead. Run ?pivot_wider look examples bottom help file examples.can also view examples pivot_longer() pivot_wider() tidyverse.org webpage. ’s nice example check different functions available data tidying case study using data World Health Organization webpage. Furthermore, week R4DS Online Learning Community posts dataset weekly #TidyTuesday event might serve nice place find data explore transform.","code":"\ndrinks_smaller## # A tibble: 4 x 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84\ndrinks_smaller_tidy <- drinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy## # A tibble: 12 x 3\n##    country      type   servings\n##    <chr>        <chr>     <int>\n##  1 China        beer         79\n##  2 China        spirit      192\n##  3 China        wine          8\n##  4 Italy        beer         85\n##  5 Italy        spirit       42\n##  6 Italy        wine        237\n##  7 Saudi Arabia beer          0\n##  8 Saudi Arabia spirit        5\n##  9 Saudi Arabia wine          0\n## 10 USA          beer        249\n## 11 USA          spirit      158\n## 12 USA          wine         84\ndrinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\ndrinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\nggplot(drinks_smaller_tidy, aes(x = country, y = servings, fill = type)) +\n  geom_col(position = \"dodge\")"},{"path":"wrangling.html","id":"other-commands","chapter":"2 Wrangling","heading":"2.8 Other Commands","text":"commands prove useful rest book.","code":""},{"path":"wrangling.html","id":"na-values","chapter":"2 Wrangling","heading":"2.8.1 NA Values","text":"importing datasets already extensively wrangled, may observations dataframe blank. called missing values, often marked NA. presence NA values dataframe can problematic. Recall previous chapter without setting na.rm = TRUE, following code gives different results.Suppose wanted simply remove missing values gain column. can filter remove missing values using helper function .na(). .na() boolean function takes observation dataframe returns TRUE observation missing value FALSE otherwise. case, use !.na(), ! operator means “.” Thus, can filter observations missing values.can also remove missing values dataset instead merely filtering using drop_na() function. flights dataset contains 336,776 rows. whenAs see , 9,430 rows flights dataset contain missing values gain column. using drop_na(), get:Note 336,776 - 9,430 = 327346, drop_na() seems check . careful drop_na(), however, may removing rows valuable data columns.Suppose want remove missing values instead replace number. can use replace_na() helper function mutate(). replace missing value gain column value 0.must careful , though. Replacing missing values affect statistical result, mean standard deviation. computer mean:Comparing previous mean variable see computed value different.many additional ways manipulate missing values, three functions cover ways need work observations data.","code":"\nflights %>% \n  mutate(gain = arr_delay - dep_delay) %>% \n  summarize(mean = mean(gain))## # A tibble: 1 x 1\n##    mean\n##   <dbl>\n## 1    NA\nflights %>% \n  mutate(gain = arr_delay - dep_delay) %>% \n  summarize(mean = mean(gain, na.rm = TRUE))## # A tibble: 1 x 1\n##    mean\n##   <dbl>\n## 1 -5.66\nflights %>%\n  mutate(gain = arr_delay - dep_delay) %>%\n  filter(!is.na(gain)) %>%\n  summarize(mean = mean(gain))## # A tibble: 1 x 1\n##    mean\n##   <dbl>\n## 1 -5.66\nflights %>%\n  mutate(gain = arr_delay - dep_delay) %>%\n  filter(is.na(gain))## # A tibble: 9,430 x 22\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1     1525           1530        -5     1934           1805\n##  2  2013     1     1     1528           1459        29     2002           1647\n##  3  2013     1     1     1740           1745        -5     2158           2020\n##  4  2013     1     1     1807           1738        29     2251           2103\n##  5  2013     1     1     1939           1840        59       29           2151\n##  6  2013     1     1     1952           1930        22     2358           2207\n##  7  2013     1     1     2016           1930        46       NA           2220\n##  8  2013     1     1       NA           1630        NA       NA           1815\n##  9  2013     1     1       NA           1935        NA       NA           2240\n## 10  2013     1     1       NA           1500        NA       NA           1825\n## # … with 9,420 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\nflights %>%\n  mutate(gain = arr_delay - dep_delay) %>%\n  drop_na(gain)## # A tibble: 327,346 x 22\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 327,336 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\nflights %>%\n  mutate(gain = arr_delay - dep_delay) %>%\n  mutate(gain = replace_na(data = gain, replace = 0))## # A tibble: 336,776 x 22\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 14 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\nflights %>%\n  mutate(gain = arr_delay - dep_delay) %>%\n  mutate(gain = replace_na(data = gain, replace = 0)) %>%\n  summarize(mean = mean(gain))## # A tibble: 1 x 1\n##    mean\n##   <dbl>\n## 1 -5.50"},{"path":"wrangling.html","id":"clean_names","chapter":"2 Wrangling","heading":"2.8.2 clean_names()","text":"Another useful function clean_names() function, included janitor package. function allows standardize column names dataset. can particularly useful reading dataset web. Look flights dataset, .can see column names lowercase, space words represented underscore character. default result clean_names(), running clean_names(flights) effect.Suppose, instead, wanted change column names camel case, word boundaries demarcated upper case letter.Nonetheless, likely ever required use clean_names() default snake case.","code":"\nglimpse(flights)## Rows: 336,776\n## Columns: 22\n## $ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, …\n## $ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558,…\n## $ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600,…\n## $ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -…\n## $ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849…\n## $ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851…\n## $ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -…\n## $ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", …\n## $ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, …\n## $ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N39…\n## $ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\"…\n## $ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\"…\n## $ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, …\n## $ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733,…\n## $ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, …\n## $ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, …\n## $ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 …\n## $ gain           <dbl> -9, -16, -31, 17, 19, -16, -24, 11, 5, -10, 0, 1, -9, …\n## $ hours          <dbl> 3.78, 3.78, 2.67, 3.05, 1.93, 2.50, 2.63, 0.88, 2.33, …\n## $ gain_per_hour  <dbl> -2.38, -4.23, -11.62, 5.57, 9.83, -6.40, -9.11, 12.45,…\nflights %>% clean_names(\"small_camel\")## # A tibble: 336,776 x 22\n##     year month   day depTime schedDepTime depDelay arrTime schedArrTime arrDelay\n##    <int> <int> <int>   <int>        <int>    <dbl>   <int>        <int>    <dbl>\n##  1  2013     1     1     517          515        2     830          819       11\n##  2  2013     1     1     533          529        4     850          830       20\n##  3  2013     1     1     542          540        2     923          850       33\n##  4  2013     1     1     544          545       -1    1004         1022      -18\n##  5  2013     1     1     554          600       -6     812          837      -25\n##  6  2013     1     1     554          558       -4     740          728       12\n##  7  2013     1     1     555          600       -5     913          854       19\n##  8  2013     1     1     557          600       -3     709          723      -14\n##  9  2013     1     1     557          600       -3     838          846       -8\n## 10  2013     1     1     558          600       -2     753          745        8\n## # … with 336,766 more rows, and 13 more variables: carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, airTime <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, timeHour <dttm>, gain <dbl>, hours <dbl>,\n## #   gainPerHour <dbl>"},{"path":"wrangling.html","id":"skim","chapter":"2 Wrangling","heading":"2.8.3 skim()","text":"skim() function skimr package useful summary function offers overview dataframe. offers users abilities see potential trends outliers variables dataframe. function creates report dataframe according variable type. ’ll run skim flights dataset.TABLE 2.1: Data summaryVariable type: characterVariable type: numericVariable type: POSIXctTake quick look section containing numeric variables. particular interest rightmost column offers insight potential distribution data, something discussed later depth .","code":"\nskim(flights)"},{"path":"wrangling.html","id":"distribution","chapter":"2 Wrangling","heading":"2.9 Distribution","text":"distribution? distribution variable shows frequently different values variable occur. Looking visualization distribution can show values centered, show values vary, give information typical value might fall. can also alert presence outliers.examine distributions greater depth, let’s first explore sample() function. sample() takes sample size vector either replacement without replacement. might seem abstract, consider six-sided dice represented vector.Suppose wanted simulate rolling dice . can use sample() function achieve .Now, suppose wanted roll dice 10 times. One arguments sample() function replace. must specify certain value can rolled . case, replace TRUE FALSE.replace TRUE. words, rolling 1 first roll preclude rolling one later roll.final argument sample() function prob argument. takes vector (length initial vector x) contains probabilities “landing ” one elements x. Suppose probability rolling 1 0.5, probability rolling value 0.1. Note, probabilities sum 1.Now, let’s return discussion distributions.","code":"\ndice <- c(1, 2, 3, 4, 5, 6)\nsample(x = dice, size = 1)## [1] 2\nsample(x = dice, size = 10, replace = TRUE)##  [1] 1 5 1 5 6 2 5 1 6 2\nprobs = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1)\n\nsample(x = dice, size = 10, replace = TRUE, prob = probs)##  [1] 2 3 2 3 6 1 3 1 1 1"},{"path":"wrangling.html","id":"runif","chapter":"2 Wrangling","heading":"2.9.1 runif()","text":"Consider Uniform distribution. case every outcome chance occurring. Uniform distribution might encounter everyday rolling fair dice. example, likely roll 2 roll 6. R, function runif() (read “r-unif”) corresponds Uniform contribution. words, function generates random values (within specified range).runif() function three arguments: n, min, max. Note R, runif() returns continuous uniform distribution, discrete one. means tried simulate die-rolling example, get decimal values.Let’s press die-rolling example now cautioned difference discrete continuous uniform distributions. three arguments runif(): n, min, max. n number observations, n = 1 since rolling die . default value min 0 default value max 1. die-rolling example, two values min = 1 max = 6, respectively. Let’s plug arguments function see get.Nice! just made first uniform distribution. , note output contains decimal numbers.","code":"\nrunif(n = 1,min = 1,max = 6)## [1] 4.5"},{"path":"wrangling.html","id":"rbinom","chapter":"2 Wrangling","heading":"2.9.2 rbinom()","text":"Now consider Binomial distribution, case probability boolean variable (instance success failure) calculated repeated, independent trials. One common example probability flipping coin landing heads. R, function rbinom() simulates distribution. function takes three arguments, n, size, prob. n number observations, size number trials, prob probability success trial. Suppose wanted flip fair coin one time, let landing hands represent success. Let n=1 size = 1, want flip coin . , since coin fair coin, let prob = 0.5.can see pretty even distribution Heads (1s) Tails (0s). even larger sample size result even spread Heads Tails values.Suppose instead wanted simulate unfair cin, probability landing Heads 0.75 instead 0.25.","code":"\nrbinom(n = 1 , size = 1, prob = 0.5)## [1] 0\ncoin_flip <- tibble(heads = rbinom(n = 100, size = 1, prob = 0.5))\n\nggplot(data = coin_flip, aes(x = heads)) +\n  geom_bar() +\n  labs(title = \"Flipping a Fair Coin 100 Times\") +\n  xlab(label = \"Flips\") +\n  ylab(label = \"Count\")\ncoin_flip_2 <- tibble(heads = rbinom(n = 100, size = 1, prob = 0.75))\n\nggplot(data = coin_flip_2, aes(x = heads)) +\n  geom_bar() +\n  labs(title = \"Flipping a Fair Coin 100 Times\") +\n  xlab(label = \"Flips\") +\n  ylab(label = \"Count\")"},{"path":"wrangling.html","id":"normal","chapter":"2 Wrangling","heading":"2.9.3 Normal distribution","text":"Let’s next discuss one particular kind distribution: normal distributions. bell-shaped distributions defined two values: (1) mean \\(\\mu\\) (“mu”) locates center distribution (2) standard deviation \\(\\sigma\\) (“sigma”) determines variation distribution. figure , plot three normal distributions :solid normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).dotted normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).dashed normal curve mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\nFIGURE 2.7: Three normal distributions.\nNotice solid dotted line normal curves center due common mean \\(\\mu\\) = 5. However, dotted line normal curve wider due larger standard deviation \\(\\sigma\\) = 5. hand, solid dashed line normal curves variation due common standard deviation \\(\\sigma\\) = 2. However, centered different locations.mean \\(\\mu\\) = 0 standard deviation \\(\\sigma\\) = 1, normal distribution special name. ’s called standard normal distribution \\(z\\)-curve.Furthermore, variable follows normal curve, three rules thumb can use:68% values lie within \\(\\pm\\) 1 standard deviation mean.95% values lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations mean.99.7% values lie within \\(\\pm\\) 3 standard deviations mean.Let’s illustrate standard normal curve. dashed lines -3, -1.96, -1, 0, 1, 1.96, 3. 7 lines cut x-axis 8 segments. areas normal curve 8 segments marked add 100%. example:middle two segments represent interval -1 1. shaded area interval represents 34% + 34% = 68% area curve. words, 68% values.middle four segments represent interval -1.96 1.96. shaded area interval represents 13.5% + 34% + 34% + 13.5% = 95% area curve. words, 95% values.middle six segments represent interval -3 3. shaded area interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% area curve. words, 99.7% values.\nFIGURE 2.8: Rules thumb areas normal curves.\n","code":""},{"path":"wrangling.html","id":"rnorm","chapter":"2 Wrangling","heading":"2.9.4 rnorm()","text":"function rnorm() can read “r-norm.” corresponds Normal distribution.rnorm() three arguments: n, mean, sd. n corresponds number observations, mean corresponds average whole, sd corresponds size standard deviation mean. go depth future chapters statistical mechanics behind distributions, now, let’s focus creating Normal distribution.run following:get 10 observations centered around default mean 0 default sd 1. create histogram values?Now ’s looking lot similar (although imperfect)! just discovered, observations normal distribution , closer looks symmetrical bell curve.Now, let’s compare normal distributions varying means standard deviations, can set using mean sd arguments included function.","code":"\nrnorm(10)##  [1] -0.84  1.38 -1.26  0.07  1.71 -0.60 -0.47 -0.64 -0.29  0.14\ndistrib <- tibble(value = rnorm(10))\n\nggplot(distrib, aes(x = value)) + \n  geom_histogram(bins = 10)\ndistrib_2 <- tibble(value = rnorm(100))\n\nggplot(distrib_2, aes(x = value)) +\n  geom_histogram(bins = 10)\ndistrib_3 <- tibble(value = rnorm(1000))\n\nggplot(distrib_2, aes(x = value)) +\n  geom_histogram(bins = 10)\nnormal_distrib <- tibble(rnorm_5_1 = rnorm(n = 1000, mean = 5, sd = 1), \n                        rnorm_0_3 = rnorm(n = 1000, mean = 0, sd = 3),\n                        rnorm_0_1 = rnorm(n = 1000, mean = 0, sd = 1)) %>%\n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", \n               values_to = \"samples\")\n\nggplot(normal_distrib, aes(x = samples, fill = distribution)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Comparison of Normal Distributions with Differing Mean and Standard Deviation Values\", \n       fill = \"Distribution\") +\n  xlab(\"X\") + \n  ylab(\"Density\")"},{"path":"rubin-causal-model.html","id":"rubin-causal-model","chapter":"3 Rubin Causal Model","heading":"3 Rubin Causal Model","text":"ever wondered world like without ?George Bailey movie “’s Wonderful Life.” movie follows George explores world never born. clear profound impact lives many people community.showing world like without George, get idea causal effect life town people live . looking causal effects throughout rest chapter using framework potential outcomes Rubin Causal Model (RCM).\nFIGURE 3.1: Don Rubin professor Statistics Harvard.\n","code":""},{"path":"rubin-causal-model.html","id":"preceptor-tables","chapter":"3 Rubin Causal Model","heading":"3.1 Preceptor Tables","text":"Preceptor Table table rows columns data data (reasonably) like , , cells data, thing want know trivial calculate. Preceptor Tables can vary depending number rows columns , well amount data missing . use question marks placeholders data missing Preceptor Table.Assume five adult brothers given four heights. average height five brothers? given , can use statistics make best guess. Let’s look Preceptor Table problem: case, row brother column height. always ID column Preceptor Tables can identify different units. always furthest left. addition ID column, call column brother’s heights outcome column. Outcomes columns always right ID column. also types columns might included Preceptor Table introduce soon!interested predicting Andy’s height. One guess just average four brothers: \\[\\frac{(178 + 165 + 172 + 173)}{4} = 172 cm\\] realistic think best guess ? consider things like know four brothers’ heights. sampled randomly? know heights tallest family? case make sense use average best guess Andy’s height? Probably !Now let’s think slightly complex problem. Say heights 100 Harvard students, want know average height students school. Let’s make another Preceptor Table:\n , 100 students randomly sampled? estimate 90th percentile height student population? questions complicated, might less confident best guess. Now let’s say given characteristics height 100 sampled students, .e., sex age. ’ll notice Preceptor Table new type column: covariates. better able forecast random students height given age sex? Throughout rest book learn solve similar problems also considering intricacies plague even simplest questions.far, asked predictive questions. chapter, however, primarily focuses causal inference.","code":""},{"path":"rubin-causal-model.html","id":"causal-effect","chapter":"3 Rubin Causal Model","heading":"3.2 Causal effect","text":"\nFIGURE 3.2: study conducted Ryan Enos.\nRubin Causal Model (RCM) based idea potential outcomes. Consider experiment studied attitudes toward immigration among Boston commuters. Individuals exposed one two possible conditions, attitudes towards immigrants measured. One condition train platform near individuals speaking Spanish. train platform without Spanish-speakers. calculate causal effect Spanish-speakers nearby, need compare outcome individual one possible state world (Spanish-speakers) another (without Spanish-speakers). impossible observe potential outcomes . One potential outcomes always missing. dilemma Fundamental Problem Causal Inference.circumstances, interested comparing two experimental manipulations, one generally termed “treatment” “control.” difference potential outcome treatment potential outcome control called “causal effect” “treatment effect.” scenario didn’t actually happen, thus didn’t observe, called “counterfactual.” According RCM, causal effect platform Spanish-speakers difference attitude “treatment” (Spanish-speakers) “control” (Spanish-speakers).commuter survey consisted 3 questions, 1 liberal answer 5 conservative. range attitudes therefore goes 3 15. attitude towards immigrants 13 Spanish-speakers 9 without Spanish-speakers, causal effect platform Spanish-speakers 4-point increase attitude score.use variable \\(Y\\) represent values interested understanding (potential outcomes). \\(Y\\) called response variable variable want explain context experiment. case attitude score. trying understand causal effect, need two variables: control treated values can represented separately. use variables \\(Y_t\\) \\(Y_c\\).","code":""},{"path":"rubin-causal-model.html","id":"potential-outcomes","chapter":"3 Rubin Causal Model","heading":"3.2.1 Potential outcomes","text":"Suppose Yao one commuters surveyed experiment. omniscient, know outcomes Yao treatment (Spanish-speakers) control (Spanish-speakers). can show using ideal Preceptor Table. Preceptor Table considered ideal missing data, calculating number interested trivial. table know causal effect Yao. Everyone else study might attitude score go treated (liberal). Regardless causal effect subjects, causal effect Yao train platform Spanish-speakers shift towards conservative attitude.Preceptor Table can also use response variables describe potential outcomes.Yao139 ","code":""},{"path":"rubin-causal-model.html","id":"estimands","chapter":"3 Rubin Causal Model","heading":"3.2.2 Estimands","text":"order understand estimands, let’s look ideal Preceptor Table Yao:Yao139 One possible variable might interested causal effect. Remember difference Yao’s potential outcomes treatment control. convenience, let’s add ideal Preceptor Table:Yao139+4 remember actual Preceptor Table bunch missing data! normally can’t use simple arithmetic calculate causal effect Yao’s attitude toward immigration. Instead required estimate. estimand variable real world trying measure. case, \\(Y_{t}-Y_{c}\\), \\(+4\\). estimand value calculated, rather unknown variable want estimate.continue use ideal Preceptor Tables give idea examples estimands, remember normally estimands found using arithmetic lots missing values actual Preceptor Tables!Treatment effect one possible estimand might interested . ratio potential outcomes another estimand:Yao13913/9+4 percentage change outcome treatment outcome control another estimand.Yao139+44.4% ","code":""},{"path":"rubin-causal-model.html","id":"multiple-units","chapter":"3 Rubin Causal Model","heading":"3.2.3 Multiple units","text":"Generally study individuals unique potential outcomes. case, longer makes sense just \\(Y_t\\) \\(Y_c\\) represent individual potential outcomes. notation needed allow us differentiate individuals.words, needs distinction \\(Y_t\\) Yao, \\(Y_t\\) Emma. something like \\(Y_t(Yao)\\) \\(Y_t(Emma)\\), ineffective approach wanted fill full Preceptor Table. Instead, use variable \\(u\\) (\\(u\\) “unit”) indicate outcome control outcome treatment might different individual unit (person). \\(u\\) represents different individuals.Now, instead \\(Y_t\\), use \\(Y_t(u)\\) represent ‘Attitude Treated.’ want talk Emma, say “Emma’s Attitude Treated” “\\(Y_t(u = Emma)\\)” “\\(Y_t(u)\\) Emma,” just \\(Y_t\\). notation ambiguous one subject.Let’s look ideal Preceptor Table subjects using new notation:Yao139+4Emma11110Cassidy1110+1Tahmid912-3Diego54+1 ideal Preceptor Table, many possible estimands might interested . Consider examples, along true values:potential outcome one person, e.g., Yao’s potential outcome treatment (\\(13\\)).causal effect one person, Emma. difference potential outcomes, provided column (\\(11 - 11 = 0\\)).positive causal effect. , \\(+4\\), Yao (\\(13 - 9 = +4\\)).negative causal effect. , \\(-3\\), Tahmid (\\(9 - 12 = -3\\)).median causal effect (\\(+1\\)).median percentage change. , calculate percentage change person. ’ll get 5 percentages: \\(+44.4\\%\\), \\(0.0\\%\\), \\(+10.0\\%\\), \\(-25.0\\%\\), \\(+25.0\\%\\). median \\(+10.0\\%\\).total number people causal effect positive: \\(3\\).. lot things one might care !variables calculated examples estimands might interested . One common estimand might care name, average treatment effect. average treatment effect (often abbreviated ATE) mean causal effects. , mean \\(+0.6\\).Remember actual Preceptor Table riddled question marks looks like:Yao13??Emma11??Cassidy?10?Tahmid?12?Diego5?? Calculating values table longer simple math problem. Don’t worry! look ways solve estimands interested actual Preceptor Table soon.","code":""},{"path":"rubin-causal-model.html","id":"preceptor-table-structure","chapter":"3 Rubin Causal Model","heading":"3.2.4 Preceptor Table structure","text":"Remember Preceptor Table rows columns data reasonably like know. first column Preceptor Table always ID column. can columns subheadings ‘Outcomes,’ ‘Estimands,’ ‘Covariates’ order. final structure Preceptor Table look like , include whatever columns rows necessary problem trying solve:1??????2??????3??????4??????5?????? example, indeed constant treatment effect everyone: \\(+2\\). Note observed values , unobserved values estimated ATE, \\(-1.33\\), pretty far actual ATE, \\(+2\\). ’ll consider little bit reasonable assume estimate ATE good one. think reasonable estimate ATE, using value constant \\(\\tau\\) might best guess want estimate counterfactual individual. discussion, see video:","code":""},{"path":"rubin-causal-model.html","id":"simple-models","chapter":"3 Rubin Causal Model","heading":"3.3 Simple models","text":"can fill question marks? fundamental problem causal inference, can never know missing values. can never know missing values, must make assumptions. “Assumption” just means need “model,” models parameters.","code":""},{"path":"rubin-causal-model.html","id":"a-single-value-for-tau","chapter":"3 Rubin Causal Model","heading":"3.3.1 A single value for tau","text":"One model might causal effect everyone. single parameter, \\(\\tau\\), estimate. (\\(\\tau\\) Greek letter — spelled phonetically “tau” — rhymes “cow.”) estimate, can fill Preceptor Table , knowing , can estimate unobserved potential outcome person. use assumption \\(\\tau\\) estimate counterfactual outcome unit. Now estimand \\(\\tau\\); whether sensible estimand depends close real world conforms assumption causal effect everyone.Remember Preceptor Table looks like missing data:Yao13??Emma11??Cassidy?10?Tahmid?12?Diego5?? assume \\(\\tau\\) treatment effect everyone, fill table? using \\(\\tau\\) estimate causal effect. therefore true \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, clear \\(Y_t(u) = Y_c(u) + \\tau\\) \\(Y_c(u) = Y_t(u) - \\tau\\). words, add observed value every observation control group (subtract observed value every observation treatment group), thus fill missing values.Assuming constant treatment effect, \\(\\tau\\), everyone, filling missing values look like :Yao13$$13 - \\tau$$$$\\tau$$Emma11$$11 - \\tau$$$$\\tau$$Cassidy$$10 + \\tau$$10$$\\tau$$Tahmid$$12 + \\tau$$12$$\\tau$$Diego5$$5 - \\tau$$$$\\tau$$ Now need find estimate \\(\\tau\\) order fill table numbers can begin understand. , assuming \\(Y_t(u) - Y_c(u) = \\tau\\).equation outlines clear way can estimate \\(\\tau\\). can subtract average observed control values observed treated values. \\[((13 + 11 + 5) / 3) - ((10 + 12) /  2)\\] \\[9.67 - 11 = -1.33\\]gives us estimate \\(-1.33\\) \\(\\tau\\). Let’s fill missing values adding \\(\\tau\\) observed values control subtracting \\(\\tau\\) observed value treatment like :Yao13$$13 - (-1.33)$$-1.33Emma11$$11 - (-1.33)$$-1.33Cassidy$$10 + (-1.33)$$10-1.33Tahmid$$12 + (-1.33)$$12-1.33Diego5$$5 - (-1.33)$$-1.33 gives us:Yao1314.33-1.33Emma1112.33-1.33Cassidy8.6710-1.33Tahmid10.6712-1.33Diego56.33-1.33 ","code":""},{"path":"rubin-causal-model.html","id":"two-values-for-tau","chapter":"3 Rubin Causal Model","heading":"3.3.2 Two values for tau","text":"second model might assume causal effect constant within categories, across entire sample. Maybe one categories sex? Perhaps \\(\\tau_F\\) females \\(\\tau_M\\) males. making assumption give us different model fill missing values table. key concept can’t make progress unless make assumptions. inescapable result Fundamental Problem Causal Inference.Let’s look particular model think sex impact treatment effect:Yao13$$13 - \\tau_M$$$$\\tau_M$$Emma11$$11 - \\tau_F$$$$\\tau_F$$Cassidy$$10 + \\tau_F$$10$$\\tau_F$$Tahmid$$12 + \\tau_M$$12$$\\tau_M$$Diego5$$5 - \\tau_M$$$$\\tau_M$$ two different estimates \\(\\tau\\).\\(\\tau_M\\) \\[(13+5)/2 - 12 = -3\\]\n\\(\\tau_F\\) \\[(11-10 = +1)\\]Using values, fill new table like :Yao13$$13 - (-3)$$-3Emma11$$11 - (+1)$$+1Cassidy$$10 + (+1)$$10+1Tahmid$$12 + (-3)$$12-3Diego5$$5 - (-3)$$-3 gives us:Yao1316-3Emma1110+1Cassidy1110+1Tahmid912-3Diego58-3 now two different estimates Emma (everyone else table). estimate \\(Y_c(Emma)\\) using assumption constant treatment effect (single value tau), get 12.33. estimate assuming treatment effect constant sex, calculate \\(Y_c(Emma) = 10\\). difference estimates Emma heart inference.","code":""},{"path":"rubin-causal-model.html","id":"heterogenous-treatment-effects","chapter":"3 Rubin Causal Model","heading":"3.3.3 Heterogenous treatment effects","text":"train study, individuals surveyed different many different ways, example gender, income, age. Therefore, study population heterogeneous. far, made assumptions similar treatment effect subjects study, despite heterogeneity sample.assumption unfortunately rarely true. inevitably random variation treatment effect individuals. effect Yao may different treatment effect another individual similar Yao. one example heterogeneous treatment effect.Another example heterogeneous treatment effect variation individual treatments non-random correlated multiple factors. example, maybe think treatment effect varies depending sex age individual. case, sex, age treatment status three characteristics treatment effect varies. variables may interact, impacting estimate treatment effect. call interaction effect., heterogeneous treatment effect can explain individual variation, well variation groups determined variable(s).extreme case, treatment effect every individual study different. actual Preceptor Table therefore look like :Yao13$$13 - \\tau_{yao}$$$$\\tau_{yao}$$Emma11$$11 - \\tau_{emma}$$$$\\tau_{emma}$$Cassidy$$10 + \\tau_{cassidy}$$10$$\\tau_{cassidy}$$Tahmid$$12 + \\tau_{tahmid}$$12$$\\tau_{tahmid}$$Diego5$$5 - \\tau_{diego}$$$$\\tau_{diego}$$ Can solve just \\(\\tau_{yao}\\). ! Fundamental Problem Causal Inference. can make progress unwilling assume least structure causal effect across different individuals?might able solve individual’s value \\(\\tau\\), completely stuck.","code":""},{"path":"rubin-causal-model.html","id":"average-treatment-effect","chapter":"3 Rubin Causal Model","heading":"3.3.4 Average treatment effect","text":"average treatment effect average difference potential outcomes treated group control group. difference estimand estimands like \\(\\tau\\), \\(\\tau_M\\) \\(\\tau_F\\), case care using average treatment effect fill missing values row. average treatment effect useful don’t assume anything individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), can still understand something average causal effect across whole sample., simplest way estimate ATE take mean treated group (\\(9.67\\)) mean control group (\\(11.0\\)) take difference means (\\(-1.33\\)). ’ll call estimate average treatment effect, \\(\\widehat{ATE}\\), pronounced “ATE-hat.”already exact calculation , talking ? Remember unwilling assume treatment effect constant study population, solve \\(\\tau\\) \\(\\tau\\) different different individuals. \\(\\widehat{ATE}\\) helpful.estimands may require filling question marks Preceptor Table. can get good estimate average treatment effect without filling every question mark — average treatment effect just single number. Rarely study care happens individuals. case, don’t care specifically happen Cassidy’s attitude treated. Instead, care generally experiment impacts people’s attitudes towards immigrants. average estimate, like \\(\\widehat{ATE}\\) can helpful.noted , popular estimand. ?’s obvious estimator estimand: difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).’s obvious estimator estimand: difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.Just ATE often useful estimand doesn’t mean always .Consider point #3. example, let’s say treatment effect vary dependent sex. males strong negative effect (-3), females smaller positive effect (+1). However, average treatment effect whole sample, even estimate correctly, single negative number (-1.33) (since negative effect males larger positive effect females). model estimate ATE simple thus alluring, don’t assume ’s always right model problem.estimating ATE easy. \\(\\widehat{ATE}\\) good estimate actual ATE? , knew missing values Preceptor Table, calculate ATE perfectly. missing values may wildly different observed values. Consider unobservable ideal Preceptor Table:Yao1311+2Emma119+2Cassidy1210+2Tahmid1412+2Diego53+2 example, indeed constant treatment effect everyone: \\(+2\\). Note observed values , unobserved values estimated ATE, \\(-1.33\\), pretty far actual ATE, \\(+2\\). ’ll consider little bit reasonable assume estimate ATE good one. think reasonable estimate ATE, using value constant \\(\\tau\\) might best guess want estimate counterfactual individual. discussion, see video:","code":""},{"path":"rubin-causal-model.html","id":"key-concepts","chapter":"3 Rubin Causal Model","heading":"3.4 Key concepts","text":"","code":""},{"path":"rubin-causal-model.html","id":"causal-vs.-predictive-models","chapter":"3 Rubin Causal Model","heading":"3.4.1 Causal vs. Predictive Models","text":"Causal inference often compared prediction. prediction, want know outcome, \\(Y(u)\\). causal inference, want know function potential outcomes, treatment effect \\(Y_t(u) - Y_c(u)\\).Note, however, kinds missing data problems. Prediction involves getting estimate outcome variable don’t , thus missing, whether future data unable collect. Thus, prediction term using statistical inference fill missing data outcomes.Causal inference, however, term filling missing data potential outcomes. Unlike prediction, one potential outcome can ever observed, even principle. (forecasting weather, can compare forecast yesterday actual weather today.) way, however, still kind prediction: just prediction quantity can’t ever observe (potential outcome).causal inference prediction, process data missing observed crucial. think missing data similar observed data, can make inferences easily. , think dissimilarities consider model .also discuss subtleties differentiate predictive causal models.Remember predictive model care forecasting value \\(Y(u)\\) given know \\(X\\). means one column outcomes value \\(X\\). Let’s continue using train example, say \\(X_1\\) 50 years old, \\(X_2\\) 50 older.\nLet’s see look like Preceptor Table:509.2150 Over9 model, given someone 50 years old predict \\(Y(u)\\) value \\(9.21\\). Similarly, \\(9\\) prediction given \\(X\\) someone 50 years old. made strong assumptions create predictive model aware implications assumptions. think predictions make using model realistic? Probably . hopefully example differences predictive causal models may little clear.important distinction notice one \\(Y(u)\\) value \\(X\\). different RCM used two potential outcomes (treated control). one outcome column predictive model, whereas two causal model.predictive model infer happen outcome \\(Y(u)\\) changed \\(X\\). also different used causal model trying understand causal effect change (treatment). predictive model, can compare two groups note predict people 50 attitude score slightly conservative 50 . claim, however, understand happen individual’s attitude turn 50 (confounding!). Similarly, make estimate individual’s attitude might 50 years old. soon read , age something can ever estimate causal effect .sense, models predictive. data stable distribution, make predictive forecast someone’s attitude. subset models causal, meaning , given individual, can change value \\(X\\) observe change outcome, \\(Y(u)\\), calculate causal effect.employ Justice discussing type model use answer question. critical differences causal predictive models can overlooked one precise just describing models used.","code":""},{"path":"rubin-causal-model.html","id":"the-assignment-mechanism","chapter":"3 Rubin Causal Model","heading":"3.4.2 The assignment mechanism","text":"start assignment mechanism example Justice necessary. biased assignment mechanism ruin chance model fair representation reality.Remember everything missing data problem. sidestepped following question : difference sample means treated units control units, \\(\\widehat{ATE}\\), good estimate ATE? depends entirely method units assigned treatment, called assignment mechanism. mechanism whereby values missing values observed.discussed earlier, already comes non-causal context considering sampling. trying estimate average attitude towards immigrants U.S., usually taking sample. process people enter sample called sampling mechanism. process people enter sample related attitude, even indirectly, estimates sample won’t good estimates population.sampling mechanism still matters causal inference. may want estimate causal effect Eliot, example, rather Yao. even get sampling mechanism, another missing data problem can affect causal inference. assignment mechanism, process units receive treatment others . soon clear randomization ideal assignment mechanism. Whenever assignment mechanism correlated potential outcomes, say confounding. Confounding problem, since means simple estimate ATE biased.interested causal inference, randomized trials best approach. many circumstances, however, randomized trials possible due ethical practical concerns. scenarios necessity non-random assignment mechanism. following example non-random assignment mechanism illustrates potential problems.instance, let’s say interested effect college attendence earnings. People randomly assigned attend college. Rather, people may choose attend college based financial situation, parents’ education, . can introduce confounding assignment mechanism affects future earnings. example, people choose go college higher rates career paths college degree particularly beneficial – , RCM language, whose potential outcomes average higher treatment – introduce confounding.Let’s look idea non-random assignment mechanism version train experiment. Say one platform control treatment regions. Spanish-speakers randomly assigned regions, people within region considered treated. travelers however randomly assigned region, permitted move freely platform. Despite random assignment regions platform still may confounding. Say people conservative attitudes towards immigrants hyper-aware Spanish spoken, therefore choose stand treated regions. shift average attitude treated regions higher. therefore seem though treatment makes people conservative, reality bias assignment mechanism despite randomization regions. example, clear looking new data, even seemingly randomized, important consider ways still may confounding.Assignment mechanisms can also intentionally biased order manufacture desired outcomes. Let’s consider scenario entire platform either treated control. case assignment mechanism choice Spanish-speakers; allowed choose platform want stand . Let’s also say can perfectly predict attitude people platform. Spanish-speakers know platform liberal attitudes towards immigrants friendly, therefore always choose stand platforms. case, assignment mechanism platforms random. Spanish-speakers know averages various platforms experiment: Based knowledge Spanish-speakers experiment choose following treatment assignments: assignment mechanism used distorts averages \\(Y_t(u)\\) \\(Y_c(u)\\), turn distorts difference means. average treated group shifted lower (liberal), average control group shifted higher (conservative). gives illusion average treatment effect \\(\\widehat{ATE}\\) negative. true positive causal effect masked non-random assignment mechanism.Therefore, difference means longer good estimate ATE. fact, case wrong sign! merely consequence small sample: even million platforms experiment, get good estimate ATE.extreme example problem called selection bias. Selection bias person assigning treatment chooses basis potential outcomes. Spanish-speakers choosing platforms stand randomly. Rather, making treatment decisions based directly potential outcomes platform. Remember, whenever assignment mechanism correlated potential outcomes confounding, problem means estimand biased. examples confounding caused selection bias, selection bias always confoundingMuch like best way avoid making poor inferences sample population take random sample population, best assignment mechanism avoiding confounding randomization. platform flip coin determine treatment control group.Randomized assignment best assignment mechanism inferring average treatment effect sample large enough, difference sample means treated control units (\\(\\widehat{ATE}\\)) close ‘actual’ ATE sample. (still can’t know individual treatment effects without assumptions, fundamental problem causal inference.)Randomized assignments don’t look . example, randomized block assignment method random assignment can useful reducing potential confounding, making even better estimator pure random assignment. random block assignment, subjects experiment subdivided blocks. assignment blocks random, instead intentionally less variation within blocks . Within blocks subjects randomly assigned treatment control.example, let’s use method random block assignment train study. example, might think systematic differences platforms might impact treatment effect. Instead randomly assigning platforms control treated, use random block assignment. platform block. randomly assign half individuals block (platform) control (section platform Spanish-speakers), half treated (section platform Spanish-speakers). resulting random assignment look something like : Block random assignment ensures treated control groups equal proportion individuals platform. assignment mechanism way remove differences platforms potential source variability, therefore minimizing potential source confounding. Individuals within block still randomly assigned, just example different way use random assignment.interested causal inference, randomized trials best approach. many circumstances, however, randomized trials possible due ethical practical concerns. scenarios necessity non-random assignment mechanism.example, let’s say train platforms experiment loud Spanish-speakers might heard anyone nearby. Therefore necessity, quieter platforms can assigned treatment group. non-random assignment may introduce confounding. Say systematic difference people quieter platforms compared people louder platforms. case, assignment mechanism correlated potential outcomes, confounding. means even simple estimand like ATE biased.Many statistical methods developed causal inference non-random assignment mechanism, propensity score matching. methods attempt correct assignment mechanism finding control units similar treatment units. naively compare sample means treatment control assume good estimate ATE. Without randomization, misleading!Now, let’s consider Preceptor Table Yao, Emma, Cassidy, Tahmid, Diego :Yao13??Emma11??Cassidy?10?Tahmid?12?Diego5?? Let’s say random assignment treatment used get values. case, estimate \\(\\widehat{ATE} = -1.33\\) unbiased estimate ATE. don’t just always use randomized assignment assume unbiased estimand? Even randomization best assignment mechanism always perfect.","code":""},{"path":"rubin-causal-model.html","id":"the-infinite-preceptor-table","chapter":"3 Rubin Causal Model","heading":"3.4.3 The infinite Preceptor Table","text":"discussed ideal Preceptor Tables (missing data), actual Preceptor Tables (question marks representing values don’t know), one type Preceptor Table. real world, Preceptor Table infinite number rows, therefore infinite amount missing data. call type Preceptor Table infinite Preceptor Table. reality unworkable, make assumptions reduce true problem something manageable.Let’s start looking kinds missing data make infinite Preceptor Table. example, say care causal effect experiment Yao. care attitude right experiment? ! also care Yao’s potential outcomes one year now, two years now, .full Preceptor Table includes people know (Yao) people don’t (example, Eliot), now future: fact, time continuous, row Yao now, Yao one second now, Yao one day now . Preceptor Table extends downward forever. Thus, order estimate causal effect, need assumptions, aren’t dealing infinite table.obvious way eliminate rows table assume causal effect Yao now ones Yao future. plausible? Sort . Yao now Yao one second pretty similar! Yao now Yao 30 years less . Unfortunately, ’s magic way get good estimate every missing value infinite Preceptor Table! assumptions, can reduce true problem problem dealing .can extend Preceptor Table adding people sample rows, can also add additional treatments columns. Let’s go back original five people sample. also wanted test causal effect another language spoken platform? ’ll call original treatment \\(t\\) new treatment \\(t'\\).Yao13??Emma11??Cassidy??10Tahmid??12Diego5?? Note Yao, now three causal effects can estimate: difference original treatment new treatment, difference original treatment control, difference new treatment control. ’s just looking differences! Recall many potential estimands interested , ratios, percent changes, .Even just one language testing, still multiple treatments. example, amount time commuter platform Spanish-speakers might vary across commuters. case, might receive different treatment.Yao13?????Emma11?????Cassidy?????10Tahmid?????12Diego5????? , many possible estimands. ’s worth noting treatments numeric relationship (case time), may reasonable assumptions can use help fill missing values – aren’t yet!Instead considering treatment terms duration, also consider different volume levels Spanish spoken. Yao13????Emma11????Cassidy????10Tahmid????12Diego5???? Indeed, infinite number possible treatments. Preceptor Table extends right forever. , assumptions come rescue. rather, just throw hands try estimate things. crucial define one’s estimand precisely: interested difference potential outcomes Spanish spoken 10 minutes volume level 3 versus control, can ignore possible columns infinite Preceptor Table.Thus, whenever considering causal question, best way think start infinite Preceptor Table. First throw rows think duplicates (observations Yao one second now, two seconds now, etc.) outside scope interested now (maybe don’t care outcomes 30 years future study). Second, throw columns don’t care , possible treatments aren’t considering. Finally, define precisely—terms potential outcomes—estimand. may something simple, average treatment effect, something complex. done steps, can start thinking fill question marks. remember infinite Preceptor Table always , conscious rows columns throwing !","code":""},{"path":"rubin-causal-model.html","id":"data-problems","chapter":"3 Rubin Causal Model","heading":"3.4.4 Data problems","text":"Let’s look another example prudent can help us map path question data. mentioned previously likely care attitudes people surveyed.Say run train experiment, want know average attitude towards immigrants United States adults. first, seems like easy problem—’s nothing causal ! knew true values, build data set like :  , answer simply average values. table? ! actually : reality, don’t know attitude towards immigrants United States adults. , lot missing data.maybe survey 1,000 people attitudes towards immigrants, get table looks like :  surveying 1,000 people attitudes towards immigrants now values work . , however, solve missing data problem. likely interested causal effect general, just 1,000 person sample. ’ll need think whether sample representative full population. vast majority US adults still value. second two common sources missing data:units sample, see one potential outcomeFor units outside sample, see potential outcomeThere fact many potential sources missing data, like saw infinite Preceptor Table! missing data problem creates need statistical inferences. data missing, inference needed.\nFIGURE 3.3: study conducted Barfort, Klemmensen Larsen.\nLet’s consider new example experiment highlight another type data problem might encounter. Say want know causal effect elected governor life length. states minimum age requirement elected governor 30. People age 30 chance elected governor. means people less 30 one possible observable potential outcome. actual Preceptor Table, means rows 2 columns (people old enough elected), rows 1 column (people young elected).actual Preceptor Table problem might look something like : Yao--?Dean Khurana??Cassidy--?Preceptor??Tahmid--? Yao, Cassidy Tahmid question mark treatment column chance elected governor. Often real world actual Preceptor Table might look like . rows two columns, fewer. go ?consider actually interested knowing. case, don’t really care causal effect people can’t possibly elected governor. words, don’t care causal effect whole population, rather subset population. Just like case infinite Preceptor Table need throw rows specific problem. Instead saying want know causal effect elected governor, might specify want know causal effect American population 30.defined problem manageable actual Preceptor Table can begin deal question marks.","code":""},{"path":"rubin-causal-model.html","id":"the-four-cardinal-virtues","chapter":"3 Rubin Causal Model","heading":"3.5 The Four Cardinal Virtues","text":"four Cardinal Virtue Wisdom, Justice, Courage Temperance. Since data science moral professions, need cultivate virtues trod difficult path toward expertise. revisit virtues almost every chapter going forward. guide dictionary definitions lived experience.Wisdom starts us path quality data science.\nFIGURE 3.4: Wisdom\nwise show care thoughtful discretion regard future. important understanding Rubin Causal Model need decide data actually relevant problem want solve. wise go problem confronts us data access might reasonably acquire.First specifically define want know. case Enos (2014), probably want understand people’s attitudes toward immigration can understand something else act future, like vote. Yao’s attitude moment survey really matters. also probably care attitudes people surveyed. expands problem beyond simple Preceptor Tables presented far.Wisdom key making assumptions reduce “blooming, buzzing confusion” world something amenable analysis.Justice often compared fairness seeking moderation.\nFIGURE 3.5: Justice\ndata science take mean making sure model structures match reality enough; “fair” unbiased possible. want make sure models just representative real world possible. Justice starts math.Let’s represent problems discussing throughout chapter mathematical model:\\[outcome = model + \\ \\  \\ \\ \\ model\\]outcome, can represent model plus unmodeled variation riddles real world. Using model create, able come best guess outcome like predict. appropriately applying Justice understanding problem, recognize limited variation represented model.Courage synonymous fortitude, forbearance, strength endurance — ability confront fear, uncertainty intimidation.\nFIGURE 3.6: Courage\nthree languages data science words, math code, important code. need explain structure model using three languages. Courage important helping us create data generating mechanism via code.Temperance practice self-control, discretion moderation.\nFIGURE 3.7: Temperance\nTemperance important virtue data science. models never good appear . world complex , even worse, always changing. ignore uncertainty unmodeled variation come along inference.","code":""},{"path":"rubin-causal-model.html","id":"other-issues-with-causal-inference","chapter":"3 Rubin Causal Model","heading":"3.6 Other issues with causal inference","text":"","code":""},{"path":"rubin-causal-model.html","id":"no-causation-without-manipulation","chapter":"3 Rubin Causal Model","heading":"3.6.1 No causation without manipulation","text":"order potential outcome make sense, must possible, least priori. example, way Yao, circumstance, ever train study, \\(Y_{t}(u)\\) impossible . can never happen. \\(Y_{t}(u)\\) can never observed, even theory, causal effect treatment Yao’s attitude undefined.causal effect train study well defined simple difference two potential outcomes, might happen. case, (something else) can manipulate world, least conceptually, possible one thing different thing might happen.definition causal effects becomes much problematic way one potential outcomes happen, ever. example, causal effect Yao’s height weight? might seem just need compare two potential outcomes: Yao’s weight treatment (treatment defined 3 inches taller) Yao’s weight control (control defined current height).moment’s reflection highlights problem: can’t increase Yao’s height. way observe, even conceptually, Yao’s weight taller way make taller. can’t manipulate Yao’s height, makes sense investigate causal effect height weight. Hence slogan: causation without manipulation.raises question can manipulated. something manipulated, can’t consider causal. can race ever considered causal? sex? genetic condition like color-blindness? Can manipulate characteristics? modern world questions simple.Take color-blindness example. Say interested color-blindness impacts ability complete jig-saw puzzle. color-blindness genetic might argue manipulated. advances technology like gene-therapy might allow us actually change someones genes. claim ability manipulate color-blindness? yes, measure causal effect color-blindness ability complete jig-saw puzzles.problem still complicated even can manipulate genes. Gene therapy straight forward. Genes interact ways scientists still trying understand. unlikely gene controls color-blindness manipulated isolation. , questions complicated don’t clear answers.Take another example experiment race “manipulated” resumes changing names. Social scientists studied names can signal race, might seem like reasonable way manipulate race. problem name might signal race, race isn’t simple name. case, causal effect really studying? just ‘names,’ choosing names based ability signal race. also just manipulating race, race solely determined certain name.slogan “causation without manipulation” may first seem straight forward, clearly simple. Questions race, sex, gender genetics complex considered care.","code":""},{"path":"rubin-causal-model.html","id":"other-solutions-to-the-fundamental-problem-of-causal-inference","chapter":"3 Rubin Causal Model","heading":"3.6.2 Other solutions to the Fundamental Problem of Causal Inference","text":"Fundamental Problem Causal Inference impossible observe two potential outcomes , means one potential outcomes always missing (counterfactuals). Throughout chapter use statistical solution Fundamental Problem. accept fact observe potential outcomes, instead use techniques understand causal effects despite missing data.solutions Fundamental Problem commonly used hard sciences (think chemistry biology labs). still impossible observe one potential outcome , however, assumptions homogeneity allow someone believe can observe potential outcomes.example, say wanted know causal effect acid piece metal. chemistry lab, might go understanding causal effect two ways. One way measure variable piece metal put acid, measure piece metal ’ve put acid. give two potential outcomes (control - acid, treated - acid) calculate causal effect. Although may seem straight forward, making still assumptions allow believe reasonable experiment. assuming piece metal fairly static, nothing changing one moment next. likely also working control variables environment (like temperature) support assumption homogeneity. Even assumptions make valid, still assumptions. Another method often used hard sciences assuming homogeneity units. experiment, look like using two similar pieces metal control treated. calculate causal effect direct comparison. assumption two pieces metal similar enough difference treated .solution Fundamental Problem using assumptions homogeneity successful hard sciences, often make sense social sciences. example train experiment, unreasonable assume two individuals exactly . might tempting say individual constant enough beliefs one minute next measure attitude train platform Spanish-speakers. Even assumption realistic, way proving .Instead avoid using assumptions homogeneity statistical solution Fundamental Problem Causal Inference. can still understand something causal effect seeing potential outcomes.Fundamental Problem Causal Inference idea observe multiple potential outcomes unit exact time. multiple approaches overcoming problem understand causal effect. use statistical approach assume homogeneity across units time.","code":""},{"path":"rubin-causal-model.html","id":"stable-unit-treatment-value-assumption-sutva","chapter":"3 Rubin Causal Model","heading":"3.6.3 Stable unit treatment value assumption (SUTVA)","text":"One important assumption causal inference “[potential outcome] observation one unit unaffected particular assignment treatments units.”5 called Stable Unit Treatment Value Assumption (SUTVA).context example, Yao’s attitude depend Emma’s. ? Suppose upon hearing Spanish platform, Emma makes comment Yao immigration shifts opinion. Yao might thought anything Spanish spoken nearby, Emma’s comment changes individual survey answers. Therefore, outcome depend treatment received treatment Emma receives.SUTVA violation makes causal inference difficult. can account dependent observations considering treatments. create 4 treatments taking account whether Emma receives treatment: Recall causal effect defined difference two potential outcomes. case, multiple causal effects two potential outcomes:One causal effect Yao Emma receives treatment (\\(9-10\\)).Another causal effect Yao Emma receive treatment (\\(12-13\\)).third causal effect Emma’s treatment Yao Yao treated (\\(10-12\\)). words, can define causal effect Yao even situation Yao’s treatment identical situations.Note Emma treated larger causal effect Yao Yao treated!considering potential outcomes way, can cause SUTVA hold. However, units Yao dependent Emma (like someone overhears Emma), must consider potential outcomes. greater number dependent units, potential outcomes must consider complex calculations become (consider experiment 20 different people, whose treatment status can affect outcomes every one else). order (easily) estimate causal effect single treatment relative control, SUTVA hold.","code":""},{"path":"rubin-causal-model.html","id":"internal-and-external-validity","chapter":"3 Rubin Causal Model","heading":"3.6.4 Internal and external validity","text":"Recall two main sources missing data:units sample, see one potential outcomeFor units outside sample, see potential outcomesIf randomized assignment large sample, can confident good estimate average treatment effect sample. say experiment high internal validity: inferences making likely reflect truth sample. reflects first main source missing data.However, may interested population beyond particular sample, second main source missing data. example, let’s look broader context train experiment. likely exclusively concerned attitudes people ride trains, rather attitudes larger population. Train platforms, however, valuable setting run experiment. Let’s say ran randomized experiment 10,000 people Boston, found \\(\\widehat{ATE} = -1.33\\). assume estimate accurate larger general population?answer question depends part external validity study. 10,000 people study similar people want generalize findings ? Perhaps want generalize train commuters cities. Let’s say 10,000 people Boston choose ride trains environmental reasons. ’s another form selection bias. sample randomly selected population interested. problem? people differ systematically people way may affect response experiment. example, preference public transportation environmental reasons may correlated political beliefs.Note concern can expressed terms assignment mechanism. People don’t ride train 0% chance receiving treatment. Thus, study can’t directly speak treatment impact attitudes towards immigrants. way can make claims making additional assumptions, train-riders reflect makeup political beliefs people don’t ride trains.external validity study often directly related representativeness sample. Representativeness well sample represents larger population interested generalizing . Enos’ train experiment allow us calculate causal effect people commute cars? Can calculate causal effect people New York City? generalize broader populations consider experimental estimates applicable beyond experiment. Maybe think commuters Boston New York similar enough, \\(\\widehat{ATE}\\) also good estimate causal effect treatment NYC. also conclude people commute car fundamentally different people commute train. true, say estimate true commuters sample accurately represent broader group want generalize .circumstances experiment may also affect external validity study. Perhaps train study conducted middle summer platforms uncomfortably hot. , variation one aspect treatment (whether Spanish-speakers nearby), don’t variation another (temperature train platform). may attitude towards immigrants impacted train platform uncomfortably hot, treatment impact attitude otherwise.dealing human subjects, particular concern regarding external validity: Hawthorne effect. human subjects know part experiment, may change behavior.example, Hawthorne effect can impact attitudes expressed surveys. Maybe respondents extreme attitude either direction (liberal conservative) survey opportunity express opinion.always aware “realism” model. look extrapolate data beyond scope study, Temperance always employed.","code":""},{"path":"rubin-causal-model.html","id":"correlation-with-potential-outcomes","chapter":"3 Rubin Causal Model","heading":"3.6.5 Correlation with Potential Outcomes","text":"considering relationship treatment outcome, one important assumptions lack correlation treatment assignment potential outcomes. Consider version train experiment. Assume , Republican platform Spanish-speakers, attitude value X. platform hear Spanish, attitude X + 2. Democrat attitude X regardless whether platform Spanish-speakers. words, causal effect treatment group +2 Republicans, 0 Democrats. run experiment random assignment, discover average causal effect somewhere 0 2, depending relative proportion Republicans Democrats.R119+2R119+2D990R119+2D990 Unfortunately (?), people choose ride certain trains randomly. might assume/hope correlation train ride potential outcomes. true, still able estimate causal effect. Yet rarely true general. , instead, Republicans control group, Democrats treated? case, everyone attidue X! appears presence Spanish-speakers platform “matter.” correlation treatment potential outcomes invalidates naive estimate average treatment effect.R?9?R?9?D9??R?9?D9?? Keep mind problem arises correlation treatment assignment potential outcome, simply correlation treatment assignment outcome. case, correlation treatment assignment outcome zero! Everyone salary. Just looking outcomes observe enough. must make assumptions outcomes don’t observe, happened.","code":""},{"path":"rubin-causal-model.html","id":"conclusion","chapter":"3 Rubin Causal Model","heading":"3.7 Conclusion","text":"causal effect treatment single unit point time difference outcome variable treatment without treatment. fundamental problem causal inference impossible observe causal effect single unit. consequence, assumptions—models—must used order estimate missing data.","code":""},{"path":"functions.html","id":"functions","chapter":"4 Functions","heading":"4 Functions","text":"goal chapter reveal process long-time useR employs writing functions. also want illustrate process way . Merely looking finished product, e.g. source code R packages, can extremely deceiving. Reality generally much uglier … interesting! Powerful packages like dplyr purrr ready waiting apply purpose-built functions various bits data. can express analytic wishes function, tools give great power.","code":""},{"path":"functions.html","id":"introduction-to-functions","chapter":"4 Functions","heading":"4.1 Introduction to Functions","text":"First, let’s introduce concept functions. function piece code packaged way makes easy reuse. Functions make easy filter(), arrange(), select(), create tibble(), seen last chapters. Functions also allow transform variables perform bunch useful mathematical calculations, finding min(), max(), quantile()s dataset. Functions also important generating data values, using rnorm() randomly generate normal distribution.Note every time mention function, include parentheses. call function including parentheses necessary arguments parentheses. run function name without parentheses, R return code makes function.run function name without parentheses, R return code makes function.Functions can sorts things. , sample() takes vector values spits number random values vector. can specify number random values argument size. Let’s take spin. equivalent rolling die!Functions can also take functions arguments. example, replicate() takes expression repeats n specified times. really useful want generate many iterations data. replicated rolling die ten times?especially useful type function family map_* functions. map_* functions come purrr package, loaded loaded tidyverse. functions apply function every row tibble.Let’s create tibble 3 values: 3, 7, 2.wanted take square root value?map_dbl() (pronounced “map-double”) took function sqrt() applied element x. Note , passed function sqrt() map_dbl(), passed just name, without closing parentheses. code fails:sqrt() parentheses call function. , first thing R tries run function. fails sqrt() requires argument x, empty.Lesson: passing function map_* functions, pass just name.called map_* functions (plural) . know expected output function, can specify kind vector:map(): listmap_lgl(): logicalmap_int(): integermap_dbl(): double (numeric)map_chr(): charactermap_df(): data frameSo, since example produces numeric output, use map_dbl() instead map().Now, may wondering, ’s difference using mutate() map_* functions? answer lies iteration, specialization purrr package. map_* functions powerful ability apply functions every single element list, mutate() handle.Speaking lists, map_* functions known take list inputs, sometimes even create list-columns depending specific map_* function.Notice changed map_dbl() map(), returns list-column. can use str easily view contents list-column.… exactly list-column? next section give quick refresher lists, dive lists, list-columns, map_* functions relate.","code":"\nrnorm(1)## [1] 0.29\nrnorm## function (n, mean = 0, sd = 1) \n## .Call(C_rnorm, n, mean, sd)\n## <bytecode: 0x7fda5a835c80>\n## <environment: namespace:stats>\nsample(x = 1:6, size = 1)## [1] 1\nreplicate(10, sample(1:6, 1))##  [1] 4 2 2 1 5 3 6 5 2 5\nlibrary(tidyverse)\ntibble(x = c(3, 7, 2))## # A tibble: 3 x 1\n##       x\n##   <dbl>\n## 1     3\n## 2     7\n## 3     2\ntibble(x = c(3, 7, 2)) %>% \n  mutate(new = map_dbl(x, sqrt))## # A tibble: 3 x 2\n##       x   new\n##   <dbl> <dbl>\n## 1     3  1.73\n## 2     7  2.65\n## 3     2  1.41\ntibble(x = c(3, 7, 2)) %>% \n  mutate(new = map_dbl(x, sqrt()))## Error: Problem with `mutate()` input `new`.\n## x 0 arguments passed to 'sqrt' which requires 1\n## ℹ Input `new` is `map_dbl(x, sqrt())`.\ntibble(x = c(3, 7, 2)) %>% \n  mutate(new = map(x, sqrt))## # A tibble: 3 x 2\n##       x new      \n##   <dbl> <list>   \n## 1     3 <dbl [1]>\n## 2     7 <dbl [1]>\n## 3     2 <dbl [1]>\ntibble(x = c(3, 7, 2)) %>% \n  mutate(new = map(x, sqrt)) %>%\n  str()## tibble [3 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ x  : num [1:3] 3 7 2\n##  $ new:List of 3\n##   ..$ : num 1.73\n##   ..$ : num 2.65\n##   ..$ : num 1.41"},{"path":"functions.html","id":"lists-and-list-columns","chapter":"4 Functions","heading":"4.2 Lists and List-columns","text":"Let’s quickly refresh lists functions commonly deal lists. Recall list different atomic vector. Atomic vectors familiar us: element vector one value, thus atomic vector column dataset, observation gets single value. Lists, however, can contain vectors elements.object, x, contains numeric vector element. extract first element list?Right! Recall example Chapter 2 lists pepper jar contain pepper packets. Don’t forget square brackets!Let’s index lists make sure works. Remember lists can contain multitude data types. First, let’s create new lists. Pay attention placement list().can see, two objects defined lists. also displays data type element. list_1 list length 3, character, numeric, character. list_2 also list length 3, numeric, character, numeric. Note [1:3] denotes numeric vector length 3 x.Now, let’s extract first element new lists.might wondering, list x different lists list_1 list_2? Besides contents lists, wanted demonstrate various ways create lists. can directly input values inside list(), wrap values c() first create vector element.number built-R functions output lists. example, ggplots making store graph information lists.function returns multiple values can used create list output. example, consider following list x :take range() x, expect two values returned: minimum value maximum value numeric vector.output range(x) numeric vector two values, 2 7. turn output list, simply wrap line code list().want create tibble values range(x)?Notice 1x1 tibble one observation, list one element. element vector integers 2 7. Voila! just created *list-column**.list column column data list rather atomic vector. Like lists, can pipe str() read column easily.Note case crucial use tibble(), data.frame()! used data.frame() last example, wouldn’t worked wanted:Note output 2 x 1 dataframe double observations, one singular list.Wrapping function list() often go creating list columns. Let’s practice kenya dataset. add column dataset included quantiles poverty variable?First, load necessary PPBDS.data library, select relevant variables, group block. grouping curious poverty rates look aggregated block, rather individual observations.Next, create list-column wrapping quantile() list(). quantile() naturally produces numeric vector quantiles poverty, wrapping list() capture numeric vector list.let’s say wanted 1) subset dataset treatment == \"control\", 2) group block, 3) get summary() distance variable continent:","code":"\nx <- list(c(3, 7, 2))\nx## [[1]]\n## [1] 3 7 2\nx[[1]][1]## [1] 3\nlist_1 <- list(\"Three\", 7, \"Two\")\nlist_2 <- c(list(3), list(\"7\"), list(2))\n\nlist_1 %>% str()## List of 3\n##  $ : chr \"Three\"\n##  $ : num 7\n##  $ : chr \"Two\"\nlist_2 %>% str()## List of 3\n##  $ : num 3\n##  $ : chr \"7\"\n##  $ : num 2\nx %>% str()## List of 1\n##  $ : num [1:3] 3 7 2\nlist_1[[1]][1]## [1] \"Three\"\nlist_2[[1]][1]## [1] 3\ny <- ggplot(mtcars, aes(x = cyl, y = mpg)) +\n  geom_col()\ny %>% str()## List of 9\n##  $ data       :'data.frame': 32 obs. of  11 variables:\n##   ..$ mpg : num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##   ..$ cyl : num [1:32] 6 6 4 6 8 6 8 4 4 6 ...\n##   ..$ disp: num [1:32] 160 160 108 258 360 ...\n##   ..$ hp  : num [1:32] 110 110 93 110 175 105 245 62 95 123 ...\n##   ..$ drat: num [1:32] 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##   ..$ wt  : num [1:32] 2.62 2.88 2.32 3.21 3.44 ...\n##   ..$ qsec: num [1:32] 16.5 17 18.6 19.4 17 ...\n##   ..$ vs  : num [1:32] 0 0 1 1 0 1 0 1 1 1 ...\n##   ..$ am  : num [1:32] 1 1 1 0 0 0 0 0 0 0 ...\n##   ..$ gear: num [1:32] 4 4 4 3 3 3 3 4 4 4 ...\n##   ..$ carb: num [1:32] 4 4 1 1 2 1 4 2 2 4 ...\n##  $ layers     :List of 1\n##   ..$ :Classes 'LayerInstance', 'Layer', 'ggproto', 'gg' <ggproto object: Class LayerInstance, Layer, gg>\n##     aes_params: list\n##     compute_aesthetics: function\n##     compute_geom_1: function\n##     compute_geom_2: function\n##     compute_position: function\n##     compute_statistic: function\n##     data: waiver\n##     draw_geom: function\n##     finish_statistics: function\n##     geom: <ggproto object: Class GeomCol, GeomRect, Geom, gg>\n##         aesthetics: function\n##         default_aes: uneval\n##         draw_group: function\n##         draw_key: function\n##         draw_layer: function\n##         draw_panel: function\n##         extra_params: na.rm orientation\n##         handle_na: function\n##         non_missing_aes: xmin xmax ymin ymax\n##         optional_aes: \n##         parameters: function\n##         required_aes: x y\n##         setup_data: function\n##         setup_params: function\n##         use_defaults: function\n##         super:  <ggproto object: Class GeomRect, Geom, gg>\n##     geom_params: list\n##     inherit.aes: TRUE\n##     layer_data: function\n##     map_statistic: function\n##     mapping: NULL\n##     position: <ggproto object: Class PositionStack, Position, gg>\n##         compute_layer: function\n##         compute_panel: function\n##         fill: FALSE\n##         required_aes: \n##         reverse: FALSE\n##         setup_data: function\n##         setup_params: function\n##         type: NULL\n##         vjust: 1\n##         super:  <ggproto object: Class Position, gg>\n##     print: function\n##     setup_layer: function\n##     show.legend: NA\n##     stat: <ggproto object: Class StatIdentity, Stat, gg>\n##         aesthetics: function\n##         compute_group: function\n##         compute_layer: function\n##         compute_panel: function\n##         default_aes: uneval\n##         extra_params: na.rm\n##         finish_layer: function\n##         non_missing_aes: \n##         optional_aes: \n##         parameters: function\n##         required_aes: \n##         retransform: TRUE\n##         setup_data: function\n##         setup_params: function\n##         super:  <ggproto object: Class Stat, gg>\n##     stat_params: list\n##     super:  <ggproto object: Class Layer, gg> \n##  $ scales     :Classes 'ScalesList', 'ggproto', 'gg' <ggproto object: Class ScalesList, gg>\n##     add: function\n##     clone: function\n##     find: function\n##     get_scales: function\n##     has_scale: function\n##     input: function\n##     n: function\n##     non_position_scales: function\n##     scales: list\n##     super:  <ggproto object: Class ScalesList, gg> \n##  $ mapping    :List of 2\n##   ..$ x: language ~cyl\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   ..$ y: language ~mpg\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   ..- attr(*, \"class\")= chr \"uneval\"\n##  $ theme      : list()\n##  $ coordinates:Classes 'CoordCartesian', 'Coord', 'ggproto', 'gg' <ggproto object: Class CoordCartesian, Coord, gg>\n##     aspect: function\n##     backtransform_range: function\n##     clip: on\n##     default: TRUE\n##     distance: function\n##     expand: TRUE\n##     is_free: function\n##     is_linear: function\n##     labels: function\n##     limits: list\n##     modify_scales: function\n##     range: function\n##     render_axis_h: function\n##     render_axis_v: function\n##     render_bg: function\n##     render_fg: function\n##     setup_data: function\n##     setup_layout: function\n##     setup_panel_guides: function\n##     setup_panel_params: function\n##     setup_params: function\n##     train_panel_guides: function\n##     transform: function\n##     super:  <ggproto object: Class CoordCartesian, Coord, gg> \n##  $ facet      :Classes 'FacetNull', 'Facet', 'ggproto', 'gg' <ggproto object: Class FacetNull, Facet, gg>\n##     compute_layout: function\n##     draw_back: function\n##     draw_front: function\n##     draw_labels: function\n##     draw_panels: function\n##     finish_data: function\n##     init_scales: function\n##     map_data: function\n##     params: list\n##     setup_data: function\n##     setup_params: function\n##     shrink: TRUE\n##     train_scales: function\n##     vars: function\n##     super:  <ggproto object: Class FacetNull, Facet, gg> \n##  $ plot_env   :<environment: R_GlobalEnv> \n##  $ labels     :List of 2\n##   ..$ x: chr \"cyl\"\n##   ..$ y: chr \"mpg\"\n##  - attr(*, \"class\")= chr [1:2] \"gg\" \"ggplot\"\nx## [[1]]\n## [1] 3 7 2\nrange(x)## [1] 2 7\nclass(range(x))## [1] \"numeric\"\nlist(range(x))## [[1]]\n## [1] 2 7\nclass(list(range(x)))## [1] \"list\"\ntibble(col_1 = list(range(x)))## # A tibble: 1 x 1\n##   col_1    \n##   <list>   \n## 1 <dbl [2]>Lesson: If a function returns multiple values as a vector, like `range()` does, you can't use it directly to obtain a list-column, but you can wrap `list()` around it in order to get the same behavior.\ntibble(col_1 = list(range(x))) %>%\n  str()## tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] 2 7\ndata.frame(col_1 = list(range(x)))##   c.2..7.\n## 1       2\n## 2       7\nlibrary(PPBDS.data)\nkenya %>%\n  select(block, poverty) %>%\n  group_by(block)## # A tibble: 1,672 x 2\n## # Groups:   block [279]\n##    block    poverty\n##    <chr>      <dbl>\n##  1 KWALE/42   0.247\n##  2 KWALE/35   0.329\n##  3 KWALE/40   0.263\n##  4 KWALE/18   0.429\n##  5 KWALE/12   0.341\n##  6 KWALE/42   0.204\n##  7 KWALE/40   0.272\n##  8 KWALE/12   0.316\n##  9 KWALE/30   0.396\n## 10 KWALE/16   0.398\n## # … with 1,662 more rows\nlibrary(PPBDS.data)\nkenya %>%\n  select(block, poverty) %>%\n  group_by(block) %>%\n  mutate(povertyQuantile = list(quantile(poverty)))## # A tibble: 1,672 x 3\n## # Groups:   block [279]\n##    block    poverty povertyQuantile\n##    <chr>      <dbl> <list>         \n##  1 KWALE/42   0.247 <dbl [5]>      \n##  2 KWALE/35   0.329 <dbl [5]>      \n##  3 KWALE/40   0.263 <dbl [5]>      \n##  4 KWALE/18   0.429 <dbl [5]>      \n##  5 KWALE/12   0.341 <dbl [5]>      \n##  6 KWALE/42   0.204 <dbl [5]>      \n##  7 KWALE/40   0.272 <dbl [5]>      \n##  8 KWALE/12   0.316 <dbl [5]>      \n##  9 KWALE/30   0.396 <dbl [5]>      \n## 10 KWALE/16   0.398 <dbl [5]>      \n## # … with 1,662 more rows\nkenya %>%\n  select(treatment, block, distance) %>%\n  filter(treatment == \"control\") %>%\n  group_by(block) %>%\n  summarize(distSummary = list(summary(distance)))## # A tibble: 278 x 2\n##    block      distSummary\n##    <chr>      <list>     \n##  1 BUNGOMA/1  <table [6]>\n##  2 BUNGOMA/10 <table [6]>\n##  3 BUNGOMA/11 <table [6]>\n##  4 BUNGOMA/12 <table [6]>\n##  5 BUNGOMA/13 <table [6]>\n##  6 BUNGOMA/14 <table [6]>\n##  7 BUNGOMA/15 <table [6]>\n##  8 BUNGOMA/16 <table [6]>\n##  9 BUNGOMA/17 <table [6]>\n## 10 BUNGOMA/18 <table [6]>\n## # … with 268 more rows"},{"path":"functions.html","id":"using-map_-functions-to-create-list-columns","chapter":"4 Functions","heading":"4.2.1 Using map_* functions to create list-columns","text":"example, use nhanes dataset PPBDS.data package demonstrate use map_* functions create list columns.First, let’s wrangle data observation grouped age gender. ’ll create list column all_weights consists every subject’s weight, grouped age gender.Now list column, can use input map(), outputting another list column. Let’s say wanted new list column, log_weights, takes logarithmic value weight observation.Note took list column all_weights , applying anonymous function map(), created another list column log_weights. common process. similar taking tibble piping dplyr function (mutate()) gives new tibble can work .try replace map() function last line simple mutate(), get error., mutate() function ill-equipped handle lists. avoided creating list-columns first place found log weight observation, losing grouping ability lists. see map_* functions equipped handle inputs different mutate()?can also use map_* functions take list column input return atomic vector – column single value per observation – output. instance, let’s say now wanted mean log weights:, also see map_* functions ... argument, allows na.rm = TRUE passed along mean().\nwanted extract top five log weights per row?See chained map_* functions:all_weights used input map() create log_weightslog_weights used input map() create sorted_log_weightssorted_log_weights used input map() create top5_log_weightsUntil now, practiced using map functions built-R functions log() sort() create list columns. Next, show write functions!","code":"\nnhanes %>%\n  group_by(age, gender) %>%\n  summarize(all_weights = list(weight))\nnhanes %>%\n  group_by(age, gender) %>%\n  summarize(all_weights = list(weight)) %>%\n  mutate(log_weights = map(all_weights, log))## Error: Problem with `mutate()` input `log_weights`.\n## x non-numeric argument to mathematical function\n## ℹ Input `log_weights` is `log(all_weights)`.\n## ℹ The error occurred in group 1: age = 0.\nnhanes %>%\n  group_by(age, gender) %>%\n  summarize(all_weights = list(weight)) %>%\n  mutate(log_weights = map(all_weights, log),\n         mean_log_weights = map_dbl(log_weights, mean, na.rm = TRUE))\nnhanes %>%\n  group_by(age, gender) %>%\n  summarize(all_weights = list(weight)) %>%\n  mutate(log_weights = map(all_weights, log),\n         sorted_log_weights = map(log_weights, ~ sort(., decreasing = TRUE)),\n         top5_log_weights = map(sorted_log_weights, ~ .[1:5]))"},{"path":"functions.html","id":"custom-functions","chapter":"4 Functions","heading":"4.3 Custom Functions","text":"","code":""},{"path":"functions.html","id":"anonymous-functions-with-map_-functions","chapter":"4 Functions","heading":"4.3.1 Anonymous functions with map_* functions","text":"can create functions operations “fly” without bothering give name. nameless functions called anonymous functions.can use anonymous functions conjunction map_* family functions. ’re commonly used conduct mathematical operations repeatedly.can call anonymous function using ~ operator using . represent current element.Note parentheses necessary. long everything ~ works R code, anonymous function work, time replace . value .x variable — old case — value row.~ shorthand convenient get used . Let’s see example. ’ll use weather dataset nycflights13 package. Let’s say want return list-column recorded temperatures day Celsius basic summary statistics.First, let’s wrangle data observation day rather hour. selecting necessary variables grouping origin, year, month, day.Next, ’ll create list column temps_F consists temperatures recorded day particular origin. Note temperatures Fahrenheit default.Now list-column, can use input map(), outputting another list-column. Let’s say wanted new list-column, temps_C, records temperature Celsius.formula convert Fahrenheit Celsius \\(\\frac{(F-32)*5}{9}\\). Let’s “eyeball” results using str().Let’s “eyeball” first results using str().anonymous function! can see, anonymous functions used map_* function context get complex. next section teach write custom functions flexibility.","code":"\ntibble(old = c(3, 7, 2)) %>% \n  mutate(new = map_dbl(old, ~ (. + 1)))## # A tibble: 3 x 2\n##     old   new\n##   <dbl> <dbl>\n## 1     3     4\n## 2     7     8\n## 3     2     3\ntibble(old = c(3, 7, 2)) %>% \n  mutate(new = map_dbl(old, ~ . + 1))## # A tibble: 3 x 2\n##     old   new\n##   <dbl> <dbl>\n## 1     3     4\n## 2     7     8\n## 3     2     3\nweather %>%\n  select(origin, year, month, day, temp) %>%\n  group_by(origin, year, month, day)\nweather %>%\n  select(origin, year, month, day, temp) %>%\n  group_by(origin, year, month, day) %>%\n  summarize(temps_F = list(c(temp)))## # A tibble: 1,092 x 5\n## # Groups:   origin, year, month [36]\n##    origin  year month   day temps_F   \n##    <chr>  <int> <int> <int> <list>    \n##  1 EWR     2013     1     1 <dbl [22]>\n##  2 EWR     2013     1     2 <dbl [24]>\n##  3 EWR     2013     1     3 <dbl [24]>\n##  4 EWR     2013     1     4 <dbl [24]>\n##  5 EWR     2013     1     5 <dbl [24]>\n##  6 EWR     2013     1     6 <dbl [24]>\n##  7 EWR     2013     1     7 <dbl [24]>\n##  8 EWR     2013     1     8 <dbl [24]>\n##  9 EWR     2013     1     9 <dbl [24]>\n## 10 EWR     2013     1    10 <dbl [24]>\n## # … with 1,082 more rows\nweather %>%\n  select(origin, year, month, day, temp) %>%\n  group_by(origin, year, month, day) %>%\n  summarize(temps_F = list(c(temp))) %>%\n  mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9))## # A tibble: 1,092 x 6\n## # Groups:   origin, year, month [36]\n##    origin  year month   day temps_F    temps_C   \n##    <chr>  <int> <int> <int> <list>     <list>    \n##  1 EWR     2013     1     1 <dbl [22]> <dbl [22]>\n##  2 EWR     2013     1     2 <dbl [24]> <dbl [24]>\n##  3 EWR     2013     1     3 <dbl [24]> <dbl [24]>\n##  4 EWR     2013     1     4 <dbl [24]> <dbl [24]>\n##  5 EWR     2013     1     5 <dbl [24]> <dbl [24]>\n##  6 EWR     2013     1     6 <dbl [24]> <dbl [24]>\n##  7 EWR     2013     1     7 <dbl [24]> <dbl [24]>\n##  8 EWR     2013     1     8 <dbl [24]> <dbl [24]>\n##  9 EWR     2013     1     9 <dbl [24]> <dbl [24]>\n## 10 EWR     2013     1    10 <dbl [24]> <dbl [24]>\n## # … with 1,082 more rows\nweather %>%\n  select(origin, year, month, day, temp) %>%\n  group_by(origin, year, month, day) %>%\n  summarize(temps_F = list(c(temp))) %>%\n  mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9)) %>%\n  ungroup() %>%\n  slice(1:5) %>%\n  str()## tibble [5 × 6] (S3: tbl_df/tbl/data.frame)\n##  $ origin : chr [1:5] \"EWR\" \"EWR\" \"EWR\" \"EWR\" ...\n##  $ year   : int [1:5] 2013 2013 2013 2013 2013\n##  $ month  : int [1:5] 1 1 1 1 1\n##  $ day    : int [1:5] 1 2 3 4 5\n##  $ temps_F:List of 5\n##   ..$ : num [1:22] 39 39 39 39.9 39 ...\n##   ..$ : num [1:24] 27 26.1 25 24.1 24.1 ...\n##   ..$ : num [1:24] 28 28 27 27 26.1 ...\n##   ..$ : num [1:24] 30 28.9 30 28.9 28.9 ...\n##   ..$ : num [1:24] 33.1 33.1 32 32 32 ...\n##  $ temps_C:List of 5\n##   ..$ : num [1:22] 3.9 3.9 3.9 4.4 3.9 3.3 3.9 4.4 4.4 5 ...\n##   ..$ : num [1:24] -2.8 -3.3 -3.9 -4.4 -4.4 -4.4 -4.4 -3.9 -3.9 -2.8 ...\n##   ..$ : num [1:24] -2.2 -2.2 -2.8 -2.8 -3.3 -3.3 -3.3 -3.3 -2.8 -2.2 ...\n##   ..$ : num [1:24] -1.1 -1.7 -1.1 -1.7 -1.7 ...\n##   ..$ : num [1:24] 0.6 0.6 0 0 0 ..."},{"path":"functions.html","id":"creating-your-own-functions","chapter":"4 Functions","heading":"4.3.2 Creating your own functions","text":"plenty built-functions R, ones mentioned . can also create custom functions, may look something like :just created function! function return 1+1 whenever called.Now wanted leave mystery function? say, want add number 6 value x, user provides us.Congratulations! incorporated first formal argument. Formal arguments functions additional parameters allow user customize use function. Instead adding 1+1 , function takes number x user defines adds 6. Now let’s drive home make function two formal arguments.Great work! Now let’s create example apply kenya dataset.interested taking difference highest poverty rate lowest poverty rate? First, develop working code interactive use, using representative input. Use poverty variable. Built-R functions useful: min(), max(), range().Internalize “answer” informal testing relies noticing departures .","code":"\nadd_one_and_one <- function() {\n  1 + 1\n}\nadd_one_and_one()\nadd_six_to_something <- function(x){\n  x + 6\n}\nadd_six_to_something(1)## [1] 7\nadd_x_to_y <- function(x,y) {\n  x + y\n}\n\nadd_x_to_y(1,2)## [1] 3\nadd_x_to_y(4,3)## [1] 7\nmin(kenya$poverty)## [1] 0.18\nmax(kenya$poverty)## [1] 0.9\nrange(kenya$poverty)## [1] 0.18 0.90\nmax(kenya$poverty) - min(kenya$poverty)## [1] 0.72\nrange(kenya$poverty)[2] - range(kenya$poverty)[1]## [1] 0.72\ndiff(range(kenya$poverty))## [1] 0.72"},{"path":"functions.html","id":"skateboard-perfectly-formed-rear-view-mirror","chapter":"4 Functions","heading":"4.3.3 Skateboard >> perfectly formed rear-view mirror","text":"image — widely attributed Spotify development team — conveys important point.\nFIGURE 4.1: ultimate guide Minimum Viable Product (+great examples)\nBuild skateboard build car fancy car part. limited--functioning thing useful. also keeps spirits high.related valuable Telescope Rule:faster make four-inch mirror six-inch mirror make six-inch mirror.","code":""},{"path":"functions.html","id":"turn-the-working-interactive-code-into-a-function","chapter":"4 Functions","heading":"4.3.4 Turn the working interactive code into a function","text":"Add new functionality! Just write first R function.Check ’re getting answer interactive code. Test eyeball-o-metrically point.One thing note functions functions selective memory. fact, functions “remember” happens within function .example, saved value x 1.Defining x inside function change value outside function.","code":"\nmax_minus_min <- function(x) max(x) - min(x)\nmax_minus_min(kenya$poverty)## [1] 0.72\nx <- 1\nfunction(x) x <- 2## function(x) x <- 2\nx## [1] 1"},{"path":"functions.html","id":"test-your-function","chapter":"4 Functions","heading":"4.3.5 Test your function","text":"","code":""},{"path":"functions.html","id":"test-on-new-inputs","chapter":"4 Functions","heading":"4.3.5.1 Test on new inputs","text":"Pick new artificial inputs know (least approximately) function return.know 10 minus 1 9. know random uniform [0, 1] variates 0 1. Therefore max - min less 1. take LOTS , max - min pretty close 1.intentional tested integer input well floating point. Likewise, like use valid--random data sort check.","code":"\nmax_minus_min(1:10)## [1] 9\nmax_minus_min(runif(1000))## [1] 1"},{"path":"functions.html","id":"test-on-real-data-but-different-real-data","chapter":"4 Functions","heading":"4.3.5.2 Test on real data but different real data","text":"Back real world now. Two additional quantitative variables lying around: distance rv13. Let’s go.Either check results “hand” apply “even make sense?” test.","code":"\nmax_minus_min(kenya$distance)## [1] 179\nmax_minus_min(kenya$rv13)## [1] 6764"},{"path":"functions.html","id":"test-on-weird-stuff","chapter":"4 Functions","heading":"4.3.5.3 Test on weird stuff","text":"Now try break function. Don’t get truly diabolical (yet). Just make kind mistakes can imagine making 2am , 3 years now, rediscover useful function wrote. Give function inputs ’s expecting.happy error messages? must imagine entire script failed hoping just source() without re-reading . colleague future encountered errors, run screaming room? hard pinpoint usage problem?","code":"\nmax_minus_min(kenya)## Error in FUN(X[[i]], ...): only defined on a data frame with all numeric variables\n# Hey, sometimes things \"just work\" on data.frames!\n\nmax_minus_min(kenya$treatment)## Error in Summary.factor(structure(c(2L, 4L, 3L, 5L, 4L, 3L, 6L, 2L, 1L, : 'max' not meaningful for factors\n# Factors are kind of like integer vectors, no?\n\nmax_minus_min(\"eggplants are purple\")## Error in max(x) - min(x): non-numeric argument to binary operator\n# I have no excuse for this one"},{"path":"functions.html","id":"i-will-scare-you-now","chapter":"4 Functions","heading":"4.3.5.4 I will scare you now","text":"great examples function break .cases, R’s eagerness make sense requests unfortunately successful. first case, tibble containing just quantitative variables eventually coerced numeric vector. can compute max minus min, even though makes absolutely sense . second case, logical vector converted zeroes ones, might merit error least warning.","code":"\nmax_minus_min(kenya[c('poverty', 'distance', 'rv13')])## [1] 6764\nmax_minus_min(c(TRUE, TRUE, FALSE, TRUE, TRUE))## [1] 1"},{"path":"functions.html","id":"check-the-validity-of-arguments","chapter":"4 Functions","heading":"4.3.6 Check the validity of arguments","text":"functions used – ! – good check validity arguments. implements rule Unix philosophy:Rule Repair: must fail, fail noisily soon possible.","code":""},{"path":"functions.html","id":"stop-if-not","chapter":"4 Functions","heading":"4.3.6.1 stop if not","text":"stopifnot() entry level solution. use make sure input x numeric vector.see catches self-inflicted damage like avoid.","code":"\nmmm <- function(x) {\n  stopifnot(is.numeric(x))\n  max(x) - min(x)\n}\nmmm(kenya)## Error in mmm(kenya): is.numeric(x) is not TRUE\nmmm(kenya$treatment)## Error in mmm(kenya$treatment): is.numeric(x) is not TRUE\nmmm(\"eggplants are purple\")## Error in mmm(\"eggplants are purple\"): is.numeric(x) is not TRUE\nmmm(kenya[c('poverty', 'distance', 'rv13')])## Error in mmm(kenya[c(\"poverty\", \"distance\", \"rv13\")]): is.numeric(x) is not TRUE\nmmm(c(TRUE, TRUE, FALSE, TRUE, TRUE))## Error in mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE)): is.numeric(x) is not TRUE"},{"path":"functions.html","id":"if-then-stop","chapter":"4 Functions","heading":"4.3.6.2 if then stop","text":"stopifnot() doesn’t provide good error message. next approach widely used. Put validity check inside () statement call stop() , custom error message, body.addition gratuitous apology, error also contains two pieces helpful info:function threw error.Hints fix things: expected class input vs actual class.easy , highly recommend template: “gave , need .”tidyverse style guide useful chapter construct error messages.","code":"\nmmm2 <- function(x) {\n  if(!is.numeric(x)) {\n    stop('I am so sorry, but this function only works for numeric input!\\n',\n         'You have provided an object of class: ', class(x)[1])\n  }\n  max(x) - min(x)\n}\nmmm2(kenya)## Error in mmm2(kenya): I am so sorry, but this function only works for numeric input!\n## You have provided an object of class: tbl_df**Non-programming uses for assertions**\n\nAnother good use of this pattern is to leave checks behind in data analytical scripts. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run *ad nauseam* on auto-pilot or non-interactively."},{"path":"functions.html","id":"wrap-up-and-whats-next","chapter":"4 Functions","heading":"4.3.7 Wrap-up and what’s next?","text":"’s function ’ve written introduction built-custom functions:’ve accomplished:’ve written first function.checking validity input, argument x.’ve done good amount informal testing.","code":"\nmmm2## function(x) {\n##   if(!is.numeric(x)) {\n##     stop('I am so sorry, but this function only works for numeric input!\\n',\n##          'You have provided an object of class: ', class(x)[1])\n##   }\n##   max(x) - min(x)\n## }"},{"path":"functions.html","id":"argument-specifications-and-default-values","chapter":"4 Functions","heading":"4.4 Argument Specifications and Default Values","text":"section, create new function, generalize , learn technical details R functions.goal write function simulates throwing dice.begin creating minimally viable function, starter_die(), throws one die one time., can add formal argument n, requires number dice specified. However, one dice thrown, want make sure number 1-6 chance returned. , make sure replace argument sample() set TRUE. Now generalized function starter_dice()!wanted add formal argument, n, passed sample()’s n specification? Now let’s create intermediate function add_dice() throws n dice adds results.Congratulations! just generalized function take number dice. wait… really value? Let’s add checks make sure weird inputs break function.Oops! can see, tried use negative number dice, fly. Let’s alter informal checks stop function n negative.can see, function works takes sensible arguments. Next, let’s talk argument specifications conventions.","code":"\nstarter_die <- function() {\n  sample(1:6, 1)\n}\nstarter_die <- function(n) {\n  sample(1:6, n, replace = TRUE)\n}\nadd_dice <- function(n) {\n  sum(sample(1:6, n, replace = TRUE))\n}\nadd_dice <- function(n) {\n  stopifnot(is.numeric(n))\n  sum(sample(1:6, n, replace = TRUE))\n}\nadd_dice(-1)## Error in sample.int(length(x), size, replace, prob): invalid 'size' argument\nadd_dice <- function(n) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  sum(sample(1:6, n, replace = TRUE))\n}"},{"path":"functions.html","id":"argument-names-freedom-and-conventions","chapter":"4 Functions","heading":"4.4.1 Argument names: freedom and conventions","text":"Understand importance argument names. can name arguments almost anything like. Proof:can name argument famous painter, ’s usually bad idea. Take opportunities make things self-explanatory via meaningful names.going pass arguments function arguments built-function, consider copying argument names. Unless good reason thing (argument names bad!), consistent existing function. , reason reduce cognitive load.took detour see structural relationship argument (n) sample() (also includes n). similarity equivalence names accomplishes nothing far R concerned; solely benefit humans reading, writing, using code. important!","code":"\nadd_dice <- function(bob_ross) {\n  stopifnot(is.numeric(bob_ross))\n  stopifnot(bob_ross >= 0)\n  sum(sample(1:6, bob_ross, replace = TRUE))\n}\n\nadd_dice(bob_ross = 3)## [1] 8"},{"path":"functions.html","id":"default-values-freedom-to-not-specify-the-arguments","chapter":"4 Functions","heading":"4.4.2 Default values: freedom to NOT specify the arguments","text":"happens call function neglect specify probabilities?Oops! moment, causes fatal error. can nice provide reasonable default values certain arguments. Setting n = 1 reasonable default value.check function works specifying n specifying n.Note necessarily need supply argument name. R tries best understand based order provide argument values . However, usually best practice specify argument name.","code":"\nadd_dice()## Error in stopifnot(is.numeric(bob_ross)): argument \"bob_ross\" is missing, with no default\nadd_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  sum(sample(1:6, n, replace = TRUE))\n}\nadd_dice()## [1] 6\nadd_dice(10)## [1] 41"},{"path":"functions.html","id":"wrap-up-and-whats-next-1","chapter":"4 Functions","heading":"4.4.3 Wrap-up and what’s next?","text":"’s function ’ve written far:’ve accomplished:’ve generalized dice throwing function take custom value nWe’ve specified default value: n=2","code":"\nadd_dice## function(n = 1) {\n##   stopifnot(is.numeric(n))\n##   stopifnot(n >= 0)\n##   sum(sample(1:6, n, replace = TRUE))\n## }\n## <bytecode: 0x7fda5c5fe790>"},{"path":"functions.html","id":"formal-testing","chapter":"4 Functions","heading":"4.5 Formal Testing","text":"section, tackle NAs, special argument ... formal testing.","code":""},{"path":"functions.html","id":"use-testthat-for-formal-unit-tests","chapter":"4 Functions","heading":"4.5.1 Use testthat for formal unit tests","text":"now, ’ve relied informal tests trying break function silly inputs. going use function lot, especially part package, wise use formal unit tests.[testthat][testthat-web] package ([CRAN][testthat-cran]; [GitHub][testthat-github]) provides excellent facilities , distinct emphasis automated unit testing entire packages. However, can take test drive even one measly function.construct test test_that() , within , put one expectations check actual expected results. simply harden informal, interactive tests formal unit tests. examples tests indicative expectations.Let’s keep first function around baseline.news good news! Let’s see test failure look like. Let’s revert version function check positive n value test . can watch fail.Similar advice use assertions data analytical scripts, recommend use unit tests monitor behavior functions (others) use often. tests cover function’s important behavior, can edit internals freely. ’ll rest easy knowledge , broke anything important, tests fail alert problem. function important enough unit tests probably also belongs package, obvious mechanisms running tests part overall package checks.","code":"\nadd_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  sum(sample(1:6, n, replace = TRUE))\n}\nlibrary(testthat)\n\ntest_that('invalid args are detected', {\n  expect_error(add_dice(\"desserts are great!\"))\n  expect_error(add_dice(kenya))\n})## Test passed 🥇\nadd_dice <- function(n = 2) {\n  stopifnot(is.numeric(n))\n  sum(sample(1:6, n, replace = TRUE))\n}\n\nshow_failure(add_dice(-1))## Error in sample.int(length(x), size, replace, prob): invalid 'size' argument"},{"path":"functions.html","id":"what-a-function-returns","chapter":"4 Functions","heading":"4.5.2 What a function returns","text":"default, function returns result last line body. just letting happen line sum(sample(1:6, n, replace = TRUE)). However, explicit function : return(). just easily make last line function’s body:absolutely must use return() want return early based condition, .e. execution gets last line body. Otherwise, can decide conventions use return() don’t.Right now, running add_dice(5) gives us integer. want output list roll outcome?Note achieve , took sum() swapped list(). new function, show_dice(), returns list n dice thrown.","code":"\nreturn(sum(sample(1:6, n, replace = TRUE)))\nshow_dice <- function(n = 2) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  return(list(sample(1:6, n, replace = TRUE)))\n}\nshow_dice(5)## [[1]]\n## [1] 3 6 3 2 4"},{"path":"functions.html","id":"handling-nas","chapter":"4 Functions","heading":"4.5.3 Handling NAs","text":"upside creating dice-rolling function missing values. real life, missing data make life living hell. lucky, properly indicated special value NA, don’t hold breath. Many built-R functions na.rm = argument can specify want handle NAs. Typically default value na.rm = FALSE typical default behavior either let NAs propagate raise error. Let’s see quantile() handles NAs:quantile() simply operate presence NAs unless na.rm = TRUE. shall modify function?wanted hardwire na.rm = TRUE, . Focus call quantile() inside definition new function, quantile_diff().works dangerous invert default behavior well-known built-function provide user way override .add na.rm = argument function. might even enforce preferred default – least ’re giving user way control behavior around NAs.","code":"\nz <- kenya$poverty\nz[3] <- NA\nquantile(kenya$poverty)##   0%  25%  50%  75% 100% \n## 0.18 0.35 0.43 0.49 0.90\nquantile(z)## Error in quantile.default(z): missing values and NaN's not allowed if 'na.rm' is FALSE\nquantile(z, na.rm = TRUE)##   0%  25%  50%  75% 100% \n## 0.18 0.35 0.43 0.49 0.90\nquantile_diff <- function(x, probs = c(0, 1)) {\n  stopifnot(is.numeric(x))\n  the_quantiles <- quantile(x, probs, na.rm = TRUE)\n  max(the_quantiles) - min(the_quantiles)\n}\nquantile_diff(kenya$poverty)## [1] 0.72\nquantile_diff(z)## [1] 0.72\nquantile_diff <- function(x, probs = c(0, 1), na.rm = TRUE) {\n  stopifnot(is.numeric(x))\n  the_quantiles <- quantile(x, probs, na.rm = na.rm)\n  max(the_quantiles) - min(the_quantiles)\n}\nquantile_diff(kenya$poverty)## [1] 0.72\nquantile_diff(z)## [1] 0.72\nquantile_diff(z, na.rm = FALSE)## Error in quantile.default(x, probs, na.rm = na.rm): missing values and NaN's not allowed if 'na.rm' is FALSE"},{"path":"functions.html","id":"the-useful-but-mysterious-...-argument","chapter":"4 Functions","heading":"4.5.4 The useful but mysterious ... argument","text":"probably lived long happy life without knowing least 9 different algorithms computing quantiles. [Go read type argument][rdocs-quantile] quantile(). TLDR: quantile unambiguously equal observed data point, must somehow average two data points. can weight average different ways, depending rest data, type = controls .Let’s say want give user function ability specify quantiles computed, want accomplish little fuss possible. fact, don’t even want clutter function’s interface ! calls special ... argument. English, set three dots frequently called “ellipsis.”Thanks [@wrathematics][twitter-wrathematics], ’s small example can (barely) detect difference due type.Now can call function, requesting quantiles computed different ways.difference may subtle, ’s . Marvel fact passed type = 1 quantile() even though formal argument function.special argument ... useful want ability pass arbitrary arguments another function, without constantly expanding formal arguments function. leaves less cluttered function definition gives future flexibility specify arguments need . ... argument used conjunction map_* functions.also downsides ..., use intention. package, work harder create truly informative documentation user. Also, quiet, absorbent properties ... mean can sometimes silently swallow named arguments, user typo name. Depending whether fails, can little tricky find went wrong.ellipsis package provides tools help package developers use ... safely. -progress tidyverse principles guide provides guidance design functions take ... Data, dots, details.","code":"\nquantile_diff <- function(x, probs = c(0, 1), na.rm = TRUE, ...) {\n  the_quantiles <- quantile(x = x, probs = probs, na.rm = na.rm, ...)\n  max(the_quantiles) - min(the_quantiles)\n}\nset.seed(1234)\nz <- rnorm(10)\nquantile(z, type = 1)##    0%   25%   50%   75%  100% \n## -2.35 -0.89 -0.56  0.43  1.08\nquantile(z, type = 4)##    0%   25%   50%   75%  100% \n## -2.35 -1.05 -0.56  0.35  1.08\nall.equal(quantile(z, type = 1), quantile(z, type = 4))## [1] \"Mean relative difference: 0.18\"\nquantile_diff(z, probs = c(0.25, 0.75), type = 1)## [1] 1.3\nquantile_diff(z, probs = c(0.25, 0.75), type = 4)## [1] 1.4"},{"path":"functions.html","id":"the-crooked-casino","chapter":"4 Functions","heading":"4.6 The Crooked Casino","text":"","code":""},{"path":"functions.html","id":"restoring-add_dice","chapter":"4 Functions","heading":"4.6.1 Restoring add_dice()","text":"section, tieing concepts map_* functions, list-columns, function writing. ’ll also introducing important conditional functions (ifelse(), (), (), case_when()).Let’s return dice-throwing function, add_dice().Next, create roll_dice(), calls add_dice(n = 2) many times user specifies. , combining rep() function map_int().rep() useful input map_* want call function input multiple times. rep(2, n) creates vector length n every element 2. use input map_int() want input add_dice() 2 every time (always throwing pair dice) want perform operation n times, chosen user.Create tibble named x one variable: throws. throws list column, element three throws dice pair, .e., result calling roll_dice(n = 3). Thus, tibble ten rows two columns.Add variable x called first_seven TRUE first roll throws 7.see [[1]] extract first element list, just like [1] extracts first element atomic vector. ifelse() takes first argument condition; TRUE returns second argument (TRUE) third (FALSE).Add variable x called a_winner TRUE least one three throws 7 11 FALSE otherwise.use (): () checks element input TRUE. , let’s say particular throw 4, 6, 7: c(7, 11) %% c(4, 6, 7) returns vector TRUE FALSE (since 7 vector 11 isn’t); , (c(7, 11) %% c(4, 6, 7)) returns TRUE one conditions TRUE.Run str() x show results.","code":"\nadd_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  sum(sample(1:6, n, replace = TRUE))\n}\nroll_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  map_int(rep(2, n), add_dice)\n}\nroll_dice(1)## [1] 12\nroll_dice(5)## [1] 10  9  7  9  7\nx <- tibble(throws = map(rep(3, 10), roll_dice))\nx## # A tibble: 10 x 1\n##    throws   \n##    <list>   \n##  1 <int [3]>\n##  2 <int [3]>\n##  3 <int [3]>\n##  4 <int [3]>\n##  5 <int [3]>\n##  6 <int [3]>\n##  7 <int [3]>\n##  8 <int [3]>\n##  9 <int [3]>\n## 10 <int [3]>\nlibrary(magrittr)\n\n# The magrittr package allows us to use %<>%, the compound assignment pipe\n# operator, which (as its name suggests) both assigns and pipes\n\nx %<>% \n  mutate(first_seven = map_lgl(throws, ~ ifelse(.[[1]] == 7, TRUE, FALSE)))\nx %<>% \n  mutate(a_winner = map_lgl(throws, ~ ifelse(any(c(7, 11) %in% .), TRUE, FALSE)))\nstr(x)## tibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ throws     :List of 10\n##   ..$ : int [1:3] 8 7 7\n##   ..$ : int [1:3] 4 10 5\n##   ..$ : int [1:3] 7 7 9\n##   ..$ : int [1:3] 7 6 3\n##   ..$ : int [1:3] 4 8 4\n##   ..$ : int [1:3] 9 3 11\n##   ..$ : int [1:3] 4 5 7\n##   ..$ : int [1:3] 10 5 8\n##   ..$ : int [1:3] 11 7 5\n##   ..$ : int [1:3] 8 4 12\n##  $ first_seven: logi [1:10] FALSE FALSE TRUE TRUE FALSE FALSE ...\n##  $ a_winner   : logi [1:10] TRUE FALSE TRUE TRUE FALSE TRUE ..."},{"path":"functions.html","id":"running-simulations","chapter":"4 Functions","heading":"4.6.2 Running Simulations","text":"Calculate “surprised” someone rolls three winners row. First, create tibble 10,000 rows. Include throws list column three throws dice, just part ). Second, create column called perfection TRUE three throws either 7 11., use (), structure (), checks whether elements input TRUE.Approximately 1.1% three rolls pair fair dice equal either 7 11.friend proposes following bet. roll pair fair dice 10 times. Side gets second highest first 4 rolls. Side B gets 1 plus median remaining 6 rolls. side likely win? ’s chance tie?can think case_when() generalized ifelse(). syntax little complicated. expression followed ~ returned expression TRUE. final expression, TRUE, always true, thus residual category none expression TRUE. Thus, second case_when() code print “” winner replications B, “B” B winner , “# Victories” otherwise.Side B likely win.chance tie approximately 11%.","code":"\nsurprised <- tibble(throws = map(rep(3, 10000), roll_dice)) %>% \n  mutate(perfection = map_lgl(throws, ~ ifelse(all(. %in% c(7, 11)), TRUE, FALSE)))\n\nsurprised %>%\n  pull(perfection) %>%\n  mean()## [1] 0.011\nbet <- tibble(throws = map(rep(10, 10000), roll_dice)) %>% \n  mutate(A = map_dbl(throws, ~ sort(.[1:4])[3]),\n         B = map_dbl(throws, ~ 1 + median(.[5:10])),\n         winner = case_when(A > B ~ \"A\",\n                            B > A ~ \"B\",\n                            TRUE ~ \"tie\"))\n\ncase_when(sum(bet$winner == \"A\") > sum(bet$winner == \"B\") ~ \"A\",\n          sum(bet$winner == \"B\") > sum(bet$winner == \"A\") ~ \"B\",\n          TRUE ~ \"Same # Victories\")## [1] \"B\"\nsum(bet$winner == \"tie\")/length(bet$winner)## [1] 0.11"},{"path":"functions.html","id":"making-a-crooked-function","chapter":"4 Functions","heading":"4.6.3 Making a Crooked Function","text":"Say establishment known Crooked Casino. game craps, win roll 7 11 sum pair dice. Crooked Casino infamous rigging games difficult obtain 7 11 sum. job write function mimics behavior.First, let’s use . add line code add_dice() assigns sum(sample(1:6, n, replace = TRUE)) variable, roll, returns roll.different add_dice(), may ask? . merely assigning outcome roll pair dice variable roll, useful moment. Recall functions “remember” takes place within . means call crooked_dice() creates new variable roll takes place inside function.Now, let’s use ifelse() check whether 7 11 rolled, , assign roll 2 instead.Crooked Casino bit sly, however. know overtly crooked game craps never returned 7 11, one play. want little subtle crookedness, determine half time 7 11 rolled outcome altered.can simulate decision generating random variable using runif() checking whether result greater equal 0.5.Now can create function named crooked_craps() calls crooked_dice() many times specified.’s hard tell whether crooked_craps() working properly rolling just , even ten times. Let’s follow steps took create tibble 10 observations 10 rolls named crooked_x.","code":"\ncrooked_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  roll <- sum(sample(1:6, n, replace = TRUE))\n  return(roll)\n}\n\ncrooked_dice(2)## [1] 7\ncrooked_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  roll <- sum(sample(1:6, n, replace = TRUE))\n  ifelse(roll == 7 | roll == 11, 2, roll)\n}\ncrooked_dice <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  stopifnot(n >= 0)\n  roll <- sum(sample(1:6, n, replace = TRUE))\n  ifelse((roll == 7 | roll == 11) && runif(1) >= 0.5, 2, roll)\n}\ncrooked_craps <- function(n = 1) {\n  stopifnot(is.numeric(n))\n  map_dbl(rep(2, n), crooked_dice)\n}\ncrooked_craps(1)## [1] 6\ncrooked_craps(5)## [1]  6 10  9  4  4\ncrooked_x <- tibble(throws = map(rep(10, 10), crooked_craps))\ncrooked_x %>%\n  str()## tibble [10 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ throws:List of 10\n##   ..$ : num [1:10] 2 8 9 8 7 6 3 4 6 5\n##   ..$ : num [1:10] 10 5 8 2 6 9 3 2 5 3\n##   ..$ : num [1:10] 5 5 2 8 4 9 10 2 5 8\n##   ..$ : num [1:10] 12 4 3 5 5 5 6 4 6 2\n##   ..$ : num [1:10] 5 4 6 12 9 2 7 2 2 5\n##   ..$ : num [1:10] 12 5 2 4 2 12 10 9 6 5\n##   ..$ : num [1:10] 6 7 3 6 10 4 9 5 3 9\n##   ..$ : num [1:10] 6 8 5 8 8 3 7 6 4 11\n##   ..$ : num [1:10] 6 8 5 6 3 2 5 10 7 7\n##   ..$ : num [1:10] 6 5 8 6 9 6 9 8 7 2"},{"path":"probability.html","id":"probability","chapter":"5 Probability","heading":"5 Probability","text":"central tension, opportunity, data science interplay data science, empirical observations models use understand . Probability language use explore interplay; connects models data, data models.","code":""},{"path":"probability.html","id":"probability-distributions","chapter":"5 Probability","heading":"5.1 Probability distributions","text":"\nFIGURE 5.1: Dice Probability.\nmean Trump 30% chance winning re-election fall? 90% probability rain today? dice casino unfair? roll dice 10 times five times get three, likely dice unfair?Probability quantifying uncertainty. can think probability proportion. probability event occurring number 0 1, 1 means event 100% certain.Let’s begin simplest events: coin flips dice rolls. dice \ncoins fair, can operate assumption outcomes \nequally likely.allows us make following statements:probability rolling 1 2 2/6, 1/3.probability rolling 1, 2, 3, 4, 5, 6 1.probability flipping coin getting tails 1/2.purposes course, probability distribution mathematical object covers set outcomes, distinct outcome chance occurring 0 1 inclusive. set possible outcomes — heads tails coin, 1 6 dice — can either discrete continuous. set outcomes domain probability distribution. three types probability distributions: mathematical, empirical, posterior. Let’s walk examples better understand type probability distribution.continue, familiar notation used chapter. Whenever talking specific probability (represented single value), use \\(p\\) subscript. instance, \\(p_h\\) = 0.5 denotes probability getting heads coin toss coin fair. \\(p_t\\) denotes probability getting tails coin toss. However, referring entire probability distribution set outcomes, use \\(p\\) parentheses. example, probability distribution coin toss \\(p(coin)\\). said, \\(p(coin)\\) composed two specific probabilities mapped domain.","code":""},{"path":"probability.html","id":"flipping-a-coin","chapter":"5 Probability","heading":"5.1.1 Flipping a coin","text":"mathematical distribution based mathematical formulas. Assuming dice perfectly fair, get heads many times get tails.empirical distribution based data. can think probability distribution created running simulation. theory, increase number coins flip simulation, empirical distribution look similar mathematical distribution. probability distribution Platonic form. empirical distribution often look like mathematical probability distribution, rarely exactly .simulation, 49 heads 51 tails. outcome vary every time run simulation, proportion heads tails different coin fair.posterior distribution based beliefs expectations. displays belief things can’t see right now. may posterior distributions events past, present, future.case coin toss, posterior distribution changes depending beliefs. instance, let’s say friend brought coin school asked bet . result heads, pay $5.makes suspicious; posterior distribution reflect . might believe \\(p_h\\) 0.95 \\(p_t\\) 0.05.","code":""},{"path":"probability.html","id":"rolling-two-dice","chapter":"5 Probability","heading":"5.1.2 Rolling two dice","text":"mathematical distribution tells us , fair dice, probability getting 1, 2, 3, 4, 5, 6 equal: 1/6 chance . roll two dice time sum values, numbers closest middle (ex. 6, 7, 8) common sums numbers edge (ex. 2, 12) combinations numbers add middle values.get empirical distribution running simulation rolling two dice hundred times. result identical mathematical distribution inherent randomness real world. can observe \\(p_2\\) \\(p_12\\), displayed left right ends distribution, far lower \\(p_6\\), example.posterior distribution rolling one dice hundred depends expectations. take dice Monopoly set, reason believe assumptions underlying mathematical distribution true. However, walk crooked casino host asks play craps, might suspicious. craps, come-roll 7 11 “natural” win. might expect numbers occur less often fair dice. Meanwhile, come-roll 2, 3 bad someone betting ’s known “Pass line.” might also expect values like 2 3 occur frequently. posterior distribution might look like :Someone less suspcious casina posterior distribution looks like mathematical distribution.","code":""},{"path":"probability.html","id":"presidential-elections","chapter":"5 Probability","heading":"5.1.3 Presidential elections","text":"Now let’s say building probability distributions political events, like presidential election. want know probability Democratic candidate wins x electoral votes, x represents range possible outcomes 0 538.incredibly naive create following mathematical distribution. create distribution running simulation 10,000 times, chances Democratic candidate winning given electoral college vote 0.5.know campaign platforms, donations, charisma, many factors contribute candidate’s likeability. Elections complicated coin tosses.empirical distribution case involve looking past elections United States counting number electoral votes Democrats won . empirical distribution, create tibble electoral vote results past elections. Looking elections since 1964, can observe number electoral votes Democrats received one different. Given 14 entries, difficult draw conclusions make predictions based empirical distribution.However, model enough suggest assumptions mathematical probability distribution work electoral votes. model assumes Democrats 50% chance receiving 538 votes. Just looking mathematical probability distribution, can observe receiving 13 17 486 votes 538 extreme almost impossible mathematical model. However, empirical distribution tells us real election results.posterior distribution election case something data scientists devote lot time . analysts develop algorithms consider expectations outcome. Consider generated using data FiveThirtyEight.posterior FiveThirtyEight website August 13, 2020. created using data distribution, simply displayed differently. electoral result, height bar represents probability given event occur. However, numbered y-axis telling us specific probability outcome .posterior Economist, also August 13, 2020. looks confusing first data analysts chosen merge axes Republican Democratic electoral votes. can tell Economist optimistic Biden’s odds election compared Trump’s, relative FiveThirtyEight.two models, built smart people using similar data sources, reached fairly different conclusions. Data science difficult! one “right” answer. Real life problem set.\nFIGURE 5.2: Watch makers two models throw shade Twitter! Eliot Morris one primary authors Economist model. Nate Silver charge 538. don’t seem impressed ’s work! smack talk .\nmany political science questions explore posterior distributions can relate past, present, future.Past: many electoral votes Hilary Clinton won picked different VP?Present: median height Harvard students?Future: many electoral votes presidential candidate win?","code":""},{"path":"probability.html","id":"unnormalized-distributions","chapter":"5 Probability","heading":"5.1.4 Unnormalized distributions","text":"Remember probability distributions mathematical objects cover set outcomes, outcome domain mapped probability value 0 1 inclusive sum mappings 1. Sometimes, may see distributions similar probability distributions, y-axis displays raw counts instead proportions. Unnormalized distributions probability distributions, easy convert two. simply divide outcome counts y-axis sum outcome counts “normalize” unnormalized distribution. Unnormalized distributions often intermediary step; often handy work counts end.instance, can generate following unnormalized distribution sum rolling two dice (empirical distribution running simulation 100 times). may notice shape distribution empirical probability distribution generated earlier, y-axis labeled differently.","code":""},{"path":"probability.html","id":"joint-distributions","chapter":"5 Probability","heading":"5.1.5 Joint distributions","text":"Now understand \\(p_A\\) can represent probability distribution outcomes event . Let’s talk joint distributions, can represented \\(p_{, B}\\). Joint distributions also mathematical objects cover set outcomes, distinct outcome chance occurring 0 1 sum chances equals 1. key joint distribution measures chance events B occur simultaneously.Let’s say rolling two six-sided dice simultaneously. One weighted 50% chance rolling 6 10% chance values. weighted 50% chance rolling 5 10% chance rolling values. Let’s roll dice 1000 times. previous examples involving two dice, cared sum results outcomes first versus second rolls simulation. joint distributions, order matters instead 11 possible outcomes x-axis distribution graph (ranging 2 12), 36. Furthermore, 2D probability distribution sufficient represent variables involved, joint distribution example displayed using 3D plot.","code":""},{"path":"probability.html","id":"tree-diagrams","chapter":"5 Probability","heading":"5.2 Tree diagrams","text":"","code":""},{"path":"probability.html","id":"independence","chapter":"5 Probability","heading":"5.2.1 Independence","text":"far, learned calculate \\(p_A\\), fancy, statistical way saying probability event known . flipping coin, probability getting heads 1/2. also learned compute \\(p_{B}\\). means probability either B happening. rolling dice, \\(p_{1 2}\\)–probability getting 1 2–1/3.flipped 2 coins? know probability getting heads 1/2, odds getting heads 2 times row? Let’s take look tree diagram. read diagram left right. left, probability getting heads 0.5. Now tree branches .got heads first time, go top branch. probability getting heads 0.5.got tails first time, go bottom branch. probability getting heads 0.5.Notice regardless get first time flip coin, probability getting heads 0.5 throughout. suggests coin flips independent. result one coin flip impact likelihood getting result next time. Take look tree diagram. \\(p_{H given H}\\) represents probability getting heads given got heads first time. \\(p_{H given T}\\) represents probability getting heads given got tails first time. \\(p_{H given H}\\) = \\(p_{H given T}\\).","code":""},{"path":"probability.html","id":"conditional-probability","chapter":"5 Probability","heading":"5.2.2 Conditional probability","text":"Now imagine 60% people community disease. doctor develops test determine random person disease. However, test isn’t 100% accurate. test 80% sure correctly returning positive person disease 90% sure correctly returning negative person disease.tree diagram illustrates exactly . Starting left, see probability random person disease 0.6. Since either disease don’t (two possibilities), probability don’t disease 1 - 0.4.Now tree branches .random person disease, go top branch. probability person testing positive 0.8 test 80% sure correctly returning positive person disease.logic, random person disease, go bottom branch. probability person incorrectly testing positive 0.1.decide go top branch random person disease. go bottom branch . called conditional probability. probability testing positive dependent whether person disease.express statistical notation? \\(p(|B)\\) thing probability given B. \\(p(|B)\\) essentially means probability know sure value B. also remember \\(p(|B)\\) thing \\(p(B|)\\).concept conditional probability relevant everyday lives. example, probability Trump’s re-election might different probability Trump’s re-election given recession. probability reading textbook might vary, depending likelihood instructor cold-calling class. Whenever encounter question involving conditional probability, drawing tree diagram useful approach visualization.","code":""},{"path":"probability.html","id":"two-diagrams-for-one-set-of-coin-flips","chapter":"5 Probability","heading":"5.2.3 Two diagrams for one set of coin flips","text":"histogram probability distribution, didn’t care combinations numbers made sum. cared outcome. two ways thinking coin toss well. tree diagram look different depending whether order results matters.understand tree diagrams little better, imagine ’re flipping 2 coins. tree diagram may look different depending ’re interested measuring. figure, order coin toss results matter. can imagine rolling two coins together, care sum two dice values.following figure, order coin toss results matter. Depending first result, go different branch.","code":""},{"path":"probability.html","id":"two-models","chapter":"5 Probability","heading":"5.3 Two models","text":"city 100,000 people. true exactly 1% population disease, don’t know people . terms count, means 1,000 people disease 100,000 population. test 99% accurate. , disease 99% chance test reports , don’t disease., 1,000 people truly disease — 1,000 1% 100,000 people city — 990 correctly test positive 10 incorrectly test negative. city also 99,000 people disease, 990 incorrectly test positive 98,010 correctly test negative. means number people test positive disease 1,980 990 disease. , simple terms, probability disease given positive test \\(990\\div1980\\)!joint empirical distribution test result disease status. Note unnormalized distribution dot represents person city.joint distribution displayed 3D. Instead using “jitter” feature R unstack dots, using 3D plot visualize number dots box. number people correctly test negative city 98,010, far outweighing categories. 990 false positives, 990 true positives, 10 false negatives. can barely see 3D bar coming sections.section called “Two Models” , person, two possible states world: disease disease. assumption, possibilities. call two possible states world “models,” even though simple models.addition two models, two possible results experiment given person: test positive test negative. , assumption. allow outcome. later sections, look complex situations consider two models two possible results experiment. meantime, built unnormalized joint distribution models results. key point! Look back earlier chapter discussed unnormalized distributions joint distributions.Getting back figure, difference distributions? can useful data analysis?want analyze plots looking different slices. instance, let’s say tested positive disease. Since test always accurate, 100% certain . isolate slice test result equals 1 (meaning positive). zoom plot, 990 people tested positive disease 990 tested positive disease. case, focusing one slice probability distribution test result positive. two disease outcomes: positive negative. isolating section, looking conditional distribution. Conditional positive test, can visualize likelihood actually disease versus .looks like take slice test result positive zoom . Taking slice thing creating conditional probability distribution.rotate slice look 2D perspective. looks similar saw beginning chapter flipped coins create probability distribution.Stat 110 Animations video really good job explaining similar concept.","code":""},{"path":"probability.html","id":"three-models","chapter":"5 Probability","heading":"5.4 Three models","text":"Now, imagine friend gives bag two marbles. either two white marbles, two black marbles, one color. Thus, bag contain 0% white marbles, 50% white marbles, 100% white marbles. \\(p\\) white marbles 0, 0.5, 1.Let’s say take marble bag, record whether ’s black white, return bag. repeat three times, observing number white marbles see three trials. get three whites, two whites, one white, zero whites result trial. Let’s make call Bayes scatterplot . three models (three different proportions white marbles bag) four possible experimental results.scatterplot visualization scenario:3D visualization:Let’s say ’ve got zero white marbles. isolate slice result simulation involves three black marbles zero white ones. unnormalized probability distribution.Next, let’s normalize distribution.plot makes sense three marbles draw bag white, pretty good chance white marbles bag. smaller chance bag contains one black marble one white marble. However, chance bag contains white marbles.","code":"\nThe y-axes of both the scatterplot and the 3D visualization are labeled \"Number of White Marbles in the Bag.\" This is a model, a belief about the world you are not sure of. For instance. When the model is 0, we have no white marbles in the bag, meaning that none of the marbles we pull out in the sample will be white.\n"},{"path":"probability.html","id":"n-models","chapter":"5 Probability","heading":"5.5 N models","text":"Assume coin \\(p_h\\). guarantee 11 possible values \\(p_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). words, 11 possible models, 11 things might true world. just like situations previously discussed, except models consider.going run experiment flip coin 20 times record number heads. result tell value \\(p_h\\)? Ultimately, want calculate posterior distribution \\(p_h\\), written p(\\(p_h\\)).start, useful consider things might happen , example, \\(p_h = 0.4\\). Fortunately, R functions simulating random variables makes easy.First, notice many different things can happen! Even know, certain, \\(p_h = 0.4\\), many outcomes possible. Life remarkably random. Second, likely result experiment 8 heads, expect. Third, transformed raw counts many times total appeared probability distribution. Sometimes, however, convenient just keep track raw counts. shape figure cases.Either way, figures show happened model — \\(p_h = 0.4\\) — true.can thing 11 possible models, calculating happen true. somewhat counterfactual since one can true. Yet assumption allow us create joint distribution models might true data experiment might generate. Let’s simplify p(models, data), although keep precise meaning mind.3D version plot.idagrams, see 11 models 21 outcomes. don’t really care p(\\(models\\), \\(data\\)), joint distribution models--might--true data---experiment-might-generate. Instead, want estimate \\(p\\), unknown parameter determines probability coin come heads tossed. joint distribution alone can’t tell us . created joint distribution even conducted experiment. creation, tool use make inferences. Instead, want conditional distribution, p(\\(models\\) | \\(data = 8\\)). results experiment. results tell us probability distribution \\(p\\)?answer question, simply take vertical slice joint distribution point x-axis corresponding results experiment.part joint distribution care . aren’t interested object looks like , example, number heads 11. portion irrelevant observed 8 heads, 11. using filter function simulation tibble created, can conclude total 465 times simulation 8 heads observed.expect, time 8 coin tosses came heads, value \\(p\\) 0.4. , numerous occasions, . quite common value \\(p\\) like 0.3 0.5 generate 8 heads. Consider:Yet distribution raw counts. unnormalized density. turn proper probability density (.e., one sum probabilities across possible outcomes sums one) just divide everything total number observations.likely value \\(p_h\\) still 0.4, . , much likely \\(p\\) either 0.3 0.5. 8% chance \\(p_h \\ge 0.6\\).might wondering: use model? Well, let’s say toss coin 20 times get 8 heads . Given result, probability future samples 20 flips result 10 heads?three main ways go solving problem simulations.first wrong way assuming \\(p_h\\) certain observed 8 heads 20 tosses. conclude 8/20 gives us 0.4. big problem ignoring uncertainty estimating \\(p_h\\). lead us following code.second method involves sampling whole posterior distribution vector previously created. lead following correct code.Third way sample actual distribution, small dataset just rows, includes \\(p_h\\) probability \\(p_h\\). also gives correct answer.may noticed, calculated value using first method, believe getting 10 heads less likely really . run casino based assumptions, lose money. important careful assumptions making. tossed coin 20 times got 8 heads. However, wrong assume \\(p_h\\) = 0.4 just based result.","code":"\nThis animation shows what we want to do with joint distributions. We take a slice (the red one), isolate it, rotate it to look at the conditional distribution, normalize it (change the values along the current z-axis from counts to probabilities), then observe the resulting posterior.\n\nsims <- 10000000\n\nodds <- tibble(sim_ID = 1:sims) %>%\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) %>% \n  mutate(result = ifelse(heads >= 10, TRUE, FALSE)) %>% \n  summarize(success = sum(result)/sims)\n\nodds## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.245\np_draws <- tibble(p = rep(seq(0, 1, 0.1), 1000)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) %>%\n  filter(heads == 8)\n  \nodds <- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) %>% \n  mutate(result = ifelse(heads >= 10, TRUE, FALSE)) %>% \n  summarize(success = sum(result)/sims)\n\nodds## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.328\np_posterior <- x %>% \n  filter(heads == 8) %>% \n  group_by(p) %>% \n  summarize(total = n(), .groups = \"drop\") %>%\n  mutate(probs = total/sum(total))\n\nodds <- tibble(p = sample(p_posterior$p, \n                  size = sims, prob = p_posterior$probs, replace = TRUE)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) %>% \n  mutate(result = ifelse(heads >= 10, TRUE, FALSE)) %>% \n  summarize(success = sum(result)/sims)\n\nodds## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.320"},{"path":"probability.html","id":"wisdom","chapter":"5 Probability","heading":"5.6 Wisdom","text":"PPBDS contains four important themes relevant chapters: Wisdom, Temperance, Courage, Justice. move textbook, building refining concepts.\nFIGURE 5.3: Wisdom.\nfirst theme Wisdom whether numbers capture concepts care . concerned whether data models representative problem.Parameter uncertainty reason exercise Wisdom. Suppose friend sets pseudo-casino playground neighborhood school. ’s offering everyone opportunity play probability games bet money. first game simple: coin tosses. coin lands heads, get five dollars. lands tails, pay three dollars.sneaking suspicion coin isn’t fair ask friend flip 10 times test theory. However, must overconfident. 10 observations might judge fairness coin. Perhaps friend secretly switches coins comes time actual betting sample initial simulation representative. might nervous losing money even miscount number heads tails. examples parameter uncertainty. Sometimes, model correct form, ’s practical run simulation millions times certain outcome. ’s possible absolutely certain friend isn’t switching coins. perfect world exist, must reservations drawing likely conclusions.need aware sometimes possible capture variables care . However, long prudent, can still interpret data.","code":""},{"path":"probability.html","id":"justice","chapter":"5 Probability","heading":"5.7 Justice","text":"\nFIGURE 5.4: Justice.\nJustice data science making sure models “fair” unbiased possible. Model structure one important ideas theme Justice.section N models, talked number heads get flipping coin 20 times. One important parts model structure compressing ideas like coin toss experiment mathematical expressions.instance, Bernouli distribution \\(n = 1\\). following expression, assuming default value n = 1, flip coin one time. H represents observation (.e. result heads tails). tilda tells us making observations experiment. B() represents binomial distribution result experiment either 0 1, tails heads. \\(p\\) expression represents probability heads.\\[ H \\sim B(p) \\]\nfollowing slightly comprehensive expression, takes different values n account. might flip coin 1 time, 2 times, etc.\\[ H \\sim B(p, n)\\]experiment simulated section N models. flipped coin 20 times observed number heads got. worked different values \\(p\\) create scatterplot 3D visualization earlier chapter.\\[ H \\sim B(p, n = 20)\\]\nAnother question consider whether models generate calculating probabilities predictive causal. Knowing difference helps us fairly interpret data. Every model predictive sense , give new data drawn stable distribution, can give predictive forecast. subset models causal, meaning , given individual, can change value one input figure new output , , calculate causal effect.purposes chapter, everything worked predictive. attempt make causal claims. wanted make causal claim, need feature least two different ways experiment. instance, look way flip coin affects outcome. compare number heads get catching coin air versus letting fall ground. important part Justice understanding distinction predictive causal models.","code":""},{"path":"probability.html","id":"courage","chapter":"5 Probability","heading":"5.8 Courage","text":"\nFIGURE 5.5: Courage.\nData science involves words, math, code. Code important element. Turning idea code requires Courage.Courage helps us account unmodeled variation. Recall following equation.\\[outcome = model + \\ \\ \\ \\ \\ model\\]residual variation contained model. explains result equal fitted (also known expected) value plus residual (model). However, matter nuanced model becomes, able make perfect predictions. randomness intrinsic. instance, behavior one individual can change can’t perfectly predicted even model accounts group average.\nFIGURE 5.6: Bruno de Finetti, Italian statistician wrote famous treatise theory probability began statement “PROBABILITY EXIST.” probability exists subjectively minds. tool people made use broad-stroke estimates.\nPrediction uncertainty highlights ability, lack thereof, predict future. can conceptualized either attempt forecast literal future attempt model data points left original analysis way. Prediction uncertainty two primary sources previously discussed: parameter uncertainty unmodeled variation.","code":""},{"path":"probability.html","id":"temperance","chapter":"5 Probability","heading":"5.9 Temperance","text":"\nFIGURE 5.7: Temperance.\nModels often good appear. ’s important patient understand pitfalls data created.\nFIGURE 5.8: Donald Rumsfeld.\nDonald Rumsfeld, former US Secretary Defense, said following tongue-twisting quote:“known knowns. things know know. also know known unknowns. say, know things know. also unknown unknowns, ones know know.”\nFIGURE 5.9: Three Card Monte.\nmean? Well imagine crowd playing Three Card Monte streets New York. guy running game runs demo shows cards make confident. earn money making overconfident persuading bet. odds may seem good demo round, doesn’t actually say anything likely happen real, high stakes game begins. person running game many simulations, making “victim” forget actually make conclusions odds winning. variables simply know even put lot effort making posterior probability distributions. People can using slight hand, instance.need patience order study understand unknown unknowns data. Patience also important analyze “realism” models. created mathematical probability distribution presidential elections, instance, assumed Democratic candidate 50% chance winning vote electoral college. comparing mathematical model empirical cases, however, recognize mathematical model unlikely true. mathematical model suggested getting fewer 100 votes next impossible, many past Democratic candidates empirical distribution received less 100 electoral votes.","code":""},{"path":"probability.html","id":"conclusion-1","chapter":"5 Probability","heading":"5.10 Conclusion","text":"Throughout chapter, spent time going examples conditional distributions. However, ’s worth noting probability distributions conditional something. Even simple examples, flipping coin multiple times, assuming probability getting heads versus tails change tosses.also discussed difference empirical, mathematical, posterior probability distributions. Even though developed heuristics better understand distributions, every time make claim world, based beliefs - think world. wrong. beliefs can differ. Two reasonable people can conflicting beliefs fairness dice.useful understand three types distributions concept conditional distributions, almost every probability distribution conditional posterior. can leave words future discussions, generally book. implicit.keen learn probability, video featuring Professor Gary King. great way review concepts covered chapter.","code":""},{"path":"one-parameter.html","id":"one-parameter","chapter":"6 One Parameter","heading":"6 One Parameter","text":"scene Hunger Games, dystopian novel children selected via lottery fight death. basic elements event, called “Reaping,” :children ages 12 entered lottery.children ages 12 entered lottery.Children aged 12 entered , children aged 13 entered twice, children aged 14 entered three times, etc.Children aged 12 entered , children aged 13 entered twice, children aged 14 entered three times, etc.Families may enter names children required exchange food.Families may enter names children required exchange food.One boy one girl selected year tributes represent district battle death.One boy one girl selected year tributes represent district battle death.Children selected urn may volunteer tribute.Children selected urn may volunteer tribute.scene , Primrose Everdeen selected urn. twelve year old, name entered . Though know total number names, know poor families (make majority Everdeen’s district) able enter times exchange food. Therefore, can reasonably assume children, including Primrose’s sister, entered many times required. , , Primrose misfortune selected? , refer henceforth, sampled?explore sampling, come understand , unlikely, anomalies like Primrose Everdeen certainly possible. Let’s get started!\nFIGURE 6.1: Mr. DeVito correct: sampling badass.\nLast chapter, learned probability, act quantifying uncertainty. chapter, learn sampling, beginning journey toward inference. sample, take portion total population attempt draw conclusion portion, conclusion can generalize entire population.see headline approval rating politician, given number represents approval rating every citizen total population. looking inference based much smaller sample people. source reputable, sample mirrors real world closely possible. said, variation reported approval rating depending specific sample taken day. one reason see different approval ratings different news sources.Despite variation, normally see estimates ballpark. surprising see two approval ratings 10% apart given day. possible, however! number reasons large gap, many result errors sampling . also just consequence sampling variation.\nconduct polling, let’s look instead easier scenario. Observe urn Figure 6.2. certain number red certain number white beads equal size. appears urn mixed beforehand, seem coherent pattern spatial distribution red white beads.Let’s now ask , proportion urn’s beads red?\nFIGURE 6.2: urn red white beads.\nOne way answer question perform exhaustive count: remove bead individually, count number red beads number white beads, divide number red beads total number beads. However, long tedious process. Therefore, use sampling! Consider two questions:get 16 red beads random sample size 50 taken urn 2,400 (white red) beads, many red beads urn?probability, using earn, draw 8 red beads use shovel size 20?always, need tidyverse package.","code":"\nlibrary(tidyverse)"},{"path":"one-parameter.html","id":"sampling-activity","chapter":"6 One Parameter","heading":"6.1 Real sampling activity","text":"\nFIGURE 6.3: urn red white beads.\n","code":""},{"path":"one-parameter.html","id":"using-the-shovel-method-once","chapter":"6 One Parameter","heading":"6.1.1 Using the shovel method once","text":"Instead performing exhaustive count, let’s insert shovel urn seen Figure 6.2. Using shovel, let’s remove \\(5 \\cdot 10 = 50\\) beads, seen Figure 6.3. , taking sample total population beads.\nFIGURE 6.4: Inserting shovel urn.\n\nFIGURE 6.5: Removing 50 beads urn.\nObserve 17 50 sampled beads red thus 17/50 = 0.34 = 34% shovel’s beads red. can view proportion beads red shovel guess proportion beads red entire urn. exact exhaustive count beads urn, guess 34% took much less time energy make.Imagine started activity beginning, replacing 50 beads back urn starting . remove exactly 17 red beads? Maybe?repeated activity many times? guess proportion urn’s beads red exactly 34% every time? Surely .Let’s repeat exercise help 33 groups friends understand value varies across 33 independent trials.","code":""},{"path":"one-parameter.html","id":"student-shovels","chapter":"6 One Parameter","heading":"6.1.2 Using the shovel 33 times","text":"33 groups friends following:Use shovel remove 50 beads .Count number red beads compute proportion 50 beads red.Return beads urn.Mix contents urn let previous group’s results influence next group’s.33 groups friends make note proportion red beads sample collected. group marks proportion 50 beads red appropriate bin hand-drawn histogram seen .\nFIGURE 6.6: Constructing histogram proportions.\nHistograms allow us visualize distribution numerical variable. particular, center values falls values vary. partially completed histogram first 10 33 groups friends’ results can seen figure .\nFIGURE 6.7: Hand-drawn histogram first 10 33 proportions.\nObserve following details histogram:low end, one group removed 50 beads urn proportion red 0.20 0.25.high end, another group removed 50 beads urn proportion 0.45 0.5 red.However, frequently occurring proportions 0.30 0.35 red, right middle distribution.distribution somewhat bell-shaped.tactile_sample_urn saves results 33 groups friends.Observe group names, number red_beads obtained, corresponding proportion 50 beads red, called prop_red. also ID variable whoch gives 33 groups unique identifier. row can viewed one instance replicated activity: using shovel remove 50 beads computing proportion beads red.Let’s visualize distribution 33 proportions using geom_histogram() binwidth = 0.05. computerized complete version partially completed hand-drawn histogram saw earlier. Setting boundary = 0.4 indicates want binning scheme one bins’ boundary 0.4. color = \"white\" modifies color boundary visual clarity.","code":"## # A tibble: 33 x 4\n##    group         red_beads prop_red    ID\n##    <chr>             <dbl>    <dbl> <int>\n##  1 Mal, Francis         17     0.34     1\n##  2 Nam, Joshua          19     0.38     2\n##  3 Mark, Ramses         21     0.42     3\n##  4 Maeve, Josh          18     0.36     4\n##  5 Morgan, Emily        21     0.42     5\n##  6 Ace, Chris           18     0.36     6\n##  7 Mia, James           15     0.3      7\n##  8 Griffin, Mary        18     0.36     8\n##  9 Yuki, Harry          21     0.42     9\n## 10 Frank, Clara         21     0.42    10\n## # … with 23 more rows\ntactile_sample_urn %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\",\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") "},{"path":"one-parameter.html","id":"what-did-we-just-do","chapter":"6 One Parameter","heading":"6.1.3 What did we just do?","text":"just demonstrated activity statistical concept sampling. want know proportion urn’s beads red. Performing exhaustive count red white beads time-consuming. Therefore, extracted sample 50 beads using shovel. Using sample 50 beads, estimated proportion urn’s beads red 34%.Moreover, mixed beads use shovel, samples randomly drawn. sample drawn random, samples different . samples different , obtained different proportions red observed previous histogram. known concept sampling variation.purpose sampling activity develop understanding two key concepts relating sampling:Understanding effect sampling variation.Understanding effect sample size sampling variation.Section 6.2, ’ll mimic hands-sampling activity just performed computer. allow us repeat sampling exercise much 33 times, also allow us use shovels different numbers slots just 50.Afterwards, ’ll present definitions, terminology, notation related sampling Section 6.3. many disciplines, necessary background knowledge may seem confusing first. However, many difficult topics, truly understand underlying concepts practice, ’ll able master .tie contents chapter real world, ’ll present example one common uses sampling: polls. Section 6.4 ’ll look particular case study: 2013 poll U.S. President Barack Obama’s popularity among young Americans, conducted Kennedy School’s Institute Politics Harvard University. close chapter, ’ll generalize “sampling urn” exercise sampling scenarios.","code":""},{"path":"one-parameter.html","id":"sampling-simulation","chapter":"6 One Parameter","heading":"6.2 Virtual sampling","text":"previous Section 6.1, performed tactile sampling activity. words, used physical urn beads physical shovel. performed sampling activity hand develop firm understanding root ideas behind sampling. section, mimic tactile sampling activity virtual sampling activity using computer. virtual urn virtual shovel.","code":""},{"path":"one-parameter.html","id":"using-the-virtual-shovel-once","chapter":"6 One Parameter","heading":"6.2.1 Using the virtual shovel once","text":"Let’s start performing virtual analog tactile sampling exercise performed Section 6.1. first need virtual analog urn seen beginning chapter. end, create tibble named urn. rows urn correspond exactly contents actual urn.reference, sample_frac() merely re-arranges rows tibble. use set.seed() ensure beads virtual urn always order. ensures figures book match written descriptions.Observe urn 2,400 rows, meaning urn contains\n2,400 equally-sized beads. first variable ID used identification variable; none beads actual urn marked numbers. second variable color indicates whether particular virtual bead red white. View contents urn RStudio’s data viewer scroll contents convince urn indeed virtual analog actual urn.Note , Chapter, used variable ID two different ways: first, keep track samples drawn 33 individual teams , second, keep track \n2,400 beads virtual urn. OK! ID means concept cases. neeed careful, however, working one table ID column. case, often rename one columns order avoid conflicts.Now virtual analog urn, need virtual analog shovel seen Figure 6.2 generate virtual samples 50 beads. ’re going use sample_n() list-columns.Let’s take sample 50 beads virtual urn.usual, map functions list-columns powerful confusing.Let’s compute proportion beads virtual sample red. First, add column indicates number red beads.work? R treats TRUE like number 1 FALSE like number 0. summing number TRUEs FALSEs equivalent summing 1’s 0’s. end, operation counts number beads color red.Second, add column total number beads. (already “know” 50, never hurts make code general.)Third, calculate proportion red:Careful readers note numb_red changing example . reason, course, block re-runs shovel exercise, getting (potentially) different number red beads time. wanted number block, need use set.seed() time, always providing seed time.Let’s now perform virtual analog 33 groups students use sampling shovel!","code":"\nset.seed(10)\nurn <- tibble(color = c(rep(\"red\", 900), rep(\"white\", 1500))) %>%\n  sample_frac() %>% \n  mutate(ID = 1:2400) %>% \n  select(ID, color)\n\nurn  ## # A tibble: 2,400 x 2\n##       ID color\n##    <int> <chr>\n##  1     1 red  \n##  2     2 red  \n##  3     3 white\n##  4     4 white\n##  5     5 white\n##  6     6 white\n##  7     7 red  \n##  8     8 white\n##  9     9 white\n## 10    10 white\n## # … with 2,390 more rows\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50)))## # A tibble: 1 x 2\n##      ID shovel           \n##   <dbl> <list>           \n## 1     1 <tibble [50 × 2]>\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))## # A tibble: 1 x 3\n##      ID shovel            numb_red\n##   <dbl> <list>               <int>\n## 1     1 <tibble [50 × 2]>       14\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color)))## # A tibble: 1 x 4\n##      ID shovel            numb_red numb_beads\n##   <dbl> <list>               <int>      <int>\n## 1     1 <tibble [50 × 2]>       18         50\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)## # A tibble: 1 x 5\n##      ID shovel            numb_red numb_beads prop_red\n##   <dbl> <list>               <int>      <int>    <dbl>\n## 1     1 <tibble [50 × 2]>       21         50     0.42"},{"path":"one-parameter.html","id":"using-the-virtual-shovel-33-times","chapter":"6 One Parameter","heading":"6.2.2 Using the virtual shovel 33 times","text":"tactile sampling exercise Section 6.1, 33 groups students use shovel, yielding 33 samples size 50 beads. used 33 samples compute 33 proportions. can perform repeated/replicated sampling virtually just thing 33 times.’ll save results data frame called virtual_samples.Let’s visualize variation histogram Figure 6.8.Note add ‘binwidth = 0.05’ ‘boundary = 0.4’ arguments well. Recall setting ‘boundary = 0.4’ ensures binning scheme one bins’ boundaries 0.4. Since ‘binwidth = 0.05’ also set, create bins boundaries 0.30, 0.35, 0.45, 0.5, etc. well.Observe occasionally obtained proportions red less 30%. hand, occasionally obtained proportions greater 45%. However, frequently occurring proportions 35% 40%. differences proportions red? sampling variation.Compare virtual results tactile results previous section. Observe histograms somewhat similar center variation, although identical. slight differences due random sampling variation. Furthermore, observe distributions somewhat bell-shaped.\nFIGURE 6.8: Comparing 33 virtual 33 tactile proportions red.\n","code":"\nset.seed(9)\nvirtual_samples <- tibble(ID = 1:33) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\nvirtual_samples## # A tibble: 33 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [50 × 2]>       25         50     0.5 \n##  2     2 <tibble [50 × 2]>       23         50     0.46\n##  3     3 <tibble [50 × 2]>       15         50     0.3 \n##  4     4 <tibble [50 × 2]>       18         50     0.36\n##  5     5 <tibble [50 × 2]>       21         50     0.42\n##  6     6 <tibble [50 × 2]>       22         50     0.44\n##  7     7 <tibble [50 × 2]>       21         50     0.42\n##  8     8 <tibble [50 × 2]>       22         50     0.44\n##  9     9 <tibble [50 × 2]>       18         50     0.36\n## 10    10 <tibble [50 × 2]>       18         50     0.36\n## # … with 23 more rows\nvirtual_samples %>% \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\",\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") "},{"path":"one-parameter.html","id":"shovel-1000-times","chapter":"6 One Parameter","heading":"6.2.3 Using the virtual shovel 1,000 times","text":"\nFIGURE 6.9: much sampling, little time.\nNow say want study effects sampling variation 33 samples, larger number samples (1000). two choices point. groups friends manually take 1,000 samples 50 beads compute corresponding 1,000 proportions. However, time-consuming. computers excel: automating long repetitive tasks performing quickly. point, abandon tactile sampling favor virtual sampling.Observe now 1,000 replicates prop_red, proportion 50 beads red. Using code earlier, let’s now visualize distribution 1,000 replicates prop_red histogram Figure 6.10., frequently occurring proportions red beads occur 35% 40%. Every now , obtain proportions low 20% 25%, others high 55% 60%. rare, however. Furthermore, observe now much symmetric smoother bell-shaped distribution. distribution , fact, approximated well normal distribution.","code":"\nset.seed(9)\nvirtual_samples <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\nvirtual_samples## # A tibble: 1,000 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [50 × 2]>       25         50     0.5 \n##  2     2 <tibble [50 × 2]>       23         50     0.46\n##  3     3 <tibble [50 × 2]>       15         50     0.3 \n##  4     4 <tibble [50 × 2]>       18         50     0.36\n##  5     5 <tibble [50 × 2]>       21         50     0.42\n##  6     6 <tibble [50 × 2]>       22         50     0.44\n##  7     7 <tibble [50 × 2]>       21         50     0.42\n##  8     8 <tibble [50 × 2]>       22         50     0.44\n##  9     9 <tibble [50 × 2]>       18         50     0.36\n## 10    10 <tibble [50 × 2]>       18         50     0.36\n## # … with 990 more rows\nvirtual_samples %>% \n  ggplot(aes(x = prop_red)) +\n    geom_histogram(binwidth = 0.01, \n                   boundary = 0.4, \n                   color = \"white\") +\n    labs(x = \"Proportion of 50 beads that were red\", \n         y = \"Count\",\n         title = \"Distribution of 1,000 proportions red\") "},{"path":"one-parameter.html","id":"different-shovels","chapter":"6 One Parameter","heading":"6.2.4 The effect of different shovel sizes","text":"Instead just one shovel, imagine three choices shovels extract sample beads : shovels size 25, 50, 100.goal estimate proportion urn’s beads red, shovel choose?Using newly developed tools virtual sampling, let’s unpack effect different sample sizes!Virtually use appropriate shovel generate 1,000 samples size beads.Compute resulting 1,000 replicates proportion shovel’s beads red.Visualize distribution 1,000 proportions red using histogram.Start virtually using shovel 1000 times. , compute resulting 1000 replicates proportion red. Finally, plot distribution using histogram.repeat process shovel size 50.Finally, perform process 1000 replicates map histogram using shovel size 100.easy comparison, present three resulting histograms single row matching x y axes Figure 6.12.\nFIGURE 6.10: Comparing distributions proportion red different sample sizes.\nObserve sample size increases, variation 1,000 replicates proportion red decreases. words, sample size increases, fewer differences due sampling variation distribution centers tightly around value. Eyeballing Figure 6.12, three histograms appear center around roughly 40%.can numerically explicit amount variation three sets 1,000 values prop_red using standard deviation. standard deviation summary statistic measures amount variation within numerical variable. three sample sizes, let’s compute standard deviation 1,000 proportions red running following data wrangling code uses sd() summary function. observed earlier, sample size increases, variation decreases. words, less variation 1,000 values proportion red. sample size increases, guesses true proportion urn’s beads red get precise. Therefore, larger shovel, precise result!","code":"\nvirtual_samples_25 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 25))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\n\nvirtual_samples_25 %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 beads that were red\", title = \"25\") \nvirtual_samples_50 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\n\nvirtual_samples_50  %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\", title = \"50\")  \nvirtual_samples_100 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 100))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\n\nvirtual_samples_100 %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 beads that were red\", title = \"100\") \nvirtual_samples_25 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0939\nvirtual_samples_50 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0682\nvirtual_samples_100 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0461"},{"path":"one-parameter.html","id":"functions-are-your-friend","chapter":"6 One Parameter","heading":"6.2.5 Functions are your friend!","text":"Note last section, ran less code three times, different sizes shovel (25, 50, 100). Whenever find writing code three times, write function thing. Let’s look code used shovel size 25 calculated proportion beads red one time:pipe want, creating function relatively easy. Just place code within function definition “pull ” specific variables arguments.See just uses code create virtual_prop_red_25, generalizes . Now can create tibbles , ready plot histograms, three lines code:still isn’t best way. Note three objects need deal , virtual_prop_red_25, virtual_prop_red_50, virtual_prop_red_100. Instead, let’s store results single tibble. can ? using map() create list column!First, ’ll create tibble variable named shovel_size values (25, 50, 100):Next, ’ll create list column called prop_red_results output prop_red().Adding another map_* function let us get standard deviations estimated proportions:Now framework, ’s need limit sizes 25, 50, 100. try integers 1 100? can use code, except ’ll now set shovel_size = 1:100 initializing tibble.Now, standard deviation prop_red shovel sizes 1 100. Let’s plot value see changes shovel gets larger:\nFIGURE 6.11: Comparing standard deviations proportions red 100 different shovels. standard deviation decreases square root shovel size.\n\nFIGURE 6.12: poets philosophers confused : don’t worry! won’t problem set.\npower running many analyses using map_* functions list columns: , tell standard deviation decreasing shovel size increased, looking shovel sizes 25, 50, 100, wasn’t clear quickly decreasing.","code":"\ntibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ sample_n(urn, size = 25))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)## # A tibble: 1,000 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [25 × 2]>       11         25     0.44\n##  2     2 <tibble [25 × 2]>       11         25     0.44\n##  3     3 <tibble [25 × 2]>       10         25     0.4 \n##  4     4 <tibble [25 × 2]>       10         25     0.4 \n##  5     5 <tibble [25 × 2]>        9         25     0.36\n##  6     6 <tibble [25 × 2]>       10         25     0.4 \n##  7     7 <tibble [25 × 2]>        8         25     0.32\n##  8     8 <tibble [25 × 2]>       11         25     0.44\n##  9     9 <tibble [25 × 2]>        7         25     0.28\n## 10    10 <tibble [25 × 2]>        7         25     0.28\n## # … with 990 more rows\nprop_red <- function(x, shovel_size, reps){\n  tibble(ID = 1:reps) %>% \n    mutate(shovel = map(ID, ~ sample_n(x, size = shovel_size))) %>% \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n    mutate(prop_red = numb_red / numb_beads)\n}\nvirtual_prop_red_25  <- prop_red(x = urn, shovel_size = 25,  reps = 1000)\nvirtual_prop_red_50  <- prop_red(x = urn, shovel_size = 50,  reps = 1000)\nvirtual_prop_red_100 <- prop_red(x = urn, shovel_size = 100, reps = 1000)\ntibble(shovel_size = c(25, 50, 100))## # A tibble: 3 x 1\n##   shovel_size\n##         <dbl>\n## 1          25\n## 2          50\n## 3         100\ntibble(shovel_size = c(25, 50, 100)) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000)))## # A tibble: 3 x 2\n##   shovel_size prop_red_results    \n##         <dbl> <list>              \n## 1          25 <tibble [1,000 × 5]>\n## 2          50 <tibble [1,000 × 5]>\n## 3         100 <tibble [1,000 × 5]>\nshovels <- tibble(shovel_size = c(25, 50, 100)) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000))) %>% \n  mutate(prop_red_sd = map_dbl(prop_red_results, ~ pull(., prop_red) %>% sd()))\n\nglimpse(shovels)## Rows: 3\n## Columns: 3\n## $ shovel_size      <dbl> 25, 50, 100\n## $ prop_red_results <list> [<tbl_df[1000 x 5]>, <tbl_df[1000 x 5]>, <tbl_df[10…\n## $ prop_red_sd      <dbl> 0.095, 0.068, 0.048\nshovels_100 <- tibble(shovel_size = 1:100) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000))) %>% \n  mutate(prop_red_sd = map_dbl(prop_red_results, ~ pull(., prop_red) %>% sd()))\n\nglimpse(shovels_100)## Rows: 100\n## Columns: 3\n## $ shovel_size      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n## $ prop_red_results <list> [<tbl_df[1000 x 5]>, <tbl_df[1000 x 5]>, <tbl_df[10…\n## $ prop_red_sd      <dbl> 0.476, 0.348, 0.280, 0.244, 0.220, 0.204, 0.182, 0.1…"},{"path":"one-parameter.html","id":"sampling-framework","chapter":"6 One Parameter","heading":"6.3 Sampling framework","text":"tactile virtual sampling activities, used sampling purpose estimation. extracted samples order estimate proportion urn’s beads red. virtual sampling activity built results comparing 1,000 proportions red based samples size 25, 50, 100, finally expanding sizes 1 100. first attempt understanding two key concepts relating sampling estimation:effect sampling variation estimates.effect sample size sampling variation.Let’s now introduce terminology notation well statistical definitions related sampling. likely want read section times. Keep mind, however, concepts underlying terminology, notation, definitions tie directly concepts underlying tactile virtual sampling activities. simply take time practice master .","code":""},{"path":"one-parameter.html","id":"terminology-and-notation","chapter":"6 One Parameter","heading":"6.3.1 Terminology and notation","text":"\nFIGURE 6.13: Let Yoda’s wisdom dull pain terminology section.\nlist terminology mathematical notation relating sampling.First, population collection individuals observations interested . also commonly denoted study population. mathematically denote population’s size using upper-case \\(N\\). sampling activities, (study) population collection \\(N\\) = 2400 identically sized red white beads contained urn.Second, population parameter numerical summary quantity population unknown, wish knew. example, quantity mean, population parameter interest population mean. mathematically denoted Greek letter \\(\\mu\\) pronounced “mu” (’ll see sampling activity involving means upcoming Section 7.1). earlier sampling urn activity, however, since interested proportion urn’s beads red, population parameter population proportion. mathematically denoted letter \\(p\\).Third, census exhaustive enumeration counting \\(N\\) individuals observations population order compute population parameter’s value exactly. sampling activity, correspond counting number beads \\(N\\) = 2400 red computing population proportion \\(p\\) red exactly. number \\(N\\) individuals observations population large case urn, census can quite expensive terms time, energy, money.Fourth, sampling act collecting sample population don’t means perform census. mathematically denote sample’s size using lower case \\(n\\), opposed upper case \\(N\\) denotes population’s size. Typically sample size \\(n\\) much smaller population size \\(N\\). Thus sampling cheaper alternative performing census. sampling activities, used shovels varying slots extract samples size \\(n\\) = 1 \\(n\\) = 100.Fifth, point estimate (AKA sample statistic) summary statistic computed sample estimates unknown population parameter. sampling activities, recall unknown population parameter population proportion mathematically denoted \\(p\\). point estimate sample proportion: proportion shovel’s beads red. words, guess proportion urn’s beads red. mathematically denote sample proportion using \\(\\hat{p}\\). “hat” top \\(p\\) indicates estimate unknown population proportion \\(p\\).Sixth idea representative sampling. sample said representative sample roughly looks like population. words, sample’s characteristics good representation population’s characteristics? sampling activity, samples \\(n\\) beads extracted using shovels representative urn’s \\(N\\) = 2400 beads?Seventh idea generalizability. say sample generalizable results based sample can generalize population. sampling activity, can generalize sample proportion shovels entire urn? Using mathematical notation, akin asking \\(\\hat{p}\\) “good guess” \\(p\\)?Eighth, say biased sampling occurs certain individuals observations population higher chance included sample others. say sampling procedure unbiased every observation population equal chance sampled. red beads much smaller white beads, therefore prone falling shovel, sample biased. sampling activities, since mixed \\(N = 2400\\) beads prior group’s sampling since equally sized beads equal chance sampled, samples unbiased.Ninth lastly, idea random sampling. say sampling procedure random sample randomly population unbiased fashion. sampling activities, correspond sufficiently mixing urn use shovel.\nFIGURE 6.14: Fear look like Spongebob reading section. re-cap right now!\nPhew, ’s lot new terminology notation learn! Let’s put together describe paradigm sampling.general:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can draw inferences population using sampling.Specific sampling activity:extract sample \\(n=50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 2400 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{p}\\) \\(n=50\\) beads shovel red “good guess” population proportion \\(p\\) \\(N=2400\\) beads red, thusinstead manually going 2400 beads urn, can infer urn using shovel.Note last word wrote bold: infer. act “inferring” means deduce conclude information evidence reasoning. sampling activities, wanted infer proportion urn’s beads red. Statistical inference “theory, methods, practice forming judgments parameters population reliability statistical relationships, typically basis random sampling.” words, statistical inference act inference via sampling.","code":""},{"path":"one-parameter.html","id":"sampling-definitions","chapter":"6 One Parameter","heading":"6.3.2 Statistical definitions","text":"\nFIGURE 6.15: almost done! Hang .\nNow, important statistical definitions related sampling. refresher 1,000 repeated/replicated virtual samples size \\(n\\) = 25, \\(n\\) = 50, \\(n\\) = 100 Section 6.2, let’s display figure showing difference proportions red according different shovel sizes.\nFIGURE 6.16: Previously seen three distributions sample proportion \\(\\hat{p}\\).\ntypes distributions special name: sampling distributions; visualization displays effect sampling variation distribution point estimate; case, sample proportion \\(\\hat{p}\\). Using sampling distributions, given sample size \\(n\\), can make statements values typically expect.example, observe centers three sampling distributions: roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe somewhat likely observe sample proportions red beads \\(0.2 = 20\\%\\) using shovel 25 slots, almost never observe proportion 20% using shovel 100 slots. Observe also effect sample size sampling variation. sample size \\(n\\) increases 25 50 100, variation sampling distribution decreases thus values cluster tightly around center around 40%. quantified variation using standard deviation sample proportions, seeing standard deviation decreases square root sample size:\nFIGURE 6.17: Previously seen comparing standard deviations proportions red 100 different shovels\nsample size increases, standard deviation proportion red beads decreases. type standard deviation another special name: standard error. Standard errors quantify effect sampling variation induced estimates. words, quantify much can expect different proportions shovel’s beads red vary one sample another sample another sample, . general rule, sample size increases, standard error decreases.Unfortunately, names confuse many people new statistical inference. example, ’s common people new statistical inference call “sampling distribution” “sample distribution.” Another additional source confusion name “standard deviation” “standard error.” Remember standard error merely kind standard deviation: standard deviation point estimate sampling. words, standard errors standard deviations, every standard deviation necessarily standard error.help reinforce concepts, let’s re-display Figure 6.14 using new terminology, notation, definitions relating sampling figure .\nFIGURE 6.18: Three sampling distributions sample proportion \\(\\hat{p}\\).\nFurthermore, let’s display graph standard errors \\(n = 1\\) \\(n = 100\\) using new terminology, notation, definitions relating sampling.\nFIGURE 6.19: Standard errors sample proportion based sample sizes 1 100\nRemember key message last table: sample size \\(n\\) goes , “typical” error point estimate go , quantified standard error.","code":""},{"path":"one-parameter.html","id":"moral-of-the-story","chapter":"6 One Parameter","heading":"6.3.3 The moral of the story","text":"Let’s recap section far. ’ve seen sample generated random, resulting point estimate “good guess” true unknown population parameter. sampling activities, since made sure mix beads first extracting sample shovel, resulting sample proportion \\(\\hat{p}\\) shovel’s beads red “good guess” population proportion \\(p\\) urn’s beads red.However, mean point estimate “good guess?” Sometimes, ’ll get estimate less true value population parameter, times ’ll get estimate greater. due sampling variation. Despite sampling variation, estimates “average” correct thus centered true value. sampling done random thus unbiased fashion.sampling activities, sometimes sample proportion \\(\\hat{p}\\) less true population proportion \\(p\\), times greater. due sampling variability. However, despite sampling variation, sample proportions \\(\\hat{p}\\) “average” correct thus centered true value population proportion \\(p\\). mixed urn taking samples thus sampling done random thus unbiased fashion. also known accurate estimate.value population proportion \\(p\\) \\(N\\) = 2400 beads actual urn red? 900 red beads, proportion red 900/2400 = 0.375 = 37.5%! know ? authors exhaustive count beads? ! listed contents box urn came ! Hence able make contents virtual urn match tactile urn:Let’s re-display sampling distributions earlier, now vertical red line marking true population proportion \\(p\\) beads red = 37.5% Figure 6.19. see certain amount error sample proportions \\(\\hat{p}\\) three sampling distributions, average \\(\\hat{p}\\) centered true population proportion red \\(p\\).\nFIGURE 6.20: Three sampling distributions population proportion \\(p\\) marked vertical line.\npoint, might asking : “already knew true proportion urn’s beads red 37.5%, sampling?” might also asking: “take 1,000 repeated samples various sizes (n = 1 n = 100)? Shouldn’t taking one sample ’s large possible?” ask questions, suspicion merited!sampling activity involving urn merely idealized version sampling done real life. performed exercise study understand:effect sampling variation.effect sample size sampling variation.sampling done real life. real life scenario, won’t know true value population parameter . Furthermore, wouldn’t able take 1,000 replicated samples. Rather, take single sample ’s large can afford. next section, ’ll study real life example sampling: polls.\nFIGURE 6.21: Real life sampling hard! scenarios, ’s impossible ask entire population opinions.\n","code":"\nurn %>% \n  summarize(sum_red = sum(color == \"red\"), \n            sum_not_red = sum(color != \"red\"), .groups = 'drop_last')## # A tibble: 1 x 2\n##   sum_red sum_not_red\n##     <int>       <int>\n## 1     900        1500"},{"path":"one-parameter.html","id":"sampling-case-study","chapter":"6 One Parameter","heading":"6.4 Case study: Polls","text":"Let’s now switch gears realistic sampling scenario: poll. practice, pollsters take 1,000 repeated samples previous sampling activities, rather take single sample ’s large possible.December 4, 2013, National Public Radio US reported poll President Obama’s approval rating among young Americans aged 18-29 article, “Poll: Support Obama Among Young Americans Eroding.” poll conducted Kennedy School’s Institute Politics Harvard University. quote article:voting large numbers 2008 2012, young Americans souring President Obama.According new Harvard University Institute Politics poll, just 41 percent millennials — adults ages 18-29 — approve Obama’s job performance, lowest-ever standing among group 11-point drop April.Let’s tie elements real life poll new article “tactile” “virtual” urn activity Sections 6.1 6.2 using terminology, notations, definitions learned Section 6.3. ’ll see sampling activity urn idealized version pollsters trying real life.First, (Study) Population \\(N\\) individuals observations interest?Urn: \\(N\\) = 2400 identically sized red white beadsObama poll: \\(N\\) = ? young Americans aged 18-29Second, population parameter?Urn: population proportion \\(p\\) beads urn red.Obama poll: population proportion \\(p\\) young Americans approve Obama’s job performance.Third, census look like?Urn: Manually going \\(N\\) = 2400 beads exactly computing population proportion \\(p\\) beads red.Obama poll: Locating \\(N\\) young Americans asking approve Obama’s job performance. case, don’t even know population size \\(N\\) !Fourth, perform sampling obtain sample size \\(n\\)?Urn: Using shovel \\(n\\) slots.Obama poll: One method get list phone numbers young Americans pick \\(n\\) phone numbers. poll’s case, sample size poll \\(n = 2089\\) young Americans.Fifth, point estimate (AKA sample statistic) unknown population parameter?Urn: sample proportion \\(\\hat{p}\\) beads shovel red.Obama poll: sample proportion \\(\\hat{p}\\) young Americans sample approve Obama’s job performance. poll’s case, \\(\\hat{p} = 0.41 = 41\\%\\), quoted percentage second paragraph article.Sixth, sampling procedure representative?Urn: contents shovel representative contents urn? mixed urn sampling, can feel confident .Obama poll: sample \\(n = 2089\\) young Americans representative young Americans aged 18-29? depends whether sampling random.Seventh, samples generalizable greater population?Urn: sample proportion \\(\\hat{p}\\) shovel’s beads red “good guess” population proportion \\(p\\) urn’s beads red? Given sample representative, answer yes.Obama poll: sample proportion \\(\\hat{p} = 0.41\\) sample young Americans supported Obama “good guess” population proportion \\(p\\) young Americans supported Obama time 2013? words, can confidently say roughly 41% young Americans approved Obama time poll? , depends whether sampling random.Eighth, sampling procedure unbiased? words, observations equal chance included sample?Urn: Since bead equally sized mixed urn using shovel, bead equal chance included sample hence sampling unbiased.Obama poll: young Americans equal chance represented poll? , depends whether sampling random.Ninth lastly, sampling done random?Urn: long mixed urn sufficiently sampling, samples random.Obama poll: sample conducted random? can’t answer question without knowing sampling methodology used Kennedy School’s Institute Politics Harvard University. ’ll discuss end section.words, poll Kennedy School’s Institute Politics Harvard University can thought instance using shovel sample beads urn. Furthermore, another polling company conducted similar poll young Americans roughly time, likely get different estimate 41%. due sampling variation.Let’s now revisit sampling paradigm Subsection 6.3.1:general:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can infer population using sampling.Specific urn:extract sample \\(n = 50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 2400 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{p}\\) \\(n = 50\\) beads shovel red “good guess” population proportion \\(p\\) \\(N = 2400\\) beads red, thusinstead manually going 2400 beads urn, can infer urn using shovel.Specific Obama poll:way contacting randomly chosen sample 2089 young Americans polling approval President Obama 2013, thenthese 2089 young Americans unbiased representative sample young Americans 2013, thusany results based sample 2089 young Americans can generalize entire population young Americans 2013, thusthe reported sample approval rating 41% 2089 young Americans good guess true approval rating among young Americans 2013, thusinstead performing expensive census young Americans 2013, can infer young Americans 2013 using polling.can see, critical sample obtained Kennedy School’s Institute Politics Harvard University truly random order infer young Americans’ opinions Obama. sample truly random? ’s hard answer questions without knowing sampling methodology used.example, Kennedy School’s Institute Politics Harvard University conducted poll using mobile phone numbers? People without mobile phones left therefore represented sample. flaw example censoring, exclusion certain datapoints due issue data collection. results incomplete observation increases prediction uncertainty estimand, Obama’s approval rating among young Americans. Ensuring samples random easy sampling urn exercises; however, real life situation like Obama poll, much harder .visualized chapter demonstration famous theorem, mathematically proven truth, called Central Limit Theorem. loosely states sample means based larger larger sample sizes, sampling distribution sample means becomes normally shaped narrow. words, sampling distribution increasingly follows normal distribution variation sampling distributions gets smaller, quantified standard errors.","code":""},{"path":"one-parameter.html","id":"wisdom-1","chapter":"6 One Parameter","heading":"6.4.1 Wisdom","text":"Recall Wisdom, one key themes, encompasses two primary issues: relevance estimand map concept data.polling example, estimand (Obama’s approval rating) relevant research question want answer? wanted know Obama’s approval rating nothing else, yes. , ’s case . aim poll draw conclusion voter’s likelihood voting Obama next election using approval rating predictive measure voting behavior.problem, estimating Obama’s electability young people, solved poll. wanted know answer problem, need change key aspects sampling.First, young Americans vote! fact, many . Therefore, wanted draw conclusion Obama’s favorability among young American voters, sample population need modified include registered voters intention voting next election. sample population random young Americans, question voting behavior, tells us almost nothing key problem.\nFIGURE 6.22: Sampling doesn’t work data doesn’t map research question!\nSecond, need change question. true question Obama’s favoribility among young Americans, actually: vote Obama next election? allow flexibility responses get range responses accurately refect uncertainty likelihood, might pose following question: scale one ten, one “definitely ” ten “definitely,” likely vote Obama next election?extremely important, often overlooked, data analyzing map research question trying answer. polling example, ’ve just discovered collected data tells us little really want know. sample population research question flawed therefore offer us little information relevant key issue: voting behavior young Americans next election.","code":""},{"path":"one-parameter.html","id":"justice-1","chapter":"6 One Parameter","heading":"6.4.2 Justice","text":"now , ignore fact map concept data flawed. Assume approval rating thing want know.First, must determine modeling (just) prediction (also) modeling causation. Another way looking using model prediction model explanation.polling attitude young Americans towards Obama, measuring causal effect. models causal, change value one input figure new output , thus allowing us calculate causal effect specific individual. , using attitude young Americans towards Obama predict outcome next election.Let’s consider predictive model means terms Preceptor Tables. First, know actual Preceptor Table poll, results 2089 young Americans total population young Americans exceeds 50 million units (people). means , actual Preceptor Table, 50 million rows, mere 2089 rows include approval rating. ideal Preceptor Table case report approval rating every one 50 million rows. quickly becomes clear, popuation size large, sampling generalizing essential practical inference.Recall infinite Preceptor Table gives us information want know one units. Though infinite Preceptor Table practial even possible contexts, ’s important consider decrease size infinite Preceptor Table, mostly assuming certain rows columns “exchangeable.” run issue realism polling example. poll assumes columns “approval rating Obama” “---planning--vote-” interchangable. ’ve seen, columns aren’t analogs!","code":""},{"path":"one-parameter.html","id":"courage-1","chapter":"6 One Parameter","heading":"6.4.3 Courage","text":"actual Preceptor Table riddled question marks. fill ?important concepts statistics data science Data Generating Mechanism (DGM). data, data collect see, generated complexity confusion world. universes’s mechanism brought data us. job build model process, create, computer, mechanism generates fake data consistent data see. DGM, can answer question might . particular, DGM, provide predictions data seen estimates uncertainty associated predictions.need “machine” generates predictions, thing machine fills question marks actual Preceptor Table, thing machine produces “fake data” looks lot like actual data.theme Courage places emphasis two things: code brings model life uncertainty real life impacts model’s place world.think uncertainty, consider case Primrose Everdeen chapter’s introduction. Imagine knew exactly many names entered Reaping urn, number times name entered, wanted predict names going selected. Sounds simple. data, use DGM simulate Reaping event many times wanted, create distribution, give list names selected frequently many simulations. yet, actual name drawn urn ! Primrose represents another challenge prediction game: unmodeled variation.Even perfect parameter estimates model structure matches unknown data generating mechanism, still won’t make perfect predictions. randomness intrinsic. randomness known unmodeled variation.","code":""},{"path":"one-parameter.html","id":"temperance-1","chapter":"6 One Parameter","heading":"6.4.4 Temperance","text":"Temperance perhaps important virtue data science. models never good appear . world complex , even worse, always changing. must aware unknown unknowns, concerned representative data/model problem, worried realism assumptions, leery siren call testing. simpler terms, must humble.\nFIGURE 6.23: Listen Kendrick. confident, assumptions world false!\nreally care data haven’t seen yet, mostly data tomorrow. world changes, always ? general, world changes . means forecasts uncertain naive use model might suggest.apply poll, happen , week poll conducted, America entered war another nation? outbreak dangerous virus, spurring global pandemic? poll account events, known unknown unknowns, happened time collected data. conclusions world week later, announcement war outbreak virus, largely going wrong!brings us yet another reason must humble conclusions: realism. structure model match world? (never ) inferences make wrong. just hope won’t wrong.asking random sample young Americans approval rating Obama going give us enough information predict outcome next election? ! give us enough information predict voting behavior young Americans specific point time? Sadly, also .assumptions allowed poll move infinite Preceptor Table ideal Preceptor Table plausible. goal poll (predict whether young American vote Obama) match question asked (approval rating Obama among random sample young Americans), model lacks realism allows inferences applied real world.said, even model asked better question (“likely vote Obama next election?”) appropriate sample (registered voters ages 19 34), still much uncertainty real world assume conclusions relevant next day, next week, next month.","code":""},{"path":"one-parameter.html","id":"rubin-causal-model-1","chapter":"6 One Parameter","heading":"6.4.5 Rubin Causal Model","text":"end section, let’s consider polling scenario connects Rubin Causal Model (RCM).part, doesn’t! Remember RCM’s slogan: causation without manipulation.urn example Obama poll, manipulating units (beads opinions pollees, respectively). control treatment condition compare; causal effect measure. Therefore, RCM applied scenarios.","code":""},{"path":"one-parameter.html","id":"sampling-mechanism","chapter":"6 One Parameter","heading":"6.5 Sampling Mechanism","text":"One important aspects sampling sampling mechanism: mechanism sample population. concept related, distinctly different, assignment mechanism learned Chapter 3.assignment mechanism sorts units control experiment groups, sampling mechanism means aquire sample. Assignment mechanisms place urn paradigm since measuring causal relationship assigning beads specific groups.think sampling mechanism : certain beads sampled, others ? completely random?order investigate concept, let’s revisit Preceptor Tables.","code":""},{"path":"one-parameter.html","id":"preceptor-tables-1","chapter":"6 One Parameter","heading":"6.5.1 Preceptor Tables","text":"Recall Preceptor Table table rows columns data (reasonably) like . two different types Preceptor Tables applicable urn example: actual ideal.\nFIGURE 6.24: see another Precetor Table section.\nactual Preceptor Table shows actually know. Accordingly, table riddled question marks real world saddles us . ideal Preceptor Table Preceptor Table question marks, reasonable number rows columns. ideal Preceptor Table, need inference; estimand simple matter arithmetic.visualize different Preceptor Tables usefulness us data scientists, let’s revisit urn.case Rubin Causal Model, wish knew values every single unit every possible scenario. analogous ideal table know color identity every single bead. compare previous Preceptor Table, ideal Preceptor Table look like: can create ideal Preceptor Table performing exhaustive census entire urn. Let’s say , tedious process, find true real proportion red beads exactly 37.5%. know color every bead.world, estimand, proportion red beads urn, simple matter arithmetic. However, emphasized , performing exhaustive count easiest way estimate proportion red beads. Real life sampling far complex. process extremely prone error. Despite , people overestimate validity conclusions drawn sampling. stress unknowns sampling, let’s look actual Preceptor Table.Let’s imagine use shovel sample 100 beads urn. taking sample, find 40% sampled beads red. Let’s visualize looking entire urn taken sample. actual Preceptor Table. know colors randomly sampled 100 beads, remaining bead colors missing data! data? Well, take sample 100 beads, color identifications 100 total 2400 beads urn. rest beads sampled say certain whether white red.Something else must consider beads get sampled, others . consequence sampling mechanism. drawing sample using shovel 100 slots, 100 known values. Therefore, shovel (addition mixing urn beforehand) sampling mechanism.Consider : information tell us, specifically, Bead 1? Bead 2? know proportion red beads sample 40%. mean bead 1 precisely 40% chance red? learned Chapter 5, true! uncertainty.can claim certain , 100 beads sampled (total 2400 beads urn), 40% red. making prediction probability one sampled beads red, 40% correct probability. making prediction probability unsampled bead red, answer 40% incorrect, likely extremely incorrect. sample tell us something.","code":""},{"path":"one-parameter.html","id":"precision-versus-accuracy-or-bias-versus-variance","chapter":"6 One Parameter","heading":"6.5.2 Precision versus accuracy (or bias versus variance)","text":"saw previous section sample size \\(n\\) increases, point estimates vary less less concentrated around true population parameter. variation quantified decreasing standard error. words, typical error point estimates decrease. sampling exercise, sample size increased, variation sample proportions \\(\\hat{p}\\) decreased. also known precise estimate.random sampling ensures point estimates accurate, hand large sample size ensures point estimates precise. terms “accuracy” “precision” may sound like mean thing, subtle difference. Accuracy describes “target” estimates , whereas precision describes “consistent” estimates . figure illustrates difference.\nFIGURE 6.25: Comparing accuracy precision.\nNow, ’s obvious best case scenario precise accurate option. However, real life sampling isn’t easy!option use shovel 200 slots, minor magnetic property caused pick slightly red beads 100 slotted shovel? one hand, larger shovel gives us increased precision due larger sample size. , magnetic property gives us decreased accuracy due sampling bias. , often case real world, tradeoff!","code":""},{"path":"one-parameter.html","id":"el-jefes-or-felipes","chapter":"6 One Parameter","heading":"6.5.3 El Jefe’s or Felipe’s?","text":"\nFIGURE 6.26: El Jefe’s, Harvard Square staple, known large portions delicious horchata.\n\nFIGURE 6.27: Felipe’s, another staple, said authentic experience addicting churros.\napply complex example urn, imagine conducting poll gather information whether Harvard students prefer El Jefe’s Taqueria Felipe’s Taqueria. given two options: 1) can conduct poll 50 students walking Lamont Library 2) can conduct poll 300 students walking outside El Jefe’s Taqueria.first scenario provides accurate estimate, since students outside Lamont Library much less likely biased sample. second scenario, however, provides precise estimate due larger sample size. Even though students outside El Jefe’s might patrons, safe assume least patrons, making sample somewhat biased. better option? depends views trade-offs accuracy precision. precision matters, prefer larger sample.","code":""},{"path":"one-parameter.html","id":"returning-to-our-question","chapter":"6 One Parameter","heading":"6.6 Returning to our question","text":"Recall question asked beginning chapter: pull 16 red beads sample size 50, distribution possibilities total number red beads entire urn?looked number different models accuracy precision. delve primary question, must return discussion distributions: posterior, joint, marginal.","code":""},{"path":"one-parameter.html","id":"joint-distribution","chapter":"6 One Parameter","heading":"6.6.1 Joint distribution","text":"Chapter 5 outlined key intuition behind inference: joint distribution data---might-see---experiment models----considering. take joint distribution, combine actual results experiment, calculate posterior distribution set possible models. words, start \\[p(models, data)\\]add specific results experiment, calculate conditional distribution models, given data seen:\\[p(models | data = results\\ \\ experiment)\\]\nrest just details.can thing sampling problem. Assume know 2,400 beads urn, either red white. experiment involves one time use paddle 50 slots. 2,401 models consideration. might zero red beads one red bead … 2,399 red beads 2,400 red beads. paddle size 50, 51 possible results experiment: zero red beads one red bead . . . 49 red beads 50 red beads. information can calculate joint distribution models considering experiment results might observe. joint distribution, can calculate conditional distribution total number red beads, given however many observed sample.better visualize distribution, take look 3D version plot. Note U shape graph. , mean note peaks areas number red beads paddle 0 50 50 50. due fact , draw 0 red beads sample 50 beads, possibilities number red beads urn consistently close 0. Similarily, sample 50 beads comes 50 red beads, possibilities number red beads urn consistently closer 2400.\nFIGURE 6.28: 3D image joint distribution created Rayshader.\n","code":""},{"path":"one-parameter.html","id":"posterior-distribution","chapter":"6 One Parameter","heading":"6.6.2 Posterior distribution","text":"follow steps now Chapter 5. run experiment. look question – asking posterior probability certain case. case find , using 50 slot shovel, get result 16 red beads. make posterior probability? want:\\[p(models | data = 16)\\]approach works . take “slice” joint distribution number red beads paddle equal 16. normalization, gives us posterior probability number red beads urn. Let’s call tibble data post use post create graphic posterior.visualize 3D plot, see slice figure . things note. First, x-axis extends 1500 (rather total 2400) due fact , paddle draws 16/50 = .32 = 32% red beads, almost impossible total number red beads total urn exceed 1500. likely number red beads total urn, paddle draws 16/50 red beads, somewhere 700-830 range. signified peak points. probability lower 700 830 less likely number red beads urn, conditional fact paddle drew 16/50 red beads. posterior distribution.value red beads found paddle (0 50), posterior distribution possible values red beads total urn. distribution meant illustrate , though sample gives estimate total proportion red beads entire urn, much variation actual proportion might .Recall Question #1 start Chapter: “get 16 red beads random sample size 50, many red beads entire urn?” answer single number. posterior distribution just : distribution. sample, many different results total number red beads entire urn. Certain totals red beads, like extremes close 0 2,400, essentially impossible due sample value 32%. totals, like 800, occur far frequently totals.means , can provide range possibilities (can estimate possibilities occur frequently), can never say know total number red beads certainty.key issue urn paradigm , unlike real world data science, can technically find total number red beads. just perform exhaustive count ’ve desperately avoiding. real world, “true” solution can never known. require inference, tools like sampling helpful making assumptions world around us.\nFIGURE 6.29: 3D image joint distribution created Rayshader.\n","code":"\npost <- x %>% \n  filter(red_paddle == 16) %>% \n  group_by(red_urn) %>% \n  summarize(total = n(), .groups = \"drop\") %>%\n  mutate(probs = total/sum(total)) %>% \n  select(-total)\n\n\npost %>% \n  ggplot(aes(x = red_urn, y = probs)) +\n    geom_col() +\n    labs(title = \"Posterior Probability Distribution of Red Beads in Urn\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\")"},{"path":"one-parameter.html","id":"using-the-posterior","chapter":"6 One Parameter","heading":"6.6.3 Using the posterior","text":"Now posterior distribution number red beads urn, can use object forecast outcomes, outcomes yet observed. Recall, start Chapter, Question 2: “Conditional answer Question 1, probability, using earn, draw 8 red beads use shovel size 20?”answer 21%. R code bit tricky. begin post tibble, simple way specify posterior distribution number red beads urn. row, can simulate, 100 times, number red beads might draw, conditional assumption red_urn red beads urn. list-column results annoying work , unnest() . identify simulations result 8 red beads drawn.can simply take mean values positive results different rows posterior radically different probabilities true. need weighted mean, weights correspond probability number red beads urn , indeed, equal red_urn post.","code":"\npost %>% \n  mutate(new_reds = map(red_urn, \n                            ~ rbinom(n = 100, \n                                     size = 20,\n                                     prob = ./2400))) %>% \n  unnest(new_reds) %>% \n  mutate(res = ifelse(new_reds > 8, TRUE, FALSE)) %>% \n  summarise(final = weighted.mean(res, probs))## # A tibble: 1 x 1\n##   final\n##   <dbl>\n## 1 0.209"},{"path":"one-parameter.html","id":"summary-1","chapter":"6 One Parameter","heading":"6.7 Summary","text":"chapter, performed tactile virtual sampling exercises infer unknown parameter also presented case study sampling real life polls. case, used sample proportion \\(\\hat{p}\\) estimate population proportion \\(p\\). However, just limited scenarios related proportions. words, can use sampling estimate population parameters using point estimates well.continue journey, recall case Primrose Everdeen represents: matter realistic model , predictions never certain.","code":"\nlibrary(tidyverse)\nlibrary(rstanarm)\n\nshovel <- tibble(red = c(rep(1, 17), rep(0, 33))) %>% \n  sample_frac()\n  \n\nfit_3 <- stan_glm(red ~ 1, \n                  data = shovel,\n                  family = binomial(),\n                  refresh = 0,\n                  seed = 9)"},{"path":"two-parameters.html","id":"two-parameters","chapter":"7 Two Parameters","heading":"7 Two Parameters","text":"average height American male? 90th percentile distribution height American men? certain estimates? pass 5 men walking street, odds tallest least 5 centimeters taller shortest?","code":""},{"path":"two-parameters.html","id":"resampling-tactile","chapter":"7 Two Parameters","heading":"7.1 Pennies example","text":"Chapter 6, studied sampling. started “tactile” exercise wanted know proportion beads urn red. performed exhaustive count, tedious process. instead, used shovel extract sample 50 beads used resulting proportion red estimate. Furthermore, made sure mix urn’s contents every use shovel. randomness created mixing, different uses shovel yielded different proportions red hence different estimates proportion urn’s beads red.Remember: truth . urn. red white beads . exact, unknown, number beads red. exact, unknown, number beads white. exact, unknown, percentage beads red – defined number red divided sum number red number white. goal estimate unknown percentage. wanted make statements world, even can never certain statements true. never time inclination actually count balls. use term parameter things exist unknown. use statistics estimate true values parameters.mimicked physical sampling exercise equivalent virtual sampling exercise using computer. Subsection 6.2.4, repeated sampling procedure 1,000 times, using three different virtual shovels 25, 50, 100 slots. visualized three sets 1,000 estimates Chapter 6 saw sample size increased, variation estimates decreased. expanded sample sizes 1 100., constructed sampling distributions. motivation taking 1,000 repeated samples visualizing resulting estimates study estimates varied one sample another; words, wanted study effect sampling variation. quantified variation estimates using standard deviation, special name: standard error. particular, saw sample size increased 1 100, standard error decreased thus sampling distributions narrowed. Larger sample sizes led precise estimates varied less around center.tied sampling exercises terminology mathematical notation related sampling Subsection 6.3.1. study population large urn \\(N\\) = 2,400 balls, population parameter, unknown quantity interest, population proportion \\(p\\) urn’s beads red. Since performing census expensive terms time energy, instead extracted sample size \\(n\\) = 50. point estimate, also known sample statistic, used estimate \\(p\\) sample proportion \\(\\hat{p}\\) 50 sampled beads red. Furthermore, since sample obtained random, can considered unbiased representative population. Thus results based sample generalized population. Therefore, proportion shovel’s balls red “good guess” proportion urn’s balls red. words, used sample draw inferences population.However, described Section 6.2, physical virtual sampling exercises one real life. merely activity used study effects sampling variation. real life situation, take 1,000 samples size \\(n\\), rather take single representative sample ’s large possible. Additionally, knew true proportion urn’s beads red 37.5%. real-life situation, know true value . , take sample estimate ?example realistic sampling situation poll, like Obama poll saw Section 6.4. Pollsters know true proportion young Americans supported President Obama 2013, thus took single sample size \\(n\\) = 2,089 young Americans estimate value.one quantify effects sampling variation single sample work ? directly study effects sampling variation one sample. One common method study bootstrap resampling, simply bootstrapping.want, single estimate unknown population parameter, also range highly plausible values? Going back Obama poll article, stated pollsters’ estimate proportion young Americans supported President Obama 41%. addition stated poll’s “margin error plus minus 2.1 percentage points.” “plausible range” [41% - 2.1%, 41% + 2.1%] = [38.9%, 43.1%]. range plausible values ’s known confidence interval, focus later sections chapter. Bayesian terms, want posterior distribution unknown parameter \\(p\\), proportion young Americans supported Obama.","code":""},{"path":"two-parameters.html","id":"to-the-bank","chapter":"7 Two Parameters","heading":"7.1.1 To the Bank","text":"Chapter 6, ’ll begin hands-tactile activity. almost always need tidyverse package.PPBDS.data includes data sets book. rsample includes functions bootstrapping. rstanarm makes easy create display Bayesian models.","code":"\nlibrary(PPBDS.data)\nlibrary(rsample)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(rstanarm)"},{"path":"two-parameters.html","id":"what-was-the-average-year-of-us-pennies-in-2019","chapter":"7 Two Parameters","heading":"7.1.2 What was the average year of US pennies in 2019?","text":"Try imagine pennies used United States 2019. ’s lot pennies! Now, say ’re interested average year minting pennies. One way compute value gather pennies used US, record year, compute average. However, nearly impossible! instead, let’s collect sample 50 pennies local bank downtown Northampton, Massachusetts, USA seen photo .\nFIGURE 7.1: Collecting sample 50 US pennies local bank.\n\nFIGURE 7.2: Collecting sample 50 US pennies local bank.\nimage 50 pennies can seen . 50 pennies starting top left, progressing row--row, ending bottom right, note “ID” identification variable printed black year minting printed white.\nFIGURE 7.3: 50 US pennies labelled.\nRun pennies_sample code create 50 sampled pennies.pennies_sample data frame 50 rows corresponding penny two variables. first variable ID corresponds ID labels table , whereas second variable year corresponds year minting saved numeric variable, also known double (dbl).Based 50 sampled pennies, can say US pennies 2019? Let’s study properties sample performing exploratory data analysis. Let’s first visualize distribution year 50 pennies using data visualization tools . Since year numerical variable, use histogram visualize distribution.\nFIGURE 7.4: Distribution year 50 US pennies.\nObserve slightly left-skewed distribution, since pennies fall 1980 2010 pennies older 1970. average year 50 sampled pennies? Eyeballing histogram, appears around 1990. Let’s now compute value exactly using data wrangling tools Chapter 2.Thus, ’re willing assume pennies_sample representative sample US pennies, “good guess” average year minting US pennies 1995.44. words, around 1995. start sounding similar previously Chapter 6!Chapter 6, study population urn \\(N = 2400\\) balls. population parameter population proportion balls red, denoted \\(p\\). order estimate \\(p\\), extracted sample 50 balls using shovel. computed relevant point estimate: sample proportion 50 balls red, denoted mathematically \\(\\hat{p}\\). also calculated posterior probability distribution \\(p\\)., population \\(N\\) – whatever number pennies used US, value don’t know probably never . population parameter interest now population mean year pennies, value denoted mathematically Greek letter \\(\\mu\\), pronounced “mu.” order estimate \\(\\mu\\), went bank obtained sample 50 pennies computed relevant point estimate: sample mean year 50 pennies, denoted mathematically \\(\\overline{x}\\) (pronounced “x-bar”).Going back 50 sampled pennies, point estimate interest sample mean \\(\\overline{x}\\) 1995.44. quantity estimate population mean year US pennies \\(\\mu\\).Recall also saw Chapter 6 estimates prone sampling variation. example, particular sample, observed three pennies year 1999. sampled another 50 pennies, observe exactly three pennies year 1999 ? likely . might observe none, one, two, maybe even 50! can said 26 unique years represented sample 50 pennies.sampling variation? One solution create bootstrap samples! Bootstrapping repeatedly draws independent samples data set replacement. sampling replacement, observation can sampled multiple times bootstrap sample number observations original data set.intuition bootstrapping can model inference population resampling sample data performing inference sample resample. look something like : resampled → sample → population.first thing want bootstrapping create bootstrap samples. Since concerned year pennies 2019, let’s select year data set create bootstraps. Let’s now perform virtual analog 1,000 resamples. Using results, ’ll able study variability sample means 1,000 resamples size 50. Let’s first add times = 1000 argument bootstraps() indicate like 1,000 replicates. Remember must use rsample library use bootstraps.bootstrap samples stored tibble-like object, bootstrap sample nested splits column. row different bootstrap sample id column used identify bootstrap sample.view specific bootstrap sample, use analysis() function rsample package, basically allows view specific bootstrap sample data frame. Consider first bootstrap sample:Replace 1 number see later bootstrap sample.Notice 50 rows, number rows pennies_sample. Now know create bootstrap samples view , can apply code bootstraps find desired statistic, average year pennies 2019.compute desired statistics, now create column boot.iterating bootstrap sample, applying analysis() row. boot now list-column tibble, can use want find specific characteristic sample like average year. Given boot list column want pull mean year interested , can create two columns:Voila! able create thousand bootstrap samples calculate mean year resample. Let’s now create plot visualizes posterior distribution mean year American pennies 2019.sneak word “posterior” discussion? Recall Chapter 5 defined posterior distribution beliefs unknown number: either number don’t know now know, like Biden’s electoral vote total number can never know, like average year pennies. case bootstrap samples made, posterior distribution represents beliefs \\(\\mu\\), mean year American pennies 2019 taking account information bootstrap sample means. demonstrated plot , can see \\(\\mu\\) — unknown parameter, true value never know — likely 1992 1998.proved bootstrap, almost magically, can create reasonable posterior? ! mathematics proof beyond scope book.","code":"\npennies_sample <- tibble(ID = c(1:50), \n                         year = c(2002, 1986, 2017, 1988, 2008, 1983, \n                                  2008, 1996, 2004, 2000, 1994, 1995, \n                                  2015, 1978, 1974, 2015, 2016, 1996, \n                                  1983, 1971, 1981, 1976, 1998, 2017, \n                                  1979, 1979, 1993, 2006, 1988, 1978, \n                                  2013, 1976, 1979, 1985, 1985, 2015, \n                                  1962, 1999, 2015, 1990, 1992, 1997,\n                                  2018, 2015, 1997, 2017, 1982, 1988, \n                                  2006, 2017))\npennies_sample## # A tibble: 50 x 2\n##       ID  year\n##    <int> <dbl>\n##  1     1  2002\n##  2     2  1986\n##  3     3  2017\n##  4     4  1988\n##  5     5  2008\n##  6     6  1983\n##  7     7  2008\n##  8     8  1996\n##  9     9  2004\n## 10    10  2000\n## # … with 40 more rows\npennies_sample %>%\n  ggplot(aes(x = year)) +\n  geom_histogram(binwidth = 10, color = \"white\")\npennies_sample %>% \n  summarize(mean_year = mean(year))## # A tibble: 1 x 1\n##   mean_year\n##       <dbl>\n## 1     1995.\nset.seed(9)\nvirtual_resamples <- pennies_sample %>%\n  select(year) %>%\n  bootstraps(times = 1000)\nvirtual_resamples## # Bootstrap sampling \n## # A tibble: 1,000 x 2\n##    splits          id           \n##    <list>          <chr>        \n##  1 <split [50/19]> Bootstrap0001\n##  2 <split [50/20]> Bootstrap0002\n##  3 <split [50/16]> Bootstrap0003\n##  4 <split [50/18]> Bootstrap0004\n##  5 <split [50/18]> Bootstrap0005\n##  6 <split [50/18]> Bootstrap0006\n##  7 <split [50/17]> Bootstrap0007\n##  8 <split [50/14]> Bootstrap0008\n##  9 <split [50/19]> Bootstrap0009\n## 10 <split [50/20]> Bootstrap0010\n## # … with 990 more rows\nanalysis(virtual_resamples$splits[[1]]) %>% \n  as_tibble()## # A tibble: 50 x 1\n##     year\n##    <dbl>\n##  1  1983\n##  2  2017\n##  3  1983\n##  4  2017\n##  5  1995\n##  6  1988\n##  7  1978\n##  8  2015\n##  9  1962\n## 10  1996\n## # … with 40 more rows\nvirtual_resamples <- pennies_sample %>%\n  select(year) %>%\n  bootstraps(times = 1000) %>%\n  mutate(boot = map(splits, ~ analysis(.)))\nvirtual_resamples## # Bootstrap sampling \n## # A tibble: 1,000 x 3\n##    splits          id            boot             \n##    <list>          <chr>         <list>           \n##  1 <split [50/19]> Bootstrap0001 <tibble [50 × 1]>\n##  2 <split [50/18]> Bootstrap0002 <tibble [50 × 1]>\n##  3 <split [50/20]> Bootstrap0003 <tibble [50 × 1]>\n##  4 <split [50/18]> Bootstrap0004 <tibble [50 × 1]>\n##  5 <split [50/19]> Bootstrap0005 <tibble [50 × 1]>\n##  6 <split [50/15]> Bootstrap0006 <tibble [50 × 1]>\n##  7 <split [50/13]> Bootstrap0007 <tibble [50 × 1]>\n##  8 <split [50/19]> Bootstrap0008 <tibble [50 × 1]>\n##  9 <split [50/20]> Bootstrap0009 <tibble [50 × 1]>\n## 10 <split [50/19]> Bootstrap0010 <tibble [50 × 1]>\n## # … with 990 more rows\nset.seed(9)\nvirtual_resamples <- pennies_sample %>%\n  select(year) %>%\n  bootstraps(times = 1000) %>%\n  mutate(boot = map(splits, ~ analysis(.))) %>%\n  mutate(years = map(boot, ~ pull(., year))) %>% \n  mutate(year_mean = map_dbl(years, ~ mean(.)))\nvirtual_resamples## # Bootstrap sampling \n## # A tibble: 1,000 x 5\n##    splits          id            boot              years      year_mean\n##    <list>          <chr>         <list>            <list>         <dbl>\n##  1 <split [50/19]> Bootstrap0001 <tibble [50 × 1]> <dbl [50]>     1992.\n##  2 <split [50/20]> Bootstrap0002 <tibble [50 × 1]> <dbl [50]>     1999.\n##  3 <split [50/16]> Bootstrap0003 <tibble [50 × 1]> <dbl [50]>     1992.\n##  4 <split [50/18]> Bootstrap0004 <tibble [50 × 1]> <dbl [50]>     1993.\n##  5 <split [50/18]> Bootstrap0005 <tibble [50 × 1]> <dbl [50]>     1995.\n##  6 <split [50/18]> Bootstrap0006 <tibble [50 × 1]> <dbl [50]>     1998.\n##  7 <split [50/17]> Bootstrap0007 <tibble [50 × 1]> <dbl [50]>     1993.\n##  8 <split [50/14]> Bootstrap0008 <tibble [50 × 1]> <dbl [50]>     1995.\n##  9 <split [50/19]> Bootstrap0009 <tibble [50 × 1]> <dbl [50]>     1993.\n## 10 <split [50/20]> Bootstrap0010 <tibble [50 × 1]> <dbl [50]>     2000.\n## # … with 990 more rows\nvirtual_resamples %>% \n  ggplot() +\n    geom_histogram(aes(x = year_mean, \n                       y = after_stat(count/sum(count))), \n                       binwidth = .5) +\n    labs(x = \"Mean Year\", \n         y = \"Probability\",\n         title = \"Posterior Distribution for the Mean Year of American Pennies in 2019\") "},{"path":"two-parameters.html","id":"eda-for-nhanes","chapter":"7 Two Parameters","heading":"7.2 EDA for nhanes","text":"Shifting away dealing pennies, let’s look bootstrap modeling nhanes dataset National Health Nutrition Examination Survey conducted Centers Disease Control Prevention covering children adults America. located PPBDS.data package loaded .nhanes data diverse array things like physical attributes, education, sleep. Let’s restrict attention subset, focusing gender, height year survey.Look random sample data:Notice decimal height column ch7. height <dbl> <int>.Let’s also run glimpse() new data.lookout anything suspicious. NA’s data set? types data columns, .e. survey characterized integer instead double? data collected 2009? females males? can never look data closely.addition glimpse(), can run skim(), skimr package, calculate summary statistics.TABLE 7.1: Data summaryVariable type: characterVariable type: numericInteresting! 353 missing values height subset data. Just using glimpse() show us . Let’s filter NA’s using drop_na. delete rows value variable missing. simplicity, let’s consider adults.Plot data. geom_density() smooth version geom_histogram(). geom_density(), y-axis scaled area curve equals 1.can see probable heights genders men generally taller women.","code":"\nglimpse(nhanes)## Rows: 10,000\n## Columns: 15\n## $ survey         <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, …\n## $ gender         <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Mal…\n## $ age            <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 5…\n## $ race           <chr> \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", …\n## $ education      <ord> High School, High School, High School, NA, Some Colleg…\n## $ hh_income      <ord> 25000-34999, 25000-34999, 25000-34999, 20000-24999, 35…\n## $ weight         <dbl> 87, 87, 87, 17, 87, 30, 35, 76, 76, 76, 68, 78, 75, 39…\n## $ height         <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170,…\n## $ bmi            <dbl> 32, 32, 32, 15, 31, 17, 21, 27, 27, 27, 24, 24, 26, 19…\n## $ pulse          <int> 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80…\n## $ diabetes       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ general_health <int> 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, N…\n## $ depressed      <ord> Several, Several, Several, NA, Several, NA, NA, None, …\n## $ pregnancies    <int> NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA…\n## $ sleep          <int> 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA…\nch7 <- nhanes %>% \n  select(age, gender, height, survey)\nch7 %>% \n  sample_n(5)## # A tibble: 5 x 4\n##     age gender height survey\n##   <int> <chr>   <dbl>  <int>\n## 1    80 Female   157.   2009\n## 2     0 Female    NA    2009\n## 3    19 Female   161.   2011\n## 4    39 Male     175    2009\n## 5    24 Female   159.   2009\nch7 %>%\n  glimpse()## Rows: 10,000\n## Columns: 4\n## $ age    <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9…\n## $ gender <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fem…\n## $ height <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, 182, 16…\n## $ survey <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 20…\nch7 %>% \n  skim()\nch7 <- nhanes %>% \n  select(age, gender, height, survey) %>%\n  filter(age >= 18) %>% \n  drop_na()\nch7 %>%\n  ggplot(aes(x = height, color = gender)) + \n  geom_density() + \n  labs(x = \"Height\",\n       title = \"Height by Gender in NHANES Dataset\")"},{"path":"two-parameters.html","id":"bootstrap-to-estimate-average-height","chapter":"7 Two Parameters","heading":"7.3 Bootstrap to estimate average height","text":"shown use bootstrap sampling create posterior distribution unknown parameter. Let’s use similar approach estimate value different unknown parameter: average height adult American male 2009. Let’s also name parameter \\(\\mu\\). confusing using parameter? Yes! , sadly, many Greek letters. choice reuse . convention, \\(\\mu\\) often used parameter name unknown mean. used different letter, Greek otherwise. , symbols besides \\(\\mu\\) often used unknown means. , , general, following conventions field wise.First, filter data set:Dropping missing values can dangerous, depending origin goals analysis. Never drop lightly.Second, use (almost) code :Plot results:posterior distribution includes information unknown parameter — mean height American males — used data estimate. don’t always want entire object. Instead, might want know :Mean: 175.97Mean: 175.97Median: 175.97Median: 175.9795% confidence interval: 175.66, 176.2895% confidence interval: 175.66, 176.28We use quantile() function calculate confidence interval. Note , book, use Bayesian interpretation confidence interval. truth . average height population specific number. don’t know number . 95% sure lies within 95% confidence interval.","code":"\nch7_male <- nhanes %>%\n  filter(survey == 2009, gender == \"Male\", age >= 18) %>%\n  select(height) %>%\n  drop_na()\nset.seed(9)\nvirtual_resamples <- ch7_male %>%\n  bootstraps(times = 1000) %>%\n  mutate(boot = map(splits, ~ analysis(.))) %>%\n  mutate(heights = map(boot, ~ pull(., height))) %>% \n  mutate(height_mean = map_dbl(heights, ~ mean(.)))\nvirtual_resamples## # Bootstrap sampling \n## # A tibble: 1,000 x 5\n##    splits             id            boot                heights      height_mean\n##    <list>             <chr>         <list>              <list>             <dbl>\n##  1 <split [1.8K/663]> Bootstrap0001 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  2 <split [1.8K/640]> Bootstrap0002 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  3 <split [1.8K/662]> Bootstrap0003 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  4 <split [1.8K/649]> Bootstrap0004 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  5 <split [1.8K/669]> Bootstrap0005 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  6 <split [1.8K/660]> Bootstrap0006 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  7 <split [1.8K/661]> Bootstrap0007 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  8 <split [1.8K/641]> Bootstrap0008 <tibble [1,814 × 1… <dbl [1,814…        176.\n##  9 <split [1.8K/647]> Bootstrap0009 <tibble [1,814 × 1… <dbl [1,814…        176.\n## 10 <split [1.8K/657]> Bootstrap0010 <tibble [1,814 × 1… <dbl [1,814…        176.\n## # … with 990 more rows\nvirtual_resamples %>% \n  ggplot() +\n    geom_histogram(aes(x = height_mean, \n                       y = after_stat(count/sum(count))), \n                       binwidth = 0.02) +\n    labs(x = \"Mean Height\", \n         y = \"Probability\",\n         title = \"Posterior Distribution for the Mean Height of American Males in 2009\") "},{"path":"two-parameters.html","id":"probability-to-bootstrap-to-bayesian-models","chapter":"7 Two Parameters","heading":"7.4 Probability to bootstrap to Bayesian models","text":"textbooks , stage, provide mathematical explanation transition making Chapter 5 chapter. Chapters 5 6 dealt discrete set possible models. began examples two three possible “true” states world. either infected infected. either zero, one two white marbles bag. examples grew complex, increasing number models consideration increasing number possible outcomes experiment. case urn, 2,401 possible models: either zero one two . . . 2,400 red beads urn.transition discrete set possible models infinite set possible models mathematically complex easy intuition. Just wave hands, imagine lots models, invoke aesthetic appeal smoothness. case height, infinite number possible models: average height adult American men 2009 175, 175.1, 175.14, 175.148, 175.1482, . infinite number possible values since height continuous. Yet, almost miraculously, intuition applies.Let’s use \\(\\mu\\) parameter unknown average height adult men America 2009. exactly analogous parameter \\(p\\) Chapter 6, proportion red beans urn. difference infinite number values \\(\\mu\\) might take. restricted \\(p\\) 2,401 possible values: \\(0\\), \\(1/2400\\), \\(2/2400\\), …, \\(2399/2400\\), \\(1\\).Although bootstrap can create posterior distribution, , much simpler ways . common involves function stan_glm() rstanarm library. Halfway book, now ready first full scale data science project. Let us guided cardinal virtues.","code":""},{"path":"two-parameters.html","id":"cardinal-virtues","chapter":"7 Two Parameters","heading":"7.5 Cardinal Virtues","text":"Data science ultimately moral act, use four Cardinal Virtues — Wisdom, Justice, Courage Temperance — organize approach. purpose section two-fold. First, show formal Bayesian approach results , less, answer bootstrap , much less code. Second, show Cardinal Virtues guide good data science.","code":""},{"path":"two-parameters.html","id":"wisdom-2","chapter":"7 Two Parameters","heading":"7.5.1 Wisdom","text":"decision face? reason making models , primarily making models fun, although ! reason face decision. must decide X Y. must choose , B C. must set D specific numeric value. Given decision, make model world help us.textbook, tough avoid “toy problem” trap. real world complex. substantive decision problem includes great deal complexity requires great deal context. time get level detail. , simplify. going create model height adult men. use model answer three questions:probability next adult male meet taller 180 centimeters?probability next adult male meet taller 180 centimeters?probability , among next 4 men meet, tallest least 10 cm taller shortest?probability , among next 4 men meet, tallest least 10 cm taller shortest?posterior probability distribution height 3rd tallest man next 100 meet?posterior probability distribution height 3rd tallest man next 100 meet?first two questions single number, single probability, answer. third question requires full scale posterior probability distribution.starting process, need check data — sample adult American men 2009 — allow us answer questions, however roughly.Wisdom comes . social sciences, never perfect relationship data question trying answer. Data American males 2009 thing data American males today. data men France Mexico. Moreover, problem hasn’t specified Earth , near. Walking near basketball tournament generate different answers walking around Times Square.Yet, data relevant. Right? certainly better nothing. , using -perfect data better using data .-perfect data always better? ! problem estimating median height 5th grade girls Toyko, doubt data relevant. Wisdom recognizes danger using non-relevant data build model mistakenly using model way make situation worse. data won’t help, don’t use data, don’t build model. Better just use common sense experience. find better data.aspect Wisdom ethics. Just can make model mean make model. Models can used evil , possible, evil. Fortunately, hard generate many ethical worries height models. , instead, modeling criminality, ethics become much complex . . .","code":""},{"path":"two-parameters.html","id":"justice-2","chapter":"7 Two Parameters","heading":"7.5.2 Justice","text":"Mathematical knowledge least important skill data scientist.However, little mathematical notation make modeling assumptions clear, bring precision approach. case:\\[ y_i =  \\mu + \\epsilon_i \\]\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) height male \\(\\). \\(\\mu\\) average height males population. \\(\\epsilon_i\\) “error term,” difference height male \\(\\) average height males. \\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\).simplest model can construct. Note:model two unknown parameters: \\(\\mu\\) \\(\\sigma\\). can anything else need estimate values parameters. Can ever know exact value? ! Perfection lies God’s R code. , using Bayesian approach similar used Chapters 5 6, able create posterior probability distributions parameter.model wrong, models.model wrong, models.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.\\(\\mu\\) average height men sample. can calculate directly. 175.97. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might run today. population adult men US 2009. close enough? better nothing? case different details matter.\\(\\mu\\) average height men sample. can calculate directly. 175.97. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might run today. population adult men US 2009. close enough? better nothing? case different details matter.Consider:\\[outcome = model + \\ \\ \\ \\ \\ model\\]\ncase, outcome height individual male. , also called “response,” trying understand /explain /predict. model creation, mixture data parameters, attempt capture underlying structure world generates outcome.difference outcome model? definition, model, blooming buzzing complexity real world. model always incomplete won’t capture everything. Whatever model misses thrown error term.Preceptor Table problem almost identical one saw Chapter 3: Since causal model, one potential outcome — say, one outcome: individual’s height.","code":""},{"path":"two-parameters.html","id":"courage-2","chapter":"7 Two Parameters","heading":"7.5.3 Courage","text":"data science, deal math, words, code, important code. need Courage create model, take leap faith can make ideas real.","code":""},{"path":"two-parameters.html","id":"stan_glm","chapter":"7 Two Parameters","heading":"7.5.3.1 stan_glm","text":"Bayesian models hard create R. Sticking filtered adult male 2009 data, can reduce work bootstrap approach stan_glm() function , fed correct inputs, creates Bayesian generalized linear model height. function comes rstanarm package, useful Bayesian models general.first argument stan_glm() function data, case filtered ch7_male tibble used bootstrap example. mandatory argument formula want build model around. case, since predictor variables, equation height ~ 1.Details:may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit.may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit.data argument, like usage R, used input data model.data argument, like usage R, used input data model.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.always assign result call stan_glm() object, . convention, name object often included word “fit” indicate fitted model object.always assign result call stan_glm() object, . convention, name object often included word “fit” indicate fitted model object.direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).default value family gaussian(), need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian(). \\(\\epsilon_i\\) different distribution, need use different family.default value family gaussian(), need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian(). \\(\\epsilon_i\\) different distribution, need use different family.","code":"\nset.seed(9)\nfit_obj <- stan_glm(data = ch7_male, \n                    height ~ 1, \n                    family = gaussian(), \n                    refresh = 0)"},{"path":"two-parameters.html","id":"printed-model","chapter":"7 Two Parameters","heading":"7.5.3.1.1 Printed model","text":"several ways examine fitted model. simplest print :first line telling us model used, case stan_glm().second line tells us model using Gaussian, normal, distribution. discussed distribution Section 2.9.3. normal probability distribution symmetric mean unimodal. reason, typically leave default unless working lefthand variable extremely non-normal, e.g., something takes two values like 0/1 TRUE/FALSE. Since height (roughly) normally distributed, Gaussian distribution good choice.third line gives us back formula provided. creating model predicting height constant — just simplest model can create. Formulas R constructed two parts. First, left side tilde (“~” symbol) “response” “dependent” variable, thing trying explain. Since model height, height goes lefthand side. Second, “explanatory” “independent” variables righthand side tilde. often many variables , simplest possible model, one, single constant. (number 1 indicates constant. mean think everyone height 1.)fourth fifth lines output tell us 1814 observations one predictor (constant). , terminology bit confusing. mean suggest \\(\\mu\\) “constant?” means , although \\(\\mu\\)’s value unknown, fixed. change person person. 1 formula corresponds parameter \\(\\mu\\) mathematical definition model.knew information fit model. R records fit_obj don’t want forget . second half display gives summary parameter values.see output two parameters model: intercept sigma. can confusing! Recall thing care \\(\\mu\\), average height population. ideal Preceptor Table — row every adult male population care missing data — \\(\\mu\\) trivial calculate, uncertainty. know named parameter \\(\\mu\\). R sees 1 formula. fields statistics, constant term called “intercept.” , now three things — \\(\\mu\\) (math), 1 (code), “intercept” (output) — refer exact concept. last time terminology confusing.point, stan_glm() — rather print() method rstan objects — problem. full posteriors \\(\\mu\\) \\(\\sigma\\). simple printed summary. can’t show entire distribution. , best numbers provide? right answer question! , choice provide median posterior “MAD_SD.”Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled standard deviations absolute difference observation median observations, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used .second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled standard deviations absolute difference observation median observations, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used .Instead printing whole model, can just print parameter values:Now understand meaning Median MAD_SD display, can interpret actual numbers. median intercept, 175.98, median posterior distribution \\(\\mu\\), average height American men 2009. median sigma, 7.29, median posterior distribution true \\(\\sigma\\), can roughly understood variability height men, account estimate \\(\\mu\\).MAD_SD parameter measure variability posterior distributions. spread ? Speaking roughly, 95% mass posterior distribution located within +/- 2 MAD SDs median. example, 95% confident true value \\(\\mu\\) somewhere 175.6 176.3.","code":"\nfit_obj## stan_glm\n##  family:       gaussian [identity]\n##  formula:      height ~ 1\n##  observations: 1814\n##  predictors:   1\n## ------\n##             Median MAD_SD\n## (Intercept) 176.0    0.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 7.3    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nprint(fit_obj, detail = FALSE)##             Median MAD_SD\n## (Intercept) 176.0    0.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 7.3    0.1"},{"path":"two-parameters.html","id":"plotting-the-posterior-distributions","chapter":"7 Two Parameters","heading":"7.5.3.1.2 Plotting the posterior distributions","text":"Instead math heads, can display posterior distributions. Pictures speak math mumbles. Fortunately, getting draws posteriors easy:4,000 rows “draws” estimated posteriors, column. like vectors result calling functions like rnorm() rbinom(). can create plot similar way:Although possible variable names like “(Intercept),” recommended. Avoid weird names! stuck , place backticks. Even better, rename , ., \\(\\sigma\\) usually nuisance parameter. don’t really care value us, rarely plot .","code":"\nfit_obj %>% \n  as_tibble()## # A tibble: 4,000 x 2\n##    `(Intercept)` sigma\n##            <dbl> <dbl>\n##  1          176.  7.10\n##  2          176.  7.39\n##  3          176.  7.20\n##  4          176.  7.28\n##  5          176.  7.19\n##  6          176.  7.43\n##  7          176.  7.22\n##  8          176.  7.32\n##  9          176.  7.38\n## 10          176.  7.28\n## # … with 3,990 more rows\nfit_obj %>% \n  as_tibble() %>% \n  rename(mu = `(Intercept)`) %>% \n  ggplot(aes(x = mu)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Average height among American adult men in 2009\",\n         x = \"Height in Centimeters\",\n         y = \"Probability\") +\n    theme_classic()\nfit_obj %>% \n  as_tibble() %>% \n  ggplot(aes(x = sigma)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), binwidth = 0.01, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Height standard deviation among American adult men in 2009\",\n         x = \"Sigma in Centimeters\",\n         y = \"Probability\") +\n    theme_classic()"},{"path":"two-parameters.html","id":"decomposing-the-outcome","chapter":"7 Two Parameters","heading":"7.5.3.2 Decomposing the outcome","text":"Two important concepts model creation “fitted” values residuals.fitted value represents model’s best guess true value outcome individual, given information covariates. tricky concept since, , already know actual value . residual difference outcome fitted value. definitions lead natural decomposition outcome data:","code":"\nch7_male %>% \n  mutate(fitted_value = fitted(fit_obj)) %>% \n  mutate(residual = residuals(fit_obj)) %>% \n  sample_n(5)## # A tibble: 5 x 3\n##   height fitted_value residual\n##    <dbl>        <dbl>    <dbl>\n## 1   179.         176.     2.92\n## 2   191.         176.    14.7 \n## 3   172.         176.    -3.48\n## 4   168.         176.    -8.08\n## 5   164.         176.   -12.1"},{"path":"two-parameters.html","id":"temperance-2","chapter":"7 Two Parameters","heading":"7.5.4 Temperance","text":"Recall “matrix” R rectangular array data, shaped like data frame tibble, containing one type data, e.g., numeric. Large matrices also print ugly. (differences, none care .) Example:easiest way pull information matrix use [], subset operator. grab second column m:Note matrices just one dimension “collapse” single vectors. Tibbles, hand, always maintain rectangular shapes, even one column row. Matrices important posterior_predict() functions rstanarm return matrices.model. can ? Let’s answer three questions started section.probability next adult male meet taller 180 centimeters?model American male height 2009, fit_obj, can use purpose.Unfortunately, posterior_predict() returns weird object class “ppd,” stands posterior probability distribution. advanced use cases useful class object work . , purposes book, “ppd” class complex. , whenever call posterior_predict(), always transform tibble like :requires two steps. First, use as_tibble(), just might expect. R, often transform one thing another thing functions begin as_. Unfortunately, solve problem column still class ppd. , second, use mutate_all(.numeric) incantation transform column. resulting object, pp, still easy work , variable names numbers big .4000 rows , default, stan_glm() gives us 4000 draws posterior distribution, distribution parameters (looked Courage) predicted values. \n1814 matrix provides draws posterior predictive distribution input data rows. (case, — draws posterior predictive distribution — model use covariates. complex models, columns pp tibble can draws different distributions.)want posterior prediction observation? , already know value observation! know everyone’s height data set. prediction necessary.reason , order confirm model consistent data, compare posterior probability distribution observation actual value observation. consistent. , model sensible, 95% true observations lie within 95% confidence interval respective posterior probability distributions. process comparison posterior predictive check.meantime, can still use column pp answer question. (use first column convenience.) Consider:don’t put posterior predictions tibble, makes everything easier. odds next adult male taller 180 centimeters?Somewhere around 29% ., key difficulty population. problem actually involves walking around London, wherever, today. data involve America 2009. things! totally different. Knowing whether data “close enough” problem want solve heart Wisdom. Yet decision made start process, decision create model first place. Now created model, look virtue Temperance guidance using model. data never perfect match world face. need temper confidence act humility. forecasts never good naive use model might suggest. Reality surprise us. need take model’s claims family-sized portion salt.probability , among next 4 men meet, tallest least 10 cm taller shortest?Bayesian models beautiful , via magic simulation, can answer (almost!) question. simulation, just need answer step step.75% chance , meeting 4 random men, tallest least 10 cm taller shortest.posterior probability height 3rd tallest man next 100 meet?approach work almost question.","code":"\nm <- matrix(c(3, 4, 8, 9, 12, 13), ncol = 2)\nm##      [,1] [,2]\n## [1,]    3    9\n## [2,]    4   12\n## [3,]    8   13\nm[, 2]## [1]  9 12 13\nset.seed(11)\npp <- posterior_predict(fit_obj)\nset.seed(11)\npp <- posterior_predict(fit_obj) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)\ndim(pp)## [1] 4000 1814\ntibble(pred = pp$`1`) %>% \n  mutate(gt_180 = ifelse(pred > 180, TRUE, FALSE))## # A tibble: 4,000 x 2\n##     pred gt_180\n##    <dbl> <lgl> \n##  1  172. FALSE \n##  2  191. TRUE  \n##  3  166. FALSE \n##  4  169. FALSE \n##  5  178. FALSE \n##  6  164. FALSE \n##  7  172. FALSE \n##  8  185. TRUE  \n##  9  173. FALSE \n## 10  183. TRUE  \n## # … with 3,990 more rows\ntibble(pred = pp$`1`) %>% \n  mutate(gt_180 = ifelse(pred > 180, TRUE, FALSE)) %>% \n  summarize(answer = sum(gt_180) / n())## # A tibble: 1 x 1\n##   answer\n##    <dbl>\n## 1  0.290\ntibble(pred_1 = pp$`1`,\n       pred_2 = pp$`2`,\n       pred_3 = pp$`3`,\n       pred_4 = pp$`4`) %>% \n  rowwise() %>% \n  mutate(tallest = max(c_across(pred_1:pred_4))) %>% \n  mutate(shortest = min(c_across(pred_1:pred_4))) %>% \n  mutate(diff = tallest - shortest) %>% \n  mutate(gt_10 = ifelse(diff >= 10, TRUE, FALSE)) %>% \n  ungroup() %>% \n  summarize(answer = sum(gt_10) / n())## # A tibble: 1 x 1\n##   answer\n##    <dbl>\n## 1  0.758\npp[, 1:100] %>% \n  rowwise() %>% \n  mutate(third_tallest = sort(c_across(`1`:`100`), decreasing = TRUE)[3]) %>% \n  ungroup() %>% \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior Probability of the Height\",\n         subtitle = \"of the 3rd Tallest from One Hundred Random Men\",\n         x = \"Height (cm)\",\n         y = \"Probability\")"},{"path":"two-parameters.html","id":"summary-2","chapter":"7 Two Parameters","heading":"7.6 Summary","text":"next five chapters follow process just completed . start decision make. luck, data guide us. (Without data, even best data scientist struggle make progress.) Wisdom asks us: “data close enough decision face make using data likely helpful?” Often times, answer “.” Even data, ability make model, Wisdom tap us shoulder say, “Even can make model, don’t forget ask .” Ethics matter.start build model, Justice guide us. model descriptive causal? mathematical relationship dependent variable trying explain independent variables can use explain ? assumptions making distributions, especially regard error term?set model framework, need Courage implement model code. Without code, math world useless. created model, need understand . posterior distributions unknown parameters? seem sensible? interpret ?Temperance guides final step. model, can finally get back decision motivated exercise first place. can use model make statements world, confirm model consistent world use model make predictions numbers know.Let’s practice process another dozen times.","code":""},{"path":"three-parameters.html","id":"three-parameters","chapter":"8 Three Parameters","heading":"8 Three Parameters","text":"Models parameters. Chapter 6 created models single parameter \\(p\\), proportion red beads urn. Chapter 7 , used models two parameters: \\(\\mu\\) (average height population, generically known model “intercept”) \\(\\sigma\\) (variation height population). — can guess going? — build models three parameters: \\(\\sigma\\) (serves role throughout book) two intercepts: \\(\\beta_1\\) \\(\\beta_2\\). notation confusing, least different academic fields use inconsistent schemes. key just follow cardinal virtues tackle problem step step.","code":""},{"path":"three-parameters.html","id":"eda-for-trains","chapter":"8 Three Parameters","heading":"8.1 EDA for trains","text":"Always explore data. demonstrate modeling three parameters, use trains data set PPBDS.data package. Recall discussion Chapter 3. Enos (2014) randomly placed Spanish-speaking confederates nine train platforms around Boston, Massachusetts. Exposure Spanish-speakers – treatment – influenced attitudes toward immigration. reactions measured changes answers three survey questions. Let’s load libraries need chapter, used , look data., can see information respondent’s gender, political affiliations, age, income. Additionally, treatment indicates whether subject control treatment group, attitudes toward immigration (att_start) (att_end) experiment. can type ?trains read help page information variable. Let’s restrict attention subset variables.always smart look random samples data:att_end measure person’s attitude toward immigration. higher number means conservative, .e., exclusionary stance toward immigration United States.Pay attention variable types. make sense? Perhaps. certainly grounds suspicion. age att_end doubles rather integers? values data appear integers, benefit variables doubles. party character variable treatment factor variable? intentional choices made creator tibble, .e., us. mistakes. , likely, choices mixture sensible arbitrary. Regardless, responsibility notice . can’t make good model without looking closely data using.TABLE 8.1: Data summaryVariable type: characterVariable type: factorVariable type: numericskim() shows us different values treatment factor. Unfortunately, character variables like party. ranges age att_end seem reasonable. Recall participants asked three questions immigration issues, allowed answer indicated strength agreement scale form 1 5, higher values indicating agreement conservative viewpoints. att_end sum responses three questions, liberal possible value 3 conservative 15.Always plot data.Democrats dataset Republicans. Democrats also span wider range ages Republicans.boxplot, top bottom borders box denotes 75th 25th percentiles, respectively. line inside box denotes median data. Treated individuals higher mean att_end control group, higher distribution general.chapter, make two models. first explains age function party. second uses att_end dependent variable treatment independent variable.","code":"\nlibrary(PPBDS.data)\nlibrary(rstanarm)\nlibrary(skimr)\nlibrary(tidyverse)\nglimpse(trains)## Rows: 115\n## Columns: 8\n## $ gender    <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Fema…\n## $ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE…\n## $ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat…\n## $ age       <dbl> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53,…\n## $ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 13500…\n## $ att_start <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 13, 7…\n## $ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contr…\n## $ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8…\nch8 <- trains %>% \n  select(age, att_end, party, treatment)\nch8 %>% \n  sample_n(5)## # A tibble: 5 x 4\n##     age att_end party      treatment\n##   <dbl>   <dbl> <chr>      <fct>    \n## 1    44       9 Democrat   Control  \n## 2    67       9 Democrat   Control  \n## 3    42      10 Democrat   Treated  \n## 4    52      10 Republican Treated  \n## 5    46      12 Democrat   Control\nch8 %>% \n  glimpse()## Rows: 115\n## Columns: 4\n## $ age       <dbl> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53,…\n## $ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8…\n## $ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat…\n## $ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contr…\nch8 %>% \n  skim()\nch8 %>%\n  ggplot(aes(x = party, y = age)) + \n  geom_jitter(width = 0.1, height = 0) + \n  labs(title = \"Age by Party Affiliation in Trains Dataset\",\n       subtitle = \"Where are the old Republicans?\",\n       x = \"Party\",\n       y = \"Age\")\nch8 %>%\n  ggplot(aes(x = treatment, y = att_end)) + \n  geom_boxplot() + \n  labs(title = \"Attitude End by Treatment in Trains Dataset\",\n       subtitle = \"Did the treatment make people more conservative?\",\n       x = \"Treatment\",\n       y = \"Attitude After Experiment\")"},{"path":"three-parameters.html","id":"age-party","chapter":"8 Three Parameters","heading":"8.2 age ~ party","text":"want build model use model make claims world. probability , Democrat shows train station, 50 years old? group three Democrats three Republicans, age difference expect oldest Democrat youngest Republican? can answer similar questions creating model uses party affiliation predict age","code":""},{"path":"three-parameters.html","id":"wisdom-3","chapter":"8 Three Parameters","heading":"8.2.1 Wisdom","text":"\nFIGURE 8.1: Wisdom\ndata science, two main aspects Wisdom: representativeness ethics. data limited. 115 observations, 2012 involving train commuters Boston. useful data today, populations around Boston, cities US? judgment, along advice colleagues, can guide .key concept idea “population.” larger population data (conceptually) drawn? interested age indidividuals data set, need inference. know everyone’s ages already. need tools like stan_glm() seek understand individuals data. , start data science, need decide something larger population interested . data “representative,” least extent, larger population can use data answer questions.ethical issues , fortunately, fraught. Estimating someone’s age (universally?) viewed OK. Estimating someone’s health income criminal record far dicier.","code":""},{"path":"three-parameters.html","id":"justice-3","chapter":"8 Three Parameters","heading":"8.2.2 Justice","text":"\nFIGURE 8.2: Justice\nJustice, data science, consists three topics: predictive versus causal modeling, Preceptor Table, mathematical formulation model.model age dependent variable predictive, causal, simple reason nothing, time, can change age. X years old. matter changed party registration Democrat Republican vice versa. age age.terms Preceptor Table, fact means one potential outcome, .e., one outcome. potential outcome Democrat different potential outcome Republican.dealing non-causal model, focus predicting things. underlying mechanism connects age party less important brute statistical fact connection. Predictive models care little causality.good way looking Preceptor Table, seen . Unlike previous table Chapter 7, now two columns addition ID one. left, predictor used model, political party, right variable predicting, age. Since data include Republicans Democrats world, every row filled . now know working predictive model. Recall:\\[outcome = model + \\ \\ \\ model\\]words, event depends explicitly described model well influences unknown us. Everything happens world result various factors, can consider model (know influences, data ).far treated equation conceptually, fact works just like equation. Let’s bit concrete.\\[ y_i = \\beta_1 x_{r,} + \\beta_2 x_{d,} + \\epsilon_i\\]\n\\[x_{r,}, x_{d,} \\\\{0,1\\}\\] \n\\[x_{r,} +  x_{d,} = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Don’t panic dear poets philosophers, whole thing easier looks.left-hand side outcome, \\(y_i\\), variable explained. case, age.left-hand side outcome, \\(y_i\\), variable explained. case, age.right-hand side first part contained model, consisting two similar terms. term consists parameter data point. betas two parameters: \\(\\beta_1\\) average age Republicans population \\(\\beta_2\\) average age Democrats population. \\(x\\)’s explanatory variables take values 1 0. someone Republican \\(x_{r,} = 1\\) \\(x_{d,} = 0\\), someone Democrat \\(x_{r,} = 0\\) \\(x_{d,} = 1\\). words, \\(x\\)’s binary variables mutually exclusive (Democrat, also Republican).right-hand side first part contained model, consisting two similar terms. term consists parameter data point. betas two parameters: \\(\\beta_1\\) average age Republicans population \\(\\beta_2\\) average age Democrats population. \\(x\\)’s explanatory variables take values 1 0. someone Republican \\(x_{r,} = 1\\) \\(x_{d,} = 0\\), someone Democrat \\(x_{r,} = 0\\) \\(x_{d,} = 1\\). words, \\(x\\)’s binary variables mutually exclusive (Democrat, also Republican).last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. includes factors influence someone’s age connected party affiliation. assume error follows normal distribution expected value 0 (meaning 0 average).last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. includes factors influence someone’s age connected party affiliation. assume error follows normal distribution expected value 0 (meaning 0 average).small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\). corresponding \\(x\\)’s \\(r\\) \\(d\\) subscript, Republican Democrat.small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\). corresponding \\(x\\)’s \\(r\\) \\(d\\) subscript, Republican Democrat.Keep mind model claim world works, just 115 individuals data people population seek draw inferences.Keep mind model claim world works, just 115 individuals data people population seek draw inferences.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula,” model “regression model.” terminology also apply model height Chapter 7.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula,” model “regression model.” terminology also apply model height Chapter 7.","code":""},{"path":"three-parameters.html","id":"courage-3","chapter":"8 Three Parameters","heading":"8.2.3 Courage","text":"\nFIGURE 8.3: Courage\nCourage allows us translate math code.get posterior distributions three parameters, use stan_glm(), just Chapter 7. take look formula, can see similar equation .variable tilde, age, outcome.variable tilde, age, outcome.explanatory variable party. variable two values, ‘Democrat’ ‘Republican.’explanatory variable party. variable two values, ‘Democrat’ ‘Republican.’also added -1 end equation, indicating want intercept, otherwise added default.also added -1 end equation, indicating want intercept, otherwise added default.resulting output:partyDemocrat corresponds \\(\\beta_1\\), average age Democrats population. partyRepublican corresponds \\(\\beta_2\\), average age Republicans population. Since don’t really care posterior distribution \\(\\sigma\\), won’t discuss . Graphically:unknown parameters \\(\\beta_1\\) (partyDemocrat) \\(\\beta_2\\) (partyRepublican) still unknown. can never know true average age Democrats population. can calculate posterior probability distribution parameters. Comments:Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 , data related parameter, narrower posterior distribution .estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 , data related parameter, narrower posterior distribution .great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.Look following table shows sample 8 individuals. fitted values Republicans Democrats, model produces one fitted value condition. table shows just sample 8 individuals captures wide range residuals, making difficult predict age new individual. can get better picture unmodeled variation sample plot three variables individuals data.following three histograms show actual outcomes, fitted values, residuals people trains:three plots structured like equation table . value left plot sum one value middle plot plus one right plot.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.Since residuals plot represents difference two plots, distribution looks like first plot.Since residuals plot represents difference two plots, distribution looks like first plot.","code":"\nfit_obj <- stan_glm(age ~ party - 1, \n                    data = trains, \n                    refresh = 0)\nfit_obj## stan_glm\n##  family:       gaussian [identity]\n##  formula:      age ~ party - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                 Median MAD_SD\n## partyDemocrat   42.6    1.2  \n## partyRepublican 41.1    2.8  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 12.3    0.8  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_obj %>% \n  as_tibble() %>% \n  select(-sigma) %>% \n  mutate(Democrat = partyDemocrat, Republican = partyRepublican) %>%\n  pivot_longer(cols = Democrat:Republican,\n               names_to = \"parameter\",\n               values_to = \"age\") %>% \n  ggplot(aes(x = age, color = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Average age for Boston commuters in 2012\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"three-parameters.html","id":"temperance-3","chapter":"8 Three Parameters","heading":"8.2.4 Temperance","text":"\nFIGURE 8.4: Temperance\nRecall first questions began section:probability , Democrat shows train station, 50 years old?far tried model people data set whose real age already knew. helpful understand model, ultimate goal understand real world, people don’t yet know much . Temperance guides us make meaningful predictions become aware known unknown limitations.Start simple question, chances random Democrat 50 years old? First, create tibble desired input model. case tibble variable named “party” contains single observation value “Democrat.” bit different Chapter 7.Next, ’ll use posterior_predict() function get draws posterior scenario. Note new posterior distribution consideration . unknown parameter, call \\(D_{age}\\), age Democrat. age randomly selected Democrat population next Democrat meet next Democrat interview train platform. definition “population” determines appropriate interpretation. Yet, regardless, \\(D_{age}\\) unknown parameter. one — like \\(\\beta_1\\), \\(\\beta_2\\), \\(\\sigma\\) — already created posterior probability distribution. need posterior_predict().posterior_predict() takes two arguments: model simulations run, tibble indicating many parameters want run simulations. case, model one Courage tibble one just created.might expect can use as_tibble() directly object returned posterior_predict(). Sadly, obscure technical reasons, won’t quite work. , need incantation mutate_all(.numeric) make sure resulting tibble well-behaved. command ensures every column tibble simple numeric vector, want.look first observations shows simply get ten draws model’s posterior distribution age Democrat. important understand concrete person trains dataset - algorithm posterior_predict() simply uses existing data trains estimate posterior distribution.posterior distribution, can answer (almost) reasonable question. case, probability next Democrat 50 around 27%.Recall second question:group three Democrats three Republicans, age difference expect oldest Democrat youngest Republican?start creating tibble desired input. Note name column (“party”) observations (“Democrat,” “Republican”) must always exactly original data set. tibble well model can used arguments posterior_predict():look output shows now 6 columns: one person. R name columns, arranged order specified persons tibble (D, D, D, R, R, R). determine expected age difference, can proceed follows:words, expect oldest Democrat 22 years older youngest Republican, surprised oldest Democrat actually younger youngest Republican group 6.","code":"\nnew_obs <- tibble(party = \"Democrat\")\nset.seed(9)\npp <- posterior_predict(fit_obj, newdata = new_obs) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)\n\nhead(pp, 10)## # A tibble: 10 x 1\n##      `1`\n##    <dbl>\n##  1  34.6\n##  2  34.0\n##  3  41.7\n##  4  41.4\n##  5  48.2\n##  6  26.6\n##  7  57.2\n##  8  41.0\n##  9  38.2\n## 10  39.5\npp %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior Probability Distribution for a Democrat's Age\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\ntibble(age = pp$`1`) %>% \n  mutate(ot_50 = ifelse(age > 50, TRUE, FALSE)) %>% \n  summarize(perc = sum(ot_50)/n())## # A tibble: 1 x 1\n##    perc\n##   <dbl>\n## 1 0.276\nnew <- tibble(party = c(\"Democrat\", \"Democrat\", \"Democrat\", \n                        \"Republican\", \"Republican\",\"Republican\"))\n\nset.seed(27)\npp <- posterior_predict(fit_obj, newdata = new) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)\n\nhead(pp, 10)## # A tibble: 10 x 6\n##      `1`   `2`   `3`   `4`   `5`   `6`\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 67.6   58.2  34.6  19.5  24.0  41.1\n##  2 44.6   59.4  72.1  40.5  20.8  37.8\n##  3 63.6   45.5  34.0  21.6  14.5  21.6\n##  4 44.7   49.1  37.7  30.8  46.6  44.6\n##  5  3.79  26.7  29.7  36.6  49.0  36.5\n##  6 39.9   29.5  65.8  21.5  36.7  54.2\n##  7 51.1   51.2  41.2  49.7  16.9  37.3\n##  8 59.1   70.1  22.4  29.2  20.7  48.3\n##  9 45.8   36.2  35.3  43.5  51.3  25.1\n## 10 63.0   53.5  47.5  55.2  47.4  54.3\npp %>% \n  set_names(c(\"dem_1\", \"dem_2\", \"dem_3\", \n              \"rep_1\", \"rep_2\", \"rep_3\")) %>% \n  rowwise() %>% \n  \n  # Creating three new columns. The first two are the \n  # highest age among Democrats and the lowest age\n  # among Republicans, respectively. The third one is\n  # the difference between the first two.\n  \n  mutate(dems_oldest = max(c_across(dem_1:dem_3)),\n         reps_youngest = min(c_across(rep_1:rep_3)),\n         age_diff = dems_oldest - reps_youngest) %>% \n  \n  # Ungroup to tell R not to perform rowwise operations\n  # anymore.\n  \n  ungroup() %>% \n  \n  # Create posterior probability distribution\n  \n  ggplot(aes(x = age_diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Age difference between the oldest/youngest of three Democrats/Republicans\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"three-parameters.html","id":"att_end-treatment","chapter":"8 Three Parameters","heading":"8.3 att_end ~ treatment","text":", created predictive model: someone’s party affiliation, can make better guess age absence information party. nothing causal model. Changing someone’s party registration can change age. example, build causal model. average treatment effect, exposing people Spanish-speakers, attitudes toward immigration? largest causal effect still 1 10 chance occurring?","code":""},{"path":"three-parameters.html","id":"wisdom-4","chapter":"8 Three Parameters","heading":"8.3.1 Wisdom","text":"\nFIGURE 8.5: Wisdom\nstill using data Enos (2014). Yet world different place today! data 2012, four years Donald Trump’s election president, still relevant? Can generalize data Boston commuters people Massachusetts, people US? obvious answers questions. data never perfect match problem face data always old. world constantly changing. use old data, need make assumptions stability model world parameters estimating. Whether assumptions reasonable difficult question, one requires knowledge world world now . Math won’t save us.critical issue always: population interested data come population? connection two, progress impossible. , case, willing consider population US residents last decade (including today), willing assume data set 115 individuals representative population, can use data create model answer question.Ethical issues tricky, least trickier context models dependent variable height age. (ethical issues conducting experiment first place non-trivial.) Assume make good model. prevent someone using information , say, influence voting donations? Imagine Republican Senate candidate hires Spanish-speakers ride commuter trains order shift voters’ attitudes toward immigration conservative direction. believes (correctly?) increase odds winning election. sort knowledge seek create? can make models. Indeed, purpose chapter show ! make models?","code":""},{"path":"three-parameters.html","id":"justice-4","chapter":"8 Three Parameters","heading":"8.3.2 Justice","text":"\nFIGURE 8.6: Justice\nthree elements Justice data science remain : predictive/causal models, Preceptor Table, mathematical formula.Preceptor Table model look similar one first half chapter, except now possible outcomes individual: att_end exposed treatment att_end exposed control.math model exactly math predictive model first half chapter, although change notation bit clarity.\\[ y_i = \\beta_1 x_{t,} + \\beta_2 x_{c,} + \\epsilon_i\\]\n\\[x_{t,}, x_{c,} \\\\{0,1\\}\\] \n\\[x_{t,} +  x_{c,} = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Nothing changed, except meaning data items interpretations parameters.left-hand side outcome, \\(y_i\\), variable explained. case, person’s attitude toward immigration experiment complete. \\(y_i\\) takes integer values 3 15 inclusive.left-hand side outcome, \\(y_i\\), variable explained. case, person’s attitude toward immigration experiment complete. \\(y_i\\) takes integer values 3 15 inclusive.right-hand side first part contained model, consisting two similar terms. two terms stand Treated Control work follows. term consists parameter data point. \\(\\beta_1\\) average attitude toward immigration treated individuals — exposed Spanish-speakers — population. \\(\\beta_2\\) average attitude toward immigration control individuals — exposed Spanish-speakers — population. \\(x\\)’s explanatory variables take values 1 0. someone Treated \\(x_{t,} = 1\\) \\(x_{c,} = 0\\), someone Control \\(x_{t,} = 0\\) \\(x_{c,} = 1\\). words, \\(x\\)’s binary variables mutually exclusive – Treated, also Control.right-hand side first part contained model, consisting two similar terms. two terms stand Treated Control work follows. term consists parameter data point. \\(\\beta_1\\) average attitude toward immigration treated individuals — exposed Spanish-speakers — population. \\(\\beta_2\\) average attitude toward immigration control individuals — exposed Spanish-speakers — population. \\(x\\)’s explanatory variables take values 1 0. someone Treated \\(x_{t,} = 1\\) \\(x_{c,} = 0\\), someone Control \\(x_{t,} = 0\\) \\(x_{c,} = 1\\). words, \\(x\\)’s binary variables mutually exclusive – Treated, also Control.Note formula applies everyone population, just 115 people data. index \\(\\) just go 1 115. goes 1 \\(N\\), \\(N\\) number individuals population. Conceptually, everyone att_end treatment control.Note formula applies everyone population, just 115 people data. index \\(\\) just go 1 115. goes 1 \\(N\\), \\(N\\) number individuals population. Conceptually, everyone att_end treatment control.last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. particular case, includes factors influence someone’s attitude toward immigration explained treatment status. assume error follows normal distribution expected value 0.last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. particular case, includes factors influence someone’s attitude toward immigration explained treatment status. assume error follows normal distribution expected value 0.small \\(\\)’s index number observations data set. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modelled factors person . corresponding \\(x\\)’s \\(t\\) \\(c\\) subscript, Treated Control.small \\(\\)’s index number observations data set. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modelled factors person . corresponding \\(x\\)’s \\(t\\) \\(c\\) subscript, Treated Control.","code":""},{"path":"three-parameters.html","id":"courage-4","chapter":"8 Three Parameters","heading":"8.3.3 Courage","text":"\nFIGURE 8.7: Courage\nJustice satisfied, gather Courage fit model. Note , except change variable names, code exactly , predictive model age. Predictive models causal models use math code. differences, important, lie interpretation results, creation.treatmentTreated corresponds \\(\\beta_1\\). always, R , behind scenes, estimated entire posterior probability distribution \\(\\beta_1\\). graph distribution next section. basic print method objects can’t show entire distribution, gives us summary numbers: median MAD SD. Speaking roughly, expect 95% values posterior within two MAD SD’s median. words, 95% confident true, unknowable, average attitude toward immigration among Treated population 9.2 10.8.treatmentControl corresponds \\(\\beta_2\\). analysis applies. 95% confident true value average attitude toward immigration Control population 7.9 9.1.now, used Bayesian interpretation “confidence interval.” also intuitive meaning , outside academia, almost universal. truth . don’t know, sometimes can’t know, truth. confidence interval, associated confidence level, tells us likely truth lie within specific range. boss asks confidence interval, almost certainly using interpretation., contemporary academic research, phrase “confidence interval” usually given “Frequentist” interpretation. (biggest divide statistics Bayesians Frequentist interpretations. Frequentist approach, also known “Classical” statistics, dominant 100 years. power fading, textbook uses Bayesian approach.) Frequentist, 95% confidence interval means , apply procedure used infinite number future situations like , expect true value fall within calculated confidence intervals 95% time. academia, distinction sometimes made confidence intervals (use Frequentist interpretation) credible intervals (use Bayesian interpretation). won’t worry difference book.Let’s look entire posteriors \\(\\beta_1\\) \\(\\beta_2\\).appears affect treatment change people’s attitudes conservative immigration issues. somewhat surprising!can decompose dependent variable, att_end two parts: fitted values residuals. two possible fitted values, one Treated one Control. residuals, always, simply difference outcomes fitted values.smaller spread residuals, better job model explaining outcomes.","code":"\nset.seed(9)\nfit_obj <- stan_glm(att_end ~ treatment - 1, \n                    data = trains, \n                    refresh = 0)\n\nfit_obj## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                  Median MAD_SD\n## treatmentTreated 10.0    0.4  \n## treatmentControl  8.5    0.3  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.8    0.2   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_obj %>% \n  as_tibble() %>% \n  select(-sigma) %>% \n  pivot_longer(cols = treatmentTreated:treatmentControl,\n               names_to = \"parameter\",\n               values_to = \"attitude\") %>% \n  ggplot(aes(x = attitude, color = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Average attitude toward immigration\",\n         x = \"Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()"},{"path":"three-parameters.html","id":"temperance-4","chapter":"8 Three Parameters","heading":"8.3.4 Temperance","text":"\nFIGURE 8.8: Temperance\nRecall first question began section:average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?Recall Chapter 3 discussion average treatment effect. One simple estimator average treatment effect difference \\(\\beta_1\\) \\(\\beta_2\\). , definition \\(\\beta_1\\) average attitude toward immigration, population, anyone, exposure treatment. , \\(\\beta_1 - \\beta_2\\) average treatment effect population, roughly 1.5. However, estimating posterior probability distribution parameter tricky, unless make use posterior distributions \\(\\beta_1\\) \\(\\beta_2\\). information, problem simple:true value average treatment effect much 2 little 1? course! likely value around 1.5, variation data smallness sample cause estimate imprecise. However, quite unlikely true average treatment effect zero.second question:largest effect size still 1 10 chance occurring?answer question, first need create tibble can pass posterior_predict(). variables tibbles passed newdata must exactly counterparts original data. example, treatment must factor, levels trains.Consider result posterior_predict() someone treated someone control group.case, looking distribution treatment effect single individual. different average treatment effect. particular, much variable. looking one row Preceptor Table. single individual, att_end can anywhere 3 15, treatment control. causal effect — difference two potential outcomes can, theory, anywhere -12 +12. extreme values rare, impossible.question, however, interested value 90th percentile. model, 6.78. expect treatment effect magnitude common, , time, effects big bigger occur 10% time.","code":"\nset.seed(14)\nfit_obj %>% \n  as_tibble() %>% \n  mutate(ate = treatmentTreated - treatmentControl) %>% \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Probability Distribution for ATE\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nate_data <- tibble(treatment =\n                     factor(c(\"Treated\", \"Control\"),\n                            levels = c(\"Treated\", \"Control\")))\npp <- posterior_predict(fit_obj, \n                        newdata = ate_data) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)\n\n\nte <- tibble(Treated = pp$`1`, Control = pp$`2`)  %>% \n  mutate(te = Treated - Control) \n\nte %>% \n  ggplot(aes(x = te)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior Probability Distribution for Treatment Effect\",\n         subtitle = \"Causal effects are more variable for indivduals\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"three-parameters.html","id":"conclusion-2","chapter":"8 Three Parameters","heading":"8.4 Conclusion","text":"chapter, explored relationships different variables trains data set. built predictive model causal model using following three parameters: \\(\\beta_1\\), \\(\\beta_2\\), \\(\\sigma\\).Similar previous chapters, first task use Wisdom . judge relevant data questions ask, using ethics guide decision. income data 2012 really suitable making predictions today? Probably? Justice necessary decide best way represent models make. use Courage translate models code. goal understand, generate posterior distributions, interpret meaning. Temperance leads us final stage. remind many unknown factors affect data. models never good hope . always use caution applying models real world.","code":""},{"path":"n-parameters.html","id":"n-parameters","chapter":"9 N Parameters","heading":"9 N Parameters","text":"created models one parameter Chapter 6, two parameters Chapter 7 three parameters Chapter 8, now ready make jump \\(N\\) parameters.chapter, consider models multiple parameters complexities arise therefrom.","code":""},{"path":"n-parameters.html","id":"eda-of-governors","chapter":"9 N Parameters","heading":"9.1 EDA of governors","text":"Packages:use governors data set PPBDS.data package. data set features demographic information candidates governor US. comes paper “Longevity Returns Political Office” Barfort, Klemmensen Larsen (2019), concludes winning gubernatorial election increases candidate’s lifespan.11 variables 1,092 observations. Chapter, looking variables last_name, year, state, sex, alive_post, alive_pre.alive_pre alive_post many days candidate lived election many days candidate lived election, respectively. consequence, politicians already deceased included data set. means handful observations elections last 20 years. candidates time period still alive , therefore, excluded.sex often “Male,” might expect.TABLE 9.1: Data summaryVariable type: characterVariable type: numericskim() groups variables together type. given histograms numeric data. looking histogram year, see skewed right — meaning data bunched left smaller tail right — half observations election years 1945 1962. makes sense logically, looking deceased candidates, candidates recent elections likely still alive.using data set, left-side variable alive_post. trying understand/predict many days candidate live election.Starting relationship alive_post year, can see rough line observations. data points top right portion graph possible run 2011 lived 20,000 days (55 years) election took place. edge data represents, approximately, days candidate possibly lived, still died, given year election. reason data slanted downward maximum value scenario greater earlier years. , candidates ran governor earlier years live long time election still died prior data set creation, giving higher alive_post values ran office recent years.fewer observations later years fewer recent candidates died.plot shows men live much longer, average, women election. make sense ?","code":"\nlibrary(PPBDS.data)\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(rstanarm)\nglimpse(governors)## Rows: 1,092\n## Columns: 11\n## $ state      <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Al…\n## $ year       <int> 1946, 1946, 1950, 1954, 1954, 1958, 1962, 1966, 1966, 1970…\n## $ first_name <chr> \"James\", \"Lyman\", \"Gordon\", \"Tom\", \"James\", \"William\", \"Ge…\n## $ last_name  <chr> \"Folsom\", \"Ward\", \"Persons\", \"Abernethy\", \"Folsom\", \"Longs…\n## $ party      <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Democ…\n## $ sex        <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"M…\n## $ died       <date> 1987-11-21, 1948-12-17, 1965-05-29, 1968-03-07, 1987-11-2…\n## $ status     <chr> \"Challenger\", \"Challenger\", \"Challenger\", \"Challenger\", \"C…\n## $ win_margin <dbl> 77.3, -77.3, 82.2, -46.7, 46.7, -77.5, 100.0, -34.3, 34.3,…\n## $ alive_post <int> 14991, 773, 5319, 4871, 12069, 19924, 13096, 18622, 549, 1…\n## $ alive_pre  <int> 13906, 28690, 17805, 17001, 16828, 12152, 15778, 17597, 14…\nch9_gov <- governors %>% \n  select(last_name, year, state, sex, alive_post, alive_pre)\nsample_n(ch9_gov, 5)## # A tibble: 5 x 6\n##   last_name  year state        sex    alive_post alive_pre\n##   <chr>     <int> <chr>        <chr>       <int>     <int>\n## 1 Ristine    1964 Indiana      Male        16298     16362\n## 2 Sundlun    1986 Rhode Island Male         9024     24397\n## 3 Richards   1990 Texas        Female       5791     20884\n## 4 Turner     1946 Oklahoma     Male         9715     18991\n## 5 Williams   1948 Michigan     Male        14333     13770\nskim(ch9_gov)\nch9_gov %>%\n  ggplot(aes(x = year, y = alive_post)) +\n  geom_point() +\n  labs(title = \"US Gubernatorial Candidate Lifespans\",\n       subtitle = \"Candidates who died more recently can't have lived for long post-election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Year\",\n       y = \"Days Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \nch9_gov %>%\n  ggplot(aes(x = sex, y = alive_post)) +\n  geom_boxplot() +\n  labs(title = \"US Gubernatorial Candidate Lifespans\",\n       subtitle = \"Male candidates live much longer after the election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Gender\",\n       y = \"Days Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() "},{"path":"n-parameters.html","id":"wisdom-5","chapter":"9 N Parameters","heading":"9.2 Wisdom","text":"\nFIGURE 9.1: Wisdom\nconcept “population” subtle important. population set candidates data. data set. population larger — potentially much larger — set individuals want make inferences. parameters models refer population, data set.Consider simple example. Define \\(\\mu\\) average number days lived candidates governor Election Day. Can calculate \\(\\mu\\) data? ! many candidates governor still alive, included data even though part “population” want study. \\(\\mu\\) can calculated. can estimated.Note, also, many different populations, \\(\\mu\\), might interested.population candidates governor US 1945 2012. period covered paper.population candidates governor US 1945 2012. period covered paper.population candidates governor US 1900 2012. priori, expect major difference candidates run 1946 run 1942. different ? therefore unreasonable extend population interest back time, even data earlier periods.population candidates governor US 1900 2012. priori, expect major difference candidates run 1946 run 1942. different ? therefore unreasonable extend population interest back time, even data earlier periods.population candidates governor US 1945 2030. often interested future. want make predictions happen candidates, even candidates yet run office.population candidates governor US 1945 2030. often interested future. want make predictions happen candidates, even candidates yet run office.population candidates governor around world. countries governors also! want understand longevity well.population candidates governor around world. countries governors also! want understand longevity well.population candidates political offices US. might expect candidates Senator similar lifespans candidates Governor.population candidates political offices US. might expect candidates Senator similar lifespans candidates Governor.. many possible populations questions might ask.. many possible populations questions might ask.populations different, different \\(\\mu\\). \\(\\mu\\) interested depends problem trying solve. judgment call, matter Wisdom, whether data “close enough” population interested justify making model.","code":""},{"path":"n-parameters.html","id":"justice-and-courage","chapter":"9 N Parameters","heading":"9.3 Justice and Courage","text":"\nFIGURE 9.2: Justice\n\nFIGURE 9.3: Courage\ngoing series models chapter, useful combine virtues Justice Courage.","code":""},{"path":"n-parameters.html","id":"alive_pre","chapter":"9 N Parameters","heading":"9.3.1 alive_pre","text":"begin, let’s model candidate lifespan election function candidate lifespan prior election. data:math fairly simple:\\[ alive\\_post_i =  \\beta_0 + \\beta_1 alive\\_pre_i + \\epsilon_i \\]\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(alive\\_post_i\\) number days lived election candidate \\(\\). \\(alive\\_pre_i\\) number days lived election candidate \\(\\). \\(\\epsilon_i\\) “error term,” difference actual days-lived candidate \\(\\) modeled days-lived. \\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\). key distinction :Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.\\(\\beta_0\\) “intercept” regression, average value population \\(alive\\_post\\), among \\(alive\\_pre = 0\\). \\(\\beta_1\\) “coefficient” \\(alive\\_pre\\). one day increase \\(alive\\_pre\\) associated \\(\\beta_1\\) change \\(alive\\_post\\). , value population data drawn.three unknown parameters — \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\) — just models used Chapter 8.may recall middle school algebra equation line \\(y = m x + b\\). two parameters: \\(m\\) \\(b\\). intercept \\(b\\) value \\(y\\) \\(x = 0\\). slope coefficient \\(m\\) \\(x\\) increase \\(y\\) every increase one \\(x\\). defining regression line, use slightly different notation fundamental relationship .can use geom_smooth() create fitted regression line:Consider someone 15,000 days old Election Day. score data points candidates around age. Two died soon election. lived 5,000 days election. Others lived around 20,000 days. Variation fills world. fitted line tells us , average, expect candidate age live little less 15,000 days election.model, continuous independent (“predictor”) variable, infinite number fitted values, one possible value alive_pre. different models saw Chapter 8. models two possible fitted values predictor variable took two possible values.can implement model stan_glm().discussed Chapter 8, common term model like “regression.” “regressed” alive_post, dependent variable, alive_pre, () independent variable.parameter values:almost always case, \\(\\sigma\\) nuisance parameter, somethings whose value interested . stan_glm() refers “Auxiliary” parameter.posterior distributions \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (coefficient \\(alive\\_pre_i\\)), hand, important. looking posteriors , let’s examine fitted values:code code used , except replaced geom_smooth() geom_line(). Calling fitted() model returns set fitted values, plotted hand.can create formula fitted values placing median values parameters model:\\[ alive\\_post_i =  26,500 - 0.9 alive\\_pre_i\\]Consider intercept. Since independent variable \\(alive\\_pre_i\\), intercept \\(alive\\_post_i\\) value \\(alive\\_pre_i\\) zero. , interpret intercept average lifespan gubernatorial candidate election, candidate alive zero days prior election., course, substantively nonsense. one runs office day born. next model, explore ways making intercept interpretable. meantime, math math.Consider coefficient \\(alive\\_pre_i\\), \\(\\beta_1\\). median posterior, -0.9, represents slope model. every unit increase independent variable, dependent variable change amount. every additional day candidate alive election, lifespan election 0.9 days lower, average. given number days candidate lived election want estimate long live , multiply days alive prior beta -0.9, combine intercept.descriptive model, causal model. Remember motto Chapter 3: causation without manipulation. way, person \\(\\), change days alive Election Day. day election, X days old. way change . , two () potential outcomes. Without one potential outcome, can causal effect.Given , important monitor language. believe changes alive_pre “cause” changes alive_post. obvious. words phrases — like “associated ” “change ” — close causal. (guilty using just paragraphs ago!) wary use. Always think terms comparisons using predictive model. can’t change alive_pre X Y individual candidate. can compare two candidates (two groups candidates), one alive_pre equal X alive_pre equal Y. model correct, candidates , average, differ alive_post \\(\\beta_1\\) times difference X Y.Let’s look posterior \\(\\beta_1\\), coefficient \\(alive\\_pre_i\\):center variables model’s intercept make substantive sense. center model, pick constant value, usually mean independent variable, subtract constant every value independent variable. Another option pick reasonable reference value, like 18,262 – number days someone 50 years-old lived. Either way, meaning alive_pre changes raw number days candidate alive number days relative reference value.example, want center value alive_pre, independent variable. First, must pick value center . , use mean alive_pre, 52 years old. find value, subtract every alive_pre value. , thereby, changed meaning alive_pre. now means number days candidate alive, Election Day, relative average days alive candidates respective election days.can see intercept decreased slope stayed . interpret model, change definition intercept. Rather intercept representing lifespan candidate alive zero days running governor, now represents post-election lifespan gubernatorial candidate alive mean number days (52 years) running. candidate alive 52 years running governor, expected live 10,300 days (28 years) election, average.Think hard parameters. mean? population represent? ideal Preceptor Table make calculation easy? case, \\(\\beta_0\\) average number days gubernatorial candidates live election day, just subset candidates average age Election Day. ideal Preceptor Table, trivial calculate. Just take average subset! estimation required. , actual Preceptor Table lots missing values. particular, many gubernatorial candidates . . . uh . . . died. (inconsiderate!) , can’t know many days live. can , first, define \\(\\beta_0\\) , second, estimate posterior probability distribution .","code":"\nch9_gov %>% \n  ggplot(aes(x = alive_pre, y = alive_post)) +\n    geom_point() +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Days\",\n         y = \"Days Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\nch9_gov %>% \n  ggplot(aes(x = alive_pre, y = alive_post)) +\n    geom_point() +\n    geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Days\",\n         y = \"Days Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic() \nfit_gov_1 <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ alive_pre,\n                      refresh = 0)\nprint(fit_gov_1, detail = FALSE)##             Median  MAD_SD \n## (Intercept) 26499.9   743.5\n## alive_pre      -0.9     0.0\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4056.0   86.5\nch9_gov %>% \n  ggplot(aes(x = alive_pre, y = alive_post)) +\n    geom_point() +\n    geom_line(aes(y = fitted(fit_gov_1)), color = \"blue\") +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Blue line shows fitted values\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Days\",\n         y = \"Days Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\nfit_gov_1 %>% \n  as_tibble() %>% \n  ggplot(aes(alive_pre)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `alive_pre`\",\n         y = \"Probability\",\n         x = \"Coefficient of `alive_pre`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nch9_gov$alive_pre <- ch9_gov$alive_pre - mean(ch9_gov$alive_pre)\nfit_gov_1.centered <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ alive_pre,\n                      refresh = 0)\n\nprint(fit_gov_1.centered, detail = FALSE)##             Median  MAD_SD \n## (Intercept) 10309.8   122.1\n## alive_pre      -0.9     0.0\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4055.5   87.6"},{"path":"n-parameters.html","id":"sex","chapter":"9 N Parameters","heading":"9.3.2 sex","text":"Let’s now regress alive_post sex see candidates’ post-election lifespans differ sex.Note workflow. Try one model. Interpret . Try another model. another. one “true” model. infinite space possible models. Good data science involves intelligent tour space.regression, use -1 formula make output straightforward, intercept interpret. math model saw Chapter 8:\\[ alive\\_post_i = \\beta_1 x_{f,} + \\beta_2 x_{m,} + \\epsilon_i\\]\n\\[x_{f,}, x_{m,} \\\\{0,1\\}\\] \n\\[x_{f,} +  x_{m,} = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]meanings \\(alive\\_post_i\\) \\(\\epsilon_i\\) first model. Indeed, throughout exercises. \\(x_{f,}\\) \\(x_{m,}\\) 0/1 variables, just like last chapter. variables whose values vary across individuals.important parameters \\(\\beta_1\\) \\(\\beta_2\\). average days-lived post-election , respectively, women men. , “average” data . easy calculate! estimation required. \\(\\beta_1\\) \\(\\beta_2\\) averages entire “population,” however chosen define term. averages can calculated directly. can estimated, creating posterior probability distribution.Looking back regression model just created, see intercept. Instead \\(\\beta_0\\) value, \\(\\beta_1\\) \\(\\beta_2\\) female male. makes things easier interpret. Without add subtract anything intercept, regression tells us , average, women expected live 6,000 days running governor, men expected live 10,000 days.strange result. men live twice long women election? One explanation might women don’t run governor later life, therefore expected live long.Now interpreted model using -1 formula estimate \\(\\beta_1\\) \\(\\beta_2\\), let’s take away -1 regress alive_post intercept sex see equation changes.longer value female. However intercept. regression mathematical formula :\\[ alive\\_post_i = \\beta_0  + \\beta_1 x_{m,} + \\epsilon_i\\]\\(\\beta_0\\) intercept, around 5,850. result similar female value . type model, intercept represents variable represented model. \\(\\beta_1\\) affects outcome candidate male. (candidate female, \\(x_{m,} = 0\\). Therefore, intercept value represents male, .e., females.)candidate male, add coefficient male intercept value, gives us average lifespan male gubernatorial candidate election. can see adding \\(\\beta_0\\) \\(\\beta_1\\), value got males previous model.careful notation! \\(\\beta_1\\) -intercept model different \\(\\beta_1\\) model intercept! Notation varies. must pay attention time make model.posterior distribution \\(\\beta_0 + \\beta_1\\) can constructed via simple addition.interpretation parameter seen . true average, across entire population, number days male candidates live election. can never know true average . , seems highly likely true average somewhere 10,000 10,750 days.","code":"\nfit_gov_2 <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ sex - 1,\n                      refresh = 0)\nprint(fit_gov_2, detail = FALSE)##           Median  MAD_SD \n## sexFemale  5859.3  1046.6\n## sexMale   10393.6   144.0\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4846.1  103.3\nfit_gov_2a <- stan_glm(data = ch9_gov,\n                       formula = alive_post ~ sex,\n                       refresh = 0)\nprint(fit_gov_2a, detail = FALSE)##             Median MAD_SD\n## (Intercept) 5852.4 1021.2\n## sexMale     4550.4 1048.1\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4848.6  109.4\nfit_gov_2a %>% \n  as_tibble() %>% \n  mutate(male_intercept = `(Intercept)` + sexMale) %>% \n  ggplot(aes(male_intercept)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate Days Left`\",\n         y = \"Probability\",\n         x = \"Male Days To Live After the Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"alive_pre-and-sex","chapter":"9 N Parameters","heading":"9.3.3 alive_pre and sex","text":"going transition working model one explanatory variable. outcome variable alive_post, now two different explanatory variables: alive_pre sex. Note sex categorical explanatory variable alive_pre continuous explanatory variable.math using:\\[ alive\\_post_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 alive\\_pre_i + \\epsilon_i \\]outcome variable \\(alive\\_post_i\\), number days person alive election. \\(male_i\\) one explanatory variables. predicting amount days male candidate lives election, value 1. making prediction female candidates, value 0. \\(alive\\_pre_i\\) explanatory variable. number days candidate lived election.outcome variable \\(alive\\_post_i\\), number days person alive election. \\(male_i\\) one explanatory variables. predicting amount days male candidate lives election, value 1. making prediction female candidates, value 0. \\(alive\\_pre_i\\) explanatory variable. number days candidate lived election.\\(\\beta_0\\) average number days lived election women, day election, alive average number days candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(alive\\_post_i\\), \\(male_i = 0\\) \\(alive\\_pre_i = 0\\).\\(\\beta_0\\) average number days lived election women, day election, alive average number days candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(alive\\_post_i\\), \\(male_i = 0\\) \\(alive\\_pre_i = 0\\).\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number days lived election men, day election, alive average number days candidates.\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number days lived election men, day election, alive average number days candidates.\\(\\beta_2\\) , entire population, average difference \\(alive\\_post_i\\) two individuals, one \\(alive\\_pre_i\\) value 1 greater .\\(\\beta_2\\) , entire population, average difference \\(alive\\_post_i\\) two individuals, one \\(alive\\_pre_i\\) value 1 greater .Now understand model, let’s translate following model code.Looking results, can see intercept value around 8,100. (exact values median posterior vary random sampling inherent fitting model.) means average female candidate, alive average number days candidates, live another 8,100 days election.Note sexMale around 2,250. coefficient, \\(\\beta_1\\). need connect value intercept value get something meaningful. Using formula \\(\\beta_0\\) + \\(\\beta_1\\), find number days average male candidate — , day election, alive average number days candidate — live around 10,300,.Now take look coefficient \\(alive\\_pre_i\\), \\(\\beta_2\\). median posterior, -0.8, represents slope model. every unit increase independent variable, dependent variable change coefficient. makes sense value negative. Think : days candidate lived, fewer days candidate left live. , every extra day candidate alive election, lifespan election 0.8 days lower, average.Let’s now look posteriors.graph displays posterior probability distributions \\(\\beta_0\\) $_0 + _1 $. two distributions fairly different; distribution females spread males. greater number men women data. data , precise can .Let’s now take look posterior distribution \\(\\beta_2\\), coefficient \\(alive\\_pre_i\\).","code":"\nfit_gov_3 <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ sex + alive_pre,\n                      refresh = 0)\nprint(fit_gov_3, detail = FALSE)##             Median MAD_SD\n## (Intercept) 8066.3  854.7\n## sexMale     2287.7  860.0\n## alive_pre     -0.8    0.0\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4046.1   86.8\nfit_gov_3 %>% \n  as_tibble() %>% \n  mutate(male_days = `(Intercept)` + sexMale) %>% \n  rename(female_days = `(Intercept)`) %>% \n  select(female_days, male_days) %>% \n  pivot_longer(cols = female_days:male_days, \n               names_to = \"parameters\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n     labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Men live longer\",\n         x = \"Average Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nfit_gov_3 %>% \n  as_tibble() %>% \n  ggplot(aes(alive_pre)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `alive_pre`\",\n         y = \"Probability\",\n         x = \"Coefficient of `alive_pre`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"alive_pre-sex-and-alive_presex","chapter":"9 N Parameters","heading":"9.3.4 alive_pre, sex and alive_pre*sex","text":"Let’s create another model. time, however, numeric outcome variable alive_post function two explanatory variables used , alive_pre sex, interaction.Math:\\[ alive\\_post_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 alive\\_pre_i + \\\\ \\beta_3 male_i *  alive\\_pre_i + \\epsilon_i\\]\noutcome variable still \\(alive\\_post_i\\). want know many days candidate live election.outcome variable still \\(alive\\_post_i\\). want know many days candidate live election.\\(male_i\\) still one explanatory variables. predicting amount days male candidate lives election, 1. making prediction female candidate, 0. \\(alive\\_pre_i\\) still explanatory variable, representing number days candidate lived election, relative average value candidates.\\(male_i\\) still one explanatory variables. predicting amount days male candidate lives election, 1. making prediction female candidate, 0. \\(alive\\_pre_i\\) still explanatory variable, representing number days candidate lived election, relative average value candidates.\\(\\beta_0\\) average number days lived election women, day election, alive average number days candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_0\\) average number days lived election women, day election, alive average number days candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number days lived election men, day election, alive average number days candidates.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number days lived election men, day election, alive average number days candidates.\\(\\beta_2\\) coefficient \\(alive\\_pre_i\\). just slope women. average difference \\(alive\\_post_i\\) two women, one \\(alive\\_pre_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_2\\) coefficient \\(alive\\_pre_i\\). just slope women. average difference \\(alive\\_post_i\\) two women, one \\(alive\\_pre_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.Code:intercept increased. \\(\\beta_0\\) around 6,000. intercept females. still means average number days lived election women 6,000 . sexMale coefficient, \\(\\beta_1\\), refers value must added intercept order get intercept males. calculated, result 10,000. Keep mind, however, values apply \\(alive\\_pre_i = 0\\), , , candidate \\(\\) around 52 years old.coefficient \\(alive\\_pre_i\\), \\(\\beta_2\\), -0.1. mean? slope females. every extra day female candidate alive election, lifespan election 0.1 days lower, average. Now direct attention coefficient sexMale:alive_pre, \\(\\beta_3\\), -.8. value must added coefficient \\(alive\\_pre_i\\) (recall \\(\\beta_2\\) + \\(\\beta_3\\)) order find slope males. two added together, value, slope, -0.9. every extra day male candidate alive election, lifespan election 0.9 days lower, average.Key point: interpretation intercepts apply candidates \\(alive\\_pre_i = 0\\). Candidates 52 years-old different expected number days live. interpretation slope applies everyone. words, relationship \\(alive\\_post_i\\) \\(alive\\_pre_i\\) , regardless gender old .posterior:Male candidates live longer average female candidates. Note, also, average days live election females 6,000 model. previous model, 8,000. difference? interpretation “average” different! previous model, average women. model, average 52 years-old women. different things, hardly surprised different posteriors.Slope posteriors:posterior distribution shows average slope values men women. can see men steeper slope, slope women practically 0! trying forecast number days women live election, may ignore number days already lived. definitely true men. difference?","code":"\nfit_gov_4 <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ sex*alive_pre,\n                      refresh = 0)\nprint(fit_gov_4, detail = FALSE)##                   Median MAD_SD\n## (Intercept)       6010.0 1298.4\n## sexMale           4342.2 1293.9\n## alive_pre           -0.1    0.4\n## sexMale:alive_pre   -0.8    0.4\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 4034.8   87.5\nfit_gov_4 %>% \n  as_tibble() %>% \n  mutate(male_days = `(Intercept)` + sexMale) %>% \n  rename(female_days = `(Intercept)`) %>% \n  select(female_days, male_days) %>% \n  pivot_longer(cols = female_days:male_days, \n               names_to = \"parameters\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Men live longer\",\n         x = \"Average Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nfit_gov_4 %>% \n  as_tibble() %>% \n  mutate(slope_men = alive_pre + `sexMale:alive_pre`) %>% \n  rename(slope_women = alive_pre) %>% \n  select(slope_women, slope_men) %>% \n  pivot_longer(cols = slope_women:slope_men, \n               names_to = \"parameters\",\n               values_to = \"slope\") %>% \n  ggplot(aes(slope, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Men have a steeper slope\",\n         x = \"slope\",\n         y = \"Probability\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic() "},{"path":"n-parameters.html","id":"what-is-beta_0","chapter":"9 N Parameters","heading":"9.3.5 What is \\(\\beta_0\\)?","text":"Attentive readers noticed meaning \\(\\beta_0\\) identical last two models , time, estimate posterior different. can ?Recall math:\\[ alive\\_post_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 alive\\_pre_i + \\epsilon_i \\]\\[ alive\\_post_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 alive\\_pre_i + \\\\ \\beta_3 male_i *  alive\\_pre_i + \\epsilon_i\\]\ncases, \\(\\beta_0\\) meaning. average number days lived election women, day election, alive average number days candidates. Plug 0 \\(male_i\\) 0 \\(alive\\_pre_i\\) equations simplify :\\[ alive\\_post_i =  \\beta_0 + \\epsilon_i \\]\nfar, good. , despite fact using data, posterior probability distributions \\(\\beta_0\\) meaningfully different two models:enough overlap two distributions aren’t completely incompatible. , truth, \\(\\beta_0 = 7000\\), neither posterior distribution “wrong.”meaning parameter, including \\(\\beta_0\\), conditional two assumptions: population seek make inferences structure model. case, assume population interest . models different, albeit one interaction term. enough difference make \\(\\beta_0\\) first model different “thing” \\(\\beta_0\\) second model, even one “interpretation” mean .trick individual interpretation meaning parameter just captures one characteristic meaning parameter. just single view “thing.” view fine, far goes, can’t show us aspects parameter. picture full moon tells us little appears dark side.Consider different question: expected value male candidates 1 year older average:Without interaction term: \\(\\beta_0 + \\beta_1 + \\beta_2 365\\).Without interaction term: \\(\\beta_0 + \\beta_1 + \\beta_2 365\\).interaction term: \\(\\beta_0 + \\beta_1 + \\beta_2 365 + \\beta_3 365\\).interaction term: \\(\\beta_0 + \\beta_1 + \\beta_2 365 + \\beta_3 365\\).shouldn’t surprised different models generate different formulas question. Yet note \\(\\beta_0\\) appears . words, \\(\\beta_0\\) average number days lived election women, day election, alive average number days candidates. also part multitude estimates.way understand wrapped \\(\\beta_0\\) see context full mathematical formula. Since two different models, defined two different formulas, hardly surprising posterior probability distributions \\(beta_0\\) differ, even though one specific interpretation — one many — .advice: Spend less time thinking parameters mean time using posterior_epred() posterior_predict() examine implications models. posteriors real substance model.","code":"\nm_3 <- fit_gov_3 %>% \n  as_tibble() %>%\n  select(`(Intercept)`) %>% \n  rename(`No Interaction Term` = `(Intercept)`)\n\nm_4 <- fit_gov_4 %>% \n  as_tibble() %>%\n  select(`(Intercept)`) %>% \n  rename(`With Interaction Term` = `(Intercept)`)\n\nbind_cols(m_3, m_4) %>% \n  select(`No Interaction Term`, `With Interaction Term`) %>% \n  pivot_longer(cols = `No Interaction Term`:`With Interaction Term`, \n               names_to = \"Model\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = Model)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distributions\",\n         subtitle = \"For intercept in two different models\",\n         x = \"Additional Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"state-alive_pre-sex-and-alive_presex","chapter":"9 N Parameters","heading":"9.3.6 state, alive_pre, sex and alive_pre*sex","text":"Let’s leave aside subtleties parameter interpretation consider one model, one similar one just created. now adding “state,” give us 55 different intercepts!Math:\\[ alive\\_post_i =  \\beta_0 +  \\beta_1 x_{AK,} + \\beta_2 x_{AR,} + ... \\beta_{49} x_{WY,} + \\\\\n\\beta_{50} male_i + \\beta_{51} alive\\_pre_i+ \\beta_{52} male_i *  alive\\_pre_i + \\epsilon_i\\]\\(alive\\_post_i\\) outcome variable. \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)… way \\(\\beta_{49}\\) correspond different parameters. values specific state. order find female intercept (.e. alive_post females) specific state, must add state’s beta value value \\(\\beta_0\\) find intercept alive_post outcome females candidates specific state. example, look model , \\(\\beta_1\\) intercept value Arkansas (note AK subscript). take appropriate value, appears next Arkansas printed model, add value \\(\\beta_0\\).\\(alive\\_post_i\\) outcome variable. \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)… way \\(\\beta_{49}\\) correspond different parameters. values specific state. order find female intercept (.e. alive_post females) specific state, must add state’s beta value value \\(\\beta_0\\) find intercept alive_post outcome females candidates specific state. example, look model , \\(\\beta_1\\) intercept value Arkansas (note AK subscript). take appropriate value, appears next Arkansas printed model, add value \\(\\beta_0\\).\\(\\beta_0\\) intercept one state alone. model printed, see (Intercept) takes alive_post value female candidates Alabama.\\(\\beta_0\\) intercept one state alone. model printed, see (Intercept) takes alive_post value female candidates Alabama.\\(\\beta_{50}\\) coefficient explanatory variable \\(male_i\\). trying find intercepts female candidates, explanatory variable take value 0. However, \\(male_i\\) = 1 trying find intercept values male candidates. Therefore, add \\(\\beta_{50}\\) value female intercept value find value alive_post male candidates. \\(\\beta_{51}\\) coefficient explanatory variable \\(alive\\_pre_i\\). just slope women. \\(\\beta_{52}\\) difficult interpret. However, gains meaning added \\(\\beta_{51}\\), results slope men.\\(\\beta_{50}\\) coefficient explanatory variable \\(male_i\\). trying find intercepts female candidates, explanatory variable take value 0. However, \\(male_i\\) = 1 trying find intercept values male candidates. Therefore, add \\(\\beta_{50}\\) value female intercept value find value alive_post male candidates. \\(\\beta_{51}\\) coefficient explanatory variable \\(alive\\_pre_i\\). just slope women. \\(\\beta_{52}\\) difficult interpret. However, gains meaning added \\(\\beta_{51}\\), results slope men.Note takes awhile run. dealing 55 parameters .(Intercept) refers average number days lived election women, day election, alive average number days female candidates Alabama! ’s mouthful. really different along, however. can see (Intercept) value around 4,900.However, talked women. men? Well, look bottom list, see sexMale value around 4,400. wish find alive_post specific state, must add value intercept, 4,900, also add intercept desired state.\\(\\beta_{51}\\), coefficient value \\(alive\\_pre_i\\), 0. means women slope 0.\\(\\beta_{51}\\) + \\(\\beta_{52}\\), coefficients \\(alive\\_pre_i\\) sexMale:alive_pre, results value around -0.8. value slope men.Posterior distribution female candidates Washington South Dakota.posterior shows female candidates Washington live longer election female candidates South Dakota. Interesting huh? knowledge averages always precise knowledge individuals.","code":"\nfit_gov_5 <- stan_glm(data = ch9_gov,\n                      formula = alive_post ~ state + sex*alive_pre,\n                      refresh = 0,\n                      iter = 10000)\nprint(fit_gov_5, detail = FALSE)##                     Median  MAD_SD \n## (Intercept)          4889.2  1556.5\n## stateAlaska          1129.9  1433.6\n## stateArizona         2943.9  1150.6\n## stateArkansas         547.1  1163.4\n## stateCalifornia      2032.8  1247.5\n## stateColorado        1514.0  1205.3\n## stateConnecticut     1053.6  1230.1\n## stateDelaware        3159.5  1279.1\n## stateFlorida        -1520.2  1357.2\n## stateGeorgia         1814.2  1441.3\n## stateHawaii          1634.0  1561.4\n## stateIdaho           1743.3  1246.4\n## stateIllinois        -127.5  1340.9\n## stateIndiana         1281.0  1244.5\n## stateIowa             974.6  1156.1\n## stateKansas          -113.2  1167.9\n## stateKentucky        1516.4  1276.6\n## stateLouisiana       1460.4  1446.4\n## stateMaine           2150.0  1326.0\n## stateMaryland        1721.7  1250.6\n## stateMassachusetts    128.5  1163.1\n## stateMichigan       -1130.7  1209.9\n## stateMinnesota       1682.3  1179.3\n## stateMississippi      800.5  1327.6\n## stateMissouri         501.1  1295.2\n## stateMontana          877.6  1254.3\n## stateNebraska         836.2  1170.5\n## stateNevada          1565.1  1342.6\n## stateNew Hampshire   1331.7  1168.8\n## stateNew Jersey       379.7  1303.8\n## stateNew Mexico      1050.3  1123.6\n## stateNew York         915.7  1221.4\n## stateNorth Carolina -1407.8  1422.1\n## stateNorth Dakota    2186.8  1232.5\n## stateOhio            2213.0  1173.4\n## stateOklahoma        -533.3  1322.7\n## stateOregon           272.4  1246.6\n## statePennsylvania    1318.2  1263.4\n## stateRhode Island    1978.9  1101.4\n## stateSouth Carolina  2502.1  1362.1\n## stateSouth Dakota   -1823.3  1137.3\n## stateTennessee       -418.9  1266.4\n## stateTexas           1222.2  1141.6\n## stateUtah            1232.9  1241.8\n## stateVermont          587.9  1163.2\n## stateVirginia        2566.3  1399.4\n## stateWashington      2796.2  1277.4\n## stateWest Virginia   2429.5  1286.4\n## stateWisconsin       2487.2  1141.3\n## stateWyoming         -479.2  1262.7\n## sexMale              4379.9  1331.1\n## alive_pre              -0.1     0.4\n## sexMale:alive_pre      -0.8     0.4\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 3960.4   86.5\nfit_gov_5 %>% \n  as_tibble() %>% \n  mutate(Washington_females = `(Intercept)` + stateWashington) %>% \n  mutate (SD_females = `(Intercept)` + `stateSouth Dakota`) %>% \n  select(Washington_females, SD_females) %>% \n  pivot_longer(cols = Washington_females:SD_females, \n               names_to = \"parameters\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"for women that live in Washington and South Dakota\",\n         x = \"Average Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"temperance-5","chapter":"9 N Parameters","heading":"9.4 Temperance","text":"\nFIGURE 9.4: Temperance\nConsider:long two male political candidates — one South Dakota one Washington, 10 years older average candidate — live election? different lifespans ?questions , purposely, less precise ones tackled Chapters 7 8, written conversational style. normal people talk.However, data scientists, job bring precision questions. two commonsense interpretations. First, curious expected values questions. averaged data thousand candidates like , answer ? Second, curious two specific individuals. long live? Averages involve questions parameters. fates individuals require predictions. general claims, violated often firm rules. Yet, highlight key point: expected values less variable individual predictions.calculate expected values, use parameters algebra. forecast individuals, use posterior_predict().","code":""},{"path":"n-parameters.html","id":"expected-values","chapter":"9 N Parameters","heading":"9.4.1 Expected Values","text":"Consider “average” interpretation first. answer begins posterior distributions parameters fit_gov_5.Looking posterior probability distributions , can see candidates Washington expected live longer. much longer? previous chapters, can manipulate distributions , less, way manipulate simple numbers. want know difference two posterior distributions, can simply subtract. , note , many ways, looking difference much easier considering individual posteriors.don’t really need calculate WA using complex terms. already know going subtract SD WA SD include (Intercept), sexMale, . terms common cancel subtract. , can go directly diff equal difference stateWashington stateSouth Dakota.average value difference days--live almost certainly positive, likely value around 4,500. Notice essentially zero chance , average, South Dakota candidates live longer.One annoyance approach requires us everything “hand,” recall mathematical structure model, insert relevant values data individuals want predict. bother! Fortunately, rstanarm provides posterior_epred() function make process simpler.First, create tibble desired input model. case tibble variable named sex contains two observations value “Male.” state variable desired states. alive_pre variable set 10 years worth days.Second, pass fitted model object tibble posterior_epred().name pe stands posterior expectation. different posterior prediction posterior_predict() produces. Transforming resulting object tibble makes later graphing easier. pe, can now reproduce last two plots much easily.","code":"\nfit_gov_5 %>% \n  as_tibble() %>% \n  mutate(WA = `(Intercept)` + stateWashington + sexMale + \n           (`sexMale:alive_pre` + alive_pre) * 10 * 365) %>% \n  mutate(SD = `(Intercept)` + `stateSouth Dakota` + sexMale + \n           (`sexMale:alive_pre` + alive_pre) * 10 * 365) %>% \n  select(WA, SD) %>% \n  pivot_longer(cols = WA:SD, \n               names_to = \"Parameter\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"For 62 years-old male candidates from South Dakota and Washington\",\n         x = \"Expected Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nfit_gov_5 %>% \n  as_tibble() %>% \n  mutate(diff = stateWashington - `stateSouth Dakota`) %>% \n  select(diff) %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Candidates from Washington live longer than those from South Dakota\",\n         x = \"Average Additional Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nnew_obs <- tibble(sex = c(\"Male\", \"Male\"),\n                  state = c(\"South Dakota\", \"Washington\"),\n                  alive_pre = 10 * 365)\npe <- posterior_epred(fit_gov_5, \n                      newdata = new_obs) %>%\n  as_tibble() \npe %>% \n  rename(SD = `1`,\n         WA = `2`) %>% \n  mutate(Difference = WA - SD) %>% \n  pivot_longer(cols = SD:Difference, \n               names_to = \"Parameter\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"For 62 years-old candidates from South Dakota and Washington\",\n         x = \"Expected Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"individual-predictions","chapter":"9 N Parameters","heading":"9.4.2 Individual Predictions","text":", instead, interpret question asking prediction small number individuals, need use posterior_predict().Use posterior_predict() create draws posterior probability distribution prediction cases. posterior_predict() takes two arguments: model simulations run, tibble indicating covariate values individual(s) want predict. case, using fit_gov_5 model tibble one just created . words, inputs posterior_predict() posterior_epred() identical.resulting tibble 2 columns, one person. first column shows posterior predictions candidate South Dakota. second column shows posterior predictions candidate Washington. cases, forecasts depend values covariates. , provide different forecast candidates female ages.need weird mutate_all(.numeric) incantation? reason posterior_epred() returns simple matrix, easy transform tibble. posterior_predict(), hand, returns special sort matrix much harder work . , need little hackery make next steps easier.Let’s look posterior predictive distribution candidate.big overlap predictions individuals, even though almost overlap averages. Random stuff happens individual time. Random stuff cancels take average many individuals. Consider difference posterior predictive distributions two individuals.words, predict candidate Washington live longer South Dakota candidate. much? Well, number unknown parameter. looking posterior , best estimate 4,500 days, roughly 8 years. However, important note possible male South Dakota outlive Washington male. fact, 1 5 chance .Note different move question averages question individuals. cases, likely value , 4,500 days. , average behavior expected value given individual. uncertainty much greater individual prediction. chance true average Washington candidates less South Dakota candidates essentially zero. Yet, individual pair candidates, even slightly surprising South Dakota candidate outlive Washington candidate. Individuals vary. Averages never tell whole story.","code":"\npp <- posterior_predict(fit_gov_5, \n                        newdata = new_obs) %>%\n  as_tibble() %>% \n  mutate_all(as.numeric)\n  \nhead(pp)## # A tibble: 6 x 2\n##     `1`    `2`\n##   <dbl>  <dbl>\n## 1 -820.  5067.\n## 2 3812. 10562.\n## 3 8167.  6399.\n## 4 5112. 13418.\n## 5  664.  8487.\n## 6  991.  8201.\npp %>% \n  rename(SD = `1`,\n         WA = `2`) %>% \n  pivot_longer(cols = SD:WA, \n               names_to = \"State\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = State)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Predictive Distribution\",\n         subtitle = \"For 62 years-old male candidates from South Dakota and Washington\",\n         x = \"Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\npp %>% \n  mutate(diff = `2` - `1`) %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Predictive Distribution\", \n         subtitle = \"How much longer will an individual Washington candidate live?\",\n         x = \"Days\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"expectation-versus-individual-variation","chapter":"9 N Parameters","heading":"9.4.3 Expectation versus individual variation","text":"Let’s compare results posterior_epred() posterior_predict() scenario directly. code shown , think useful look everything together.Expected values vary much less predictions. chart makes easy see. can almost certain true underlying average numbers days Washington candidates live longer South Dakota candidates positive. , two individual candidates, good chance South Dakota candidate live longer. can ignore \\(\\epsilon\\) predicting outcome individuals. estimating expected values long-run averages, \\(\\epsilon\\)’s cancel .","code":"\nnew_obs <- tibble(sex = c(\"Male\", \"Male\"),\n                  state = c(\"South Dakota\", \"Washington\"),\n                  alive_pre = 10 * 365)\n\npe <- posterior_epred(fit_gov_5, \n                      newdata = new_obs) %>%\n  as_tibble() %>% \n  mutate(diff = `2` - `1`)\n\npp <- posterior_predict(fit_gov_5, \n                        newdata = new_obs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  mutate(diff = `2` - `1`)\n\ntibble(Expectation = pe$diff,\n       Prediction = pp$diff) %>% \n  pivot_longer(cols = Expectation:Prediction, \n               names_to = \"State\",\n               values_to = \"days\") %>% \n  ggplot(aes(days, fill = State)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distributions\",\n         subtitle = \"For 62 years-old male candidates from South Dakota and Washington\",\n         x = \"Additional Days Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"testing","chapter":"9 N Parameters","heading":"9.5 Testing","text":"normal statistics book, already discussed concept “testing” extensively. terminology varies field. “Tests,” “testing,” “hypothesis tests,” “tests significance,” “null hypothesis significance testing” refer concept. refer collection approaches NHST, common abbreviation derived initials last phrase. Wikpedia provides overview.view: Amateurs test. Professionals summarize.Consider question expected difference lifespans South Dakato Washington candidates. difference “significant?” Can “reject null hypothesis?” convential answer Yes. Anytime zero outside 95% confidence interval, can declare result significant. ? gain us? already full posterior probability distribution. knowledge. Yes/question throws away much information (almost) ever useful. reason test can summarize providing full posterior probability distribution.arguments apply case “insignificant” results, “\\(p > 0.5\\), can’t”reject\" null hypothesis. Instead expected values, consider case two candidates, one South Dakota one Washington. difference predicted lifespans “significant?” cares!? full posterior probability distribution prediction — also known posterior predictive distribution — graphed . 95% confidence interval includes zero. mean throw away? ! nonsense. Yes, 20% chance South Dakota candidate lives longer, can hardly surprised happens. still think much likely Washington candidate lives longer. Indeed, consider 4--1 odds fair. fact difference “significant” relevance use posterior make decisions.reasoning applies every parameter estimate, every prediction make. Never test — unless boss demands test. Use judgment, make models, summarize knowledge world, use summary make decisions.","code":""},{"path":"n-parameters.html","id":"eda-of-shaming","chapter":"9 N Parameters","heading":"9.6 EDA of shaming","text":"","code":""},{"path":"n-parameters.html","id":"gov-50-stop-here-rest-of-the-chapter-is-a-draft-which-we-will-go-over-in-class.-tutorial-9-only-covers-material-in-first-part-of-the-chapter.","chapter":"9 N Parameters","heading":"9.6.1 GOV 50: STOP HERE! Rest of the chapter is a draft, which we will go over in class. Tutorial 9 only covers material in first part of the chapter.","text":"Imagine running Governor want better job getting voters vote. recently read large-scale experiment showing effect sending voting reminder “shames” citizens vote. considering sending “shaming” voting reminder . happen ? voters show polls? Additionally, day election female citizen randomly selected. probability vote?Consider new data set, shaming, corresponding experiment carried Gerber, Green, Larimer (2008) titled “Social Pressure Voter Turnout: Evidence Large-Scale Field Experiment.” experiment used several hundred thousand registered voters series mailings determine effect social pressure voter turnout.Let’s now another EDA, starting running glimpse().see glimpse() gives us look raw data contained within shaming data set. top output, can see number rows columns, observations variables respectively. see 344,084 observations, row corresponding unique respondent. “Columns: 10” tells us 10 variables within data set. , see cutoff version entire data set variables left rows observations list separated commas, compared tibble output presents variables columns observations rows running horizontally.summary, get idea variables working . Variables particular interest us sex, hh_size, primary_06. variable hh_size tells us size respondent’s household, sex tells us sex respondent, primary_06 tells us whether respondent voted 2006 Primary election.things note exploring data set. may – may – noticed response general_04 variable “Yes.” published article, authors note “registered voters voted November 2004 selected sample” (Gerber, Green, Larimer, 2008). , authors found history sent mailings.also important identify dependent variable meaning. shaming experiment, dependent variable primary_06, variable coded either 0 1 whether respondent voted 2006 primary election. dependent variable authors trying measure effect treatments proportion people vote 2006 general election.voting results years, 2002 2004, less interest us can removed abbreviated data set. addition removing general_04, primary_02, general_02, primary_04, also taking particular interest birth_year, no_of_names within chapter.narrowing set variables looking investigating, find meaningful relationships among . However, yet discussed important variable : treatment. treatment variable factor variable 5 levels, including control. Since curious sending mailings affects voter turnout, treatment variable tell us impact type mailing can make. Let’s start taking broad look different treatments.\nFour types treatments used experiment, voters receiving one four types mailing. mailing treatments carried message, “CIVIC DUTY - VOTE!”first treatment, Civic Duty, also read, “Remember rights responsibilities citizen. Remember vote.” message acted baseline treatments, since carried message similar one displayed mailings.second treatment, Hawthorne, households received mailing told voters studied voting behavior examined public records. adds small amount social pressure households receiving mailing.third treatment, Self, mailing includes recent voting record member household, placing word “Voted” next name fact vote 2004 election blank space next name . mailing, households also told, “intend mail updated chart” voting record household members 2006 primary. emphasizing public nature voting records, type mailing exerts social pressure voting Hawthorne treatment.fourth treatment, Neighbors, provides household members’ voting records, well voting records live nearby. mailing also told recipients, “intend mail updated chart” voted 2006 election entire neighborhood.now, let’s focus subset data. sample just 10,000 rows otherwise stan_glm() takes annoyingly large amount time work. Nothing substantive changes.create variable solo, TRUE voters live alone FALSE . curious see treatment effect, , voters live alone . also focused two “treatments”: Control Neighbors. sake simplification. want know social pressure impacts voting behavior, makes sense look treatment provides social pressure.TABLE 9.2: Data summaryVariable type: characterVariable type: factorVariable type: logicalVariable type: numericLet’s focus observations may relevant analysis. First, note treatment approximately 38,000 respondents. control group, denoted Con, approximately 190 thousand respondents. logical variable solo, see approximately 47 thousand total respondents live alone (TRUE), approximately 296 thousand live households greater 1 (FALSE). may also important note average age respondents 49.8 years standard deviation 14.4 years.get better sense respondents’ information, let’s use sample_n() gather random sample n observations data set.Now table 5 random observations respondents’ information regular table output. taking random samples, may start see patterns within data. notice anything particular variable treatment?One helpful summarizing technique can use skim(). make information contains simpler, looking three variables: primary_06, treatment, sex.TABLE 9.3: Data summaryVariable type: characterVariable type: factorVariable type: numericRunning skim() command gives us summary data set whole, well types variables individual variable summaries. top, see number columns rows within selected data set. given list different types variables, columns, often appear within data skimming. variables separated column type, given individual summaries based type.created models one parameter Chapter 6 two parameters Chapter 7, now ready make jump \\(N\\) parameters. parameters include models, flexible can become. must careful overfitting, making models inaccurate don’t enough data accurately estimate parameters. tension overfitting underfitting central practice data science.","code":"\nglimpse(shaming)## Rows: 344,084\n## Columns: 10\n## $ sex         <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"Fe…\n## $ birth_year  <int> 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 1968, 196…\n## $ primary_02  <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Y…\n## $ general_02  <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"N…\n## $ primary_04  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n## $ general_04  <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"…\n## $ treatment   <fct> Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne, …\n## $ primary_06  <int> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n## $ hh_size     <int> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, …\n## $ no_of_names <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\nshaming %>%\n  count(treatment)## # A tibble: 5 x 2\n##   treatment       n\n##   <fct>       <int>\n## 1 Civic Duty  38218\n## 2 Hawthorne   38204\n## 3 Control    191243\n## 4 Self        38218\n## 5 Neighbors   38201\nset.seed(9)\nch9_sham <- shaming %>% \n  filter(treatment %in% c(\"Control\", \"Neighbors\")) %>% \n  droplevels() %>% \n  mutate(solo = ifelse(hh_size == 1, TRUE, FALSE)) %>% \n  mutate(age = 2006 - birth_year) %>% \n  select(primary_06, treatment, solo, sex, age) %>% \n  sample_n(10000, replace = FALSE)\nch9_sham %>% \n  skim()\nch9_sham %>% \n  sample_n(10)## # A tibble: 10 x 5\n##    primary_06 treatment solo  sex      age\n##         <int> <fct>     <lgl> <chr>  <dbl>\n##  1          0 Control   FALSE Male      52\n##  2          0 Control   FALSE Female    20\n##  3          1 Control   FALSE Female    76\n##  4          0 Control   FALSE Male      39\n##  5          0 Neighbors FALSE Female    35\n##  6          1 Control   TRUE  Male      84\n##  7          0 Control   FALSE Male      61\n##  8          1 Control   FALSE Male      73\n##  9          1 Neighbors FALSE Male      46\n## 10          0 Neighbors FALSE Male      38\nshaming %>% \n  select(primary_06, treatment, sex) %>% \n  skim()"},{"path":"n-parameters.html","id":"wisdom-6","chapter":"9 N Parameters","heading":"9.7 Wisdom","text":"\nFIGURE 9.5: Wisdom\nLet’s consider one important virtues data science: wisdom. map data question. Recall mission increase voter turnout running Governor.investigate , given dataset respondents encouraged vote four treatments. accomplished sending letter citizens voted previous primary election varying degrees social pressure. remainder respondents fall control group, received mailings. dataset offers number details respondent, including age, sex, treatment type, voting outcome.truly want know make citizens vote. One immediate problem dataset , due study population, studying people voted previous primary election. words, someone vote previous primary election, included. large problem, since means can figure make citizens already voted vote. Though can’t sure, reasonable assume easier encourage citizens vote next primary election history recently voting primary elections.mean data unhelpful? course ! four treatments (therefore four different methods encouraging voting), can gain quite bit knowledge. Mostly, know effective way incentivize people history voting vote . also know method persuasion (control) best option. able tell certain methods persuasion work better certain groups people, according factors age, sex, household size. can help tremendously election.said, map question data almost never perfect. data science, often look data, understand limitations, try best make inferences help cause.","code":""},{"path":"n-parameters.html","id":"primary_06-and-treatment-age","chapter":"9 N Parameters","heading":"9.8 primary_06 and (treatment + age)","text":"\nFIGURE 9.6: Justice\n\nFIGURE 9.7: Courage\ngoing series models chapter, useful combine virtues Justice Courage. begin, let’s model primary_06, represents whether citizen voted , age treatment see connection.Let’s look relationship primary voting treatment + age.(Intercept) two key details. First, since Control comes alphabetically Neighbors, Control group baseline comparison. holds similarly age. slope age participants Control group.Second, remember data, (Intercept) mathematical interpretation, practical interpretation. ? slope age starts zero. nonsensical purposes, voter can zero age.Therefore, model shows , within control group, percent voting 0.084 = 8.4%. calculate percent voting Neighbors group? Recall treatmentNeighbors median giving standalone figure group, rather represents offset Control Neighbors groups. find Neighbors value, must add offset original value: 0.084 + 0.079 = .163 = 16.3%. nearly double rate Control group!Let’s turn age median. Begin grouping observations age counting primary_06, gives us counts 1 (yes) 0 () number voting age category.explore relationship visually, let’s create graph. coercing primary_06 character variable closely represents “yes” “” opposed numeric value.interesting takeaways .\n- First, almost every age bracket (90), majority participants vote.\n- spike ages 40 60 illustrates participants exist age bracket.\n- differences voters non-voters narrows greatly age 60.Let’s now look another graph aims show phenomena, also includes formula using lm. clearly shows upward trend voting participants age increases. can also see highest concentrations “Voted” row exist ages 45-50, whereas highest concentrations “Vote” row exist 18-25 30-60 age groups. , see , almost ages, partcipants likely vote vote. illustrated darker concentration dots “Vote” row. slope regression line, however, shows clear picture: older , likely vote.Note median age 0.004. Age therefore positively correlated voting primary election. mean? means , every year participant’s age increases, odds voting primary increases 0.004. Now, might seem like huge difference. However, think like : every decade older participant , odds voting increase .04 = 4%! makes sense considering just learned older citizens likely vote.Now, let’s return voting difference Control Neighbors groups. Let’s model posterior probability distribution rates voting.","code":"\nmodel_3 <- stan_glm(data = ch9_sham, \n                 formula = primary_06 ~ treatment + age, \n                 refresh = 0)\nprint(model_3, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ treatment + age\n##  observations: 10000\n##  predictors:   3\n## ------\n##                    Median MAD_SD\n## (Intercept)        0.084  0.016 \n## treatmentNeighbors 0.079  0.012 \n## age                0.004  0.000 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.457  0.003 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nage <- ch9_sham %>% \n  group_by(age) %>% \n  count(primary_06) \n\nage## # A tibble: 149 x 3\n## # Groups:   age [77]\n##      age primary_06     n\n##    <dbl>      <int> <int>\n##  1    20          0   119\n##  2    20          1    15\n##  3    21          0   120\n##  4    21          1    15\n##  5    22          0   150\n##  6    22          1    24\n##  7    23          0   119\n##  8    23          1    19\n##  9    24          0   102\n## 10    24          1    21\n## # … with 139 more rows\nage %>% \n  mutate(primary_06 = as.character(primary_06)) %>% \n  ggplot(aes(x = age, y = n, color = primary_06)) +\n  geom_point() +\n  labs(\n    title = \"Relationship Between Age and Voting\",\n    subtitle = \"In the 2006 Primary Elections\",\n    x = \"Age\",\n    y = \"Count\"\n  )\nshaming %>% \n  mutate(age = 2006 - birth_year) %>%  \n  ggplot(aes(age, primary_06)) + \n  geom_jitter(alpha = 0.005, height = 0.1) + \n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_y_continuous(breaks = c(0, 1), labels = c(\"Did Not Vote\", \"Voted\")) + \n  labs(title = \"Age and Voting in 2012 Michigan Primary Election\", \n       subtitle = \"Older people are more likely to vote\", \n       x = \"Age\", \n       y = NULL, \n       caption = \"Data from Gerber, Green, and Larimer (2008)\") \n# In progress. Modify x-axis labels *10. \n\nmodel_3 %>% \n  as_tibble() %>% \n  mutate(Neighbors = `(Intercept)` + `treatmentNeighbors`) %>% \n  mutate(Control = `(Intercept)`) %>% \n  select(Neighbors, Control) %>% \n  pivot_longer(cols = Neighbors:Control,\n               names_to = \"parameters\",\n               values_to = \"percent_voting\") %>% \n  ggplot(aes(percent_voting, fill = parameters)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"for Control versus Neighbors voting rates\",\n         x = \"% of group voting\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"primary_06-age-solo-treatment","chapter":"9 N Parameters","heading":"9.9 primary_06 & (age + solo + treatment)","text":"Now analyzed impact various treatments voting behavior, let’s turn three different variables together: sex, solo (living alone), treatment.First, let’s look variables included left hand side. Besides (Intercept), given treatmentNeighbors, age, soloTRUE. mean baseline comparison, (Intercept), ? Look given: treatmentControl soloFALSE. Therefore, baseline comparison participants Control group live single person households. learned , age calculates starting age 0 therefore unhelpful practially. group, percentage voting .085 = 8.5%. figure, break variables respective medians mean.treatmentNeighbors, median 0.078: offset Neighbors treatment 0.078 compared control group. percent voting group 0.085 (Control) + 0.078 (Neighbors) = .163 = 16.3%. illustrates Neighbors treatment positively correlated voting.age, median 0.004: , age represents positively correlated increase voting someone ages. every decade older participant , chance voting increases 4%.soloTRUE, median 0.015: compared participants households exceeding 1 persons, participants live alone likely vote factor 0.015 = 1.5%. Note true baseline group Control participants include analysis Neighbors.","code":"\nmodel_4 <- stan_glm(data = ch9_sham, \n                 formula = primary_06 ~ treatment + age + solo, \n                 refresh = 0)\nprint(model_4, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ treatment + age + solo\n##  observations: 10000\n##  predictors:   4\n## ------\n##                    Median MAD_SD\n## (Intercept)        0.084  0.016 \n## treatmentNeighbors 0.079  0.013 \n## age                0.004  0.000 \n## soloTRUE           0.015  0.013 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.457  0.003 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n# Modify x scale\n\nmodel_4 %>% \n  as_tibble() %>% \n  mutate(SoloTrue = `(Intercept)` + `soloTRUE`) %>% \n  mutate(SoloFalse = `(Intercept)`) %>% \n  select(SoloTrue, SoloFalse) %>% \n  pivot_longer(cols = SoloTrue:SoloFalse,\n               names_to = \"parameters\",\n               values_to = \"percent_voting\") %>% \n  ggplot(aes(percent_voting, fill = parameters)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"for those who live alone versus live with other people\",\n         x = \"% of group voting\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"n-parameters.html","id":"primary_06-age-solo-treatment-interactions","chapter":"9 N Parameters","heading":"9.10 primary_06 & (age, solo, treatment, + interactions)","text":"Since ’ve now studied model three different variables, time look interactions! , look primary_06 function age, solo, treatment, solo*treatment. solo*treatment mean us? means looking solo treatment variables correspond one another.","code":"\nmodel_5 <- stan_glm(data = ch9_sham, \n                 formula = primary_06 ~ age + solo + treatment + solo * treatment, \n                 refresh = 0)\nprint(model_5, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ age + solo + treatment + solo * treatment\n##  observations: 10000\n##  predictors:   5\n## ------\n##                             Median MAD_SD\n## (Intercept)                 0.086  0.016 \n## age                         0.004  0.000 \n## soloTRUE                    0.009  0.014 \n## treatmentNeighbors          0.073  0.013 \n## soloTRUE:treatmentNeighbors 0.040  0.034 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.458  0.003 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"n-parameters.html","id":"temperance-6","chapter":"9 N Parameters","heading":"9.11 Temperance","text":"\nFIGURE 9.8: Temperance\nFinally, let’s remember virtue Temperance. gist temperance : humble inferences, inferences always, certainly, unfortunately going match real world. apply shaming scenario?Prediction uncertainty main culprit. matter hard try, predict future. Though now conclusions shaming impacted voters 2006 primary elections, confidence say worked didn’t work work now.instance, perhaps impact neighbors knowing voting history greater midst pandemic, may locked inside interactions outside immediate proximity. Perhaps opposite true. unknown unknowns accounted models. predict pandemic, can predict change way people vote respond flyers.also issue representitiveness. voters 2006 primary election (already demonstrated willingness vote 2004 primary election) truly represent people voting gubernatorial election?complications must make inferences grain salt. say data science unhelpful! contrary, acknowledging deficits make inferences (actions take ) stronger.","code":""},{"path":"other-outcomes.html","id":"other-outcomes","chapter":"10 Other Outcomes","heading":"10 Other Outcomes","text":"previous chapters outcome variable , less, continuous -overly-different normally distributed. situations, default choice family argument stan_glm(), guassian(), worked well. corresponded situations \\(\\epsilon_i\\) normally distributed mean zero standard deviation \\(\\sigma\\).happens assumption far away reality data?","code":""},{"path":"building-models.html","id":"building-models","chapter":"11 Building Models","heading":"11 Building Models","text":"haste make progress — get way process building, interpreting using models — given shortshrift messy details model building. chapter fills lacunae.chaper 10, like showing logistic earlier, since can appear third exam.","code":""},{"path":"model-selection.html","id":"model-selection","chapter":"12 Model Selection","heading":"12 Model Selection","text":"happens one model might use? happens two different people team prefer different models? choose?","code":""},{"path":"tools.html","id":"tools","chapter":"Tools","heading":"Tools","text":"chapter broken following sections:Working terminalGit, GitHub, RStudioPDFStyle guideHow use RpubsHow get helpHow make table","code":""},{"path":"tools.html","id":"working-with-the-terminal","chapter":"Tools","heading":"Working with the terminal","text":"terminal powerful window allows interact computer’s filesystem directly without touch mouse trackpad! Learning work filesystem important keep work organized quickly allow move files across folders.Now, let’s open Terminal tab left window start learning use Terminal.","code":""},{"path":"tools.html","id":"pwd-working-directory","chapter":"Tools","heading":"pwd: Working directory","text":"first question may working Terminal might : can’t see folders, know ? Well ’s great place start learning Terminal. see current folder , type pwd (preview working directory):, can see ’m currently user folder Yao.","code":""},{"path":"tools.html","id":"ls-seeing-items-in-the-directory","chapter":"Tools","heading":"ls: Seeing items in the directory","text":"Ok, knowing folder ’m help see ’s actually folder? Well, see items current folder, use command ls (long listing):Type ls terminal hit return/enter. see something like :Notice lists exactly items bottom right window! terminal just another way interact computer’s filesystem! Anything can normally mouse/trackpad, like opening folder, can also Terminal.","code":""},{"path":"tools.html","id":"cd-changing-directories","chapter":"Tools","heading":"cd: Changing directories","text":"Speaking opening folders, let’s open folders enter familiar place. open close folders, use cd (change directory). ’ll using cd change Desktop folder, place familiar .change Desktop directory, type cd Desktop/. helpful hint, type first letters folder file name, can hit tab computer auto complete name. Try ! type cd Desk hit tab auto complete name!Now, type ls , can see item Desktop listed!go back previous folder (aka directory ), can type cd .. two periods represent one level . can see hierarchy view Mac:","code":""},{"path":"tools.html","id":"mkdir-and-rmdir-make-and-remove-a-directory","chapter":"Tools","heading":"mkdir and rmdir: Make and remove a directory","text":"Now ’re Desktop folder, let’s get set-stay organized Gov 50. Staying organized critical working many data projects. , using mkdir Gov-50 (make directory) can create folder exclusively Gov 50 like :Now, type ls, can see new folder created! Note used hyphen Gov 50. Terminal can’t recognize spaces unless put \\ , like : mkdir Gov\\ 50. can read programming naming convention .remove folder, use rmdir (remove directory). won’t using right now don’t need remove anything.","code":""},{"path":"tools.html","id":"touch-creating-files","chapter":"Tools","heading":"touch: Creating files","text":"order experiment next commands Terminal, ’ll need test file. , let’s create test.txt file play round . create file, use touch:, course, can see test.txt file created using ls.","code":""},{"path":"tools.html","id":"mv-moving-files","chapter":"Tools","heading":"mv: Moving files","text":"Oh ! created test.txt file, Gov-50 folder, right now ’s desktop. happened created Gov-50 folder using mkdir, forgot open using cd Gov-50/. worries, can move file folder using mv:using mv first thing type mv file want move. next thing location want move . case want move test.txt Gov-50/ type mv test.txt Gov-50/. , can use cd enter Gov-50 folder use ls see test.txt file successfully moved Gov-50 folder:","code":""},{"path":"tools.html","id":"cp-copying-files","chapter":"Tools","heading":"cp: Copying files","text":"Copying files similar moving files Terminal. Using previous example, wanted copy test.txt Gov-50 folder delete original test.txt file, just replace mv cp (copy paste):","code":"cp test.txt Gov-50/"},{"path":"tools.html","id":"rm-removing-files","chapter":"Tools","heading":"rm: Removing files","text":"Ok, last Terminal command book teaching . , ’re done test.txt file. Let’s remove rm (remove):Make sure Gov-50 folder type rm test.txt! Using ls, can see test file now gone.Congrats! now able basic tasks Terminal! want learn Terminal commands, check Sean Kross’s Unix Workbench.","code":""},{"path":"tools.html","id":"git-github-and-rstudio","chapter":"Tools","heading":"Git, GitHub, and RStudio","text":"next section focuses connecting GitHub RStudio using Git. care GitHub? Think Google Drive R code projects! computer blows , GitHub save R work just Google Drive saves Expos paper.","code":""},{"path":"tools.html","id":"installing-git","chapter":"Tools","heading":"Installing Git","text":"first step using GitHub installing Git computer. first, Git already installed computer? check, go Terminal type git --version. already Git, command return git version instlled. get error, can download install git .","code":""},{"path":"tools.html","id":"github-accounts","chapter":"Tools","heading":"GitHub accounts","text":"installing git, ’ll need GitHub account. like Google account Drive. However, one difference GitHub account visible public! , want pick name carefully. professional might sending potential employers link GitHub account near future! Check former Gov 50 students’ GitHub profiles inspiration:Evelyn CaiEvelyn CaiJessica EdwardsJessica EdwardsOnce GitHub account, ready connect Git RStudio account. Type following two commands terminal window link two. Replace \"Name\" name \"@email.com\" email used sign GitHub.","code":"git config --global user.name \"Your Name\"\ngit config --global user.mail \"your@email.com\""},{"path":"tools.html","id":"github-repositories","chapter":"Tools","heading":"GitHub repositories","text":"now ready create GitHub repository (repo). GitHub repo similar Google Drive folder. make first repo, make sure signed go GitHub homepage click green new button left.want choose good name repo add brief description. use productivity. can choose make repo public private, recommend make repo public important world see. keeps public GitHub profile clean professional. , repo probably private. Let’s also add REAME file repo. document can add deatils repo public.now first repo GitHub. next step clone computer start editing syncing using Git. , ’ll need copy link repo use RStudio. , green button friend. Click copy link shown (can use clipboard button right auto-copy ).","code":""},{"path":"tools.html","id":"connecting-github-to-rstudio","chapter":"Tools","heading":"Connecting GitHub to RStudio","text":"Alright! now ready connect productivity repo RStudio! repo link copied, can go back RStudio begin creating new project. can going File New Project:Next, ’ll need go steps create project: Version Control Git paste link GitHub click Create Project.Congrats! ’ve linked productivity repo RStudio.","code":""},{"path":"tools.html","id":"updating-.gitignore","chapter":"Tools","heading":"Updating .gitignore","text":"first time always working new repo updating .gitignore file. can open file bottom right window Files tab. file includes files don’t want uploaded GitHub. can come handy working big datasets files private information. case, want add .Rproj file .gitignore list. can see Git currently wants upload file GitHub:file private project file usually don’t want uploaded GitHub. , .gitignore, ’ll want add *.Rproj * tells computer want keep files ending .Rproj uploaded:Now, save .gitignore file see .Rproj file dissapear Git tab top right window. don’t see changes, click refresh button!","code":""},{"path":"tools.html","id":"commit-and-push","chapter":"Tools","heading":"Commit and Push","text":"Now ’ve updated .gitignore file, want upload new version GitHub. , first select .gitignore file click Commit button Git window:open new window write Commit message. message small note ’re adding/changing repo. case, ’ve updated .gitignore let’s write just :, want press commit. way telling Git “Yes files want upload. ’m committed.” Next, want press push. pushes uploads files GitHub. (can probably guess pull , won’t using yet)Now, go GitHub repo refresh page, can see .gitignore file uploaded commit message:Congrats! just uploaded first file GitHub.One tricky aspect caching Github ID password. likely, type things first push. bad. , , Github needs know , otherwise people mess repos. hundreds commits/pushes term. don’t want type ID/password time! Follow instructions Happy Git GitHub useR, especially Chapter 10.","code":""},{"path":"tools.html","id":"pdf","chapter":"Tools","heading":"PDF","text":"Generating PDF files RStudio easy hard. easy R markdown designed produce files variety output formats, including PDF. hard , RStudio make PDF files, computer set must set [LaTeX](https://en.wikipedia.org/wiki/LaTeX installation. four options:Making PDF files may just “work,” especially using Mac. Give try!Making PDF files may just “work,” especially using Mac. Give try!doesn’t just work, strongly recommend using tinytex R package. Type commands:doesn’t just work, strongly recommend using tinytex R package. Type commands:Restart R everything just work.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can install full LaTeX installation . Good luck! Don’t come us help.can install full LaTeX installation . Good luck! Don’t come us help.","code":"\ninstall.packages('tinytex')\ntinytex::install_tinytex()"},{"path":"tools.html","id":"style-guide","chapter":"Tools","heading":"Style guide","text":"Much material comes Tidyverse Style Guide. take points work submitted violates guidelines. extremis, may go advice, add comment code explaining decision .","code":""},{"path":"tools.html","id":"comments","chapter":"Tools","heading":"Comments","text":"Include comments code. OK easy--understand chunks code comments. code self-explanatory. code merit many, many lines comments, lines code . project, many lines comments lines code.Make comments meaningful. simple description code . best comments descriptions approaches tried considered. (code already tells us .) Good comments often “Dear Diary” quality: “. tried . finally chose thing reasons X, Y Z. work , look approach.” , structure often several lines comments followed several lines code.line comment begin comment symbol (“hash”) followed single space: #. Code comments must separated code one empty line sides. Format code comments neatly. Ctrl-Shift-/ easiest way . Name R code chunks. Spelling matters. Comments constructed sentences, appropriate capitalization punctuation.","code":""},{"path":"tools.html","id":"long-lines","chapter":"Tools","heading":"Long Lines","text":"Limit code 80 characters per line. fits comfortably printed page reasonably sized font. calling functions, can omit argument names common arguments (.e. arguments used almost every invocation function). Short unnamed arguments can also go line function name, even whole function call spans multiple lines.","code":""},{"path":"tools.html","id":"whitespace","chapter":"Tools","heading":"Whitespace","text":"%>% always space , usually followed new line. first step, line indented two spaces. structure makes easier add new steps (rearrange existing steps) harder overlook step.","code":"\n# Good\n\niris %>%\n  group_by(Species) %>%\n  summarize_if(is.numeric, mean) %>%\n  ungroup() %>%\n  gather(measure, value, -Species) %>%\n  arrange(value)\n\n# Bad\n\niris %>% group_by(Species) %>% summarize_all(mean) %>%\nungroup %>% gather(measure, value, -Species) %>%\narrange(value)"},{"path":"tools.html","id":"commas","chapter":"Tools","heading":"Commas","text":"Always put space comma, never , just like regular English.","code":"\n# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]"},{"path":"tools.html","id":"parentheses","chapter":"Tools","heading":"Parentheses","text":"put spaces inside outside parentheses regular function calls.","code":"\n# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )"},{"path":"tools.html","id":"infix-operators","chapter":"Tools","heading":"Infix operators","text":"infix operators (==, +, -, <-, etc.) always surrounded spaces:exceptions, never surrounded spaces:operators high precedence: ::, :::, $, @, [, [[, ^, unary -, unary +,\n:.Single-sided formulas right-hand side single identifier:Note single-sided formulas complex right-hand side need space:help operator","code":"\n# Good\n\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\n\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\n# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx <- 1:10\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx <- 1 : 10\n# Good\n\n~foo\ntribble(~col1, ~col2,\n         \"a\",   \"b\")\n\n# Bad\n\n~ foo\ntribble( ~ col1, ~ col2,\n        \"a\", \"b\")\n# Good\n\n~ .x + .y\n\n# Bad\n\n~.x + .y\n# Good\n\npackage?stats\n?mean\n\n# Bad\n\npackage ? stats\n? mean"},{"path":"tools.html","id":"extra-spaces","chapter":"Tools","heading":"Extra spaces","text":"may add extra spaces improves alignment = <-.add extra spaces places space usually allowed.","code":"\nlist(total = a + b + c,\n     mean = (a + b + c) / n)"},{"path":"tools.html","id":"messageswarningserrors","chapter":"Tools","heading":"Messages/Warnings/Errors","text":"R messages/warnings/errors never appear submitted document. right way deal issues find cause fix underlying problem. Students sometimes use “hacks” make messages/warnings/errors disappear. common hacks involve using code chunk options like message=FALSE, warning=FALSE, results=\"hide\", include=FALSE others. Don’t , general. message/warning/error worth understanding fixing. Don’t close eyes (metaphorically) pretend problem doesn’t exist. situations, however, , matter try, can’t fix problem. cases, can use one hacks, must make code comment directly , explaining situation. exception “setup” chunk (included default every new Rmd) comes include=FALSE. chunk, explanation necessary, convention.","code":""},{"path":"tools.html","id":"graphics","chapter":"Tools","heading":"Graphics","text":"Use captions, titles, axis labels make clear tables graphics mean.Anytime make graphic without title (explaining graphic ), subtitle (highlighting key conclusion draw), caption (information source data) axis labels (information variables), justify decision code comment. (try ) always include four situations makes less sense. Ultimately, decisions , need understand reasoning.Use best judgment. example, sometimes axis labels unnecessary. Read Data Visualization: practical introduction Kieran Healy guidance making high quality graphics.","code":""},{"path":"tools.html","id":"how-to-use-rpubs","chapter":"Tools","heading":"How to use Rpubs","text":"Rpubs provides free hosting service R work. use :Begin creating new repository GitHub. clone computer. calling repository “rpubs_example.” , put *Rproj .gitignore file. prevent private project file uploaded GitHub.Start new R Markdown file. Go File –> New File –> R Markdown. simplicity, leave name “Untitled” hit “OK.”Save file, , “Untitled” project directory.Knit. see following.Notice blue icon upper right-hand corner reads “Publish.” Click .asked whether want publish RPubs RStudio Connect. Choose RPubs. get reminder documents publish RPubs publicly visible. Click “Publish.”take RPubs website. need create account. Follow steps prompted.Add document details. Name document. Add meaningful slug – otherwise end ugly, long address didn’t choose can’t remember. can leave Description blank simplicity exercise.Hit “Continue,” et voilá! published first document Rpubs!one important step. “rsconnect” contains files specific computer want push GitHub. Therefore, .Rproj files , want add rsconnect folder .gitignore file. Click .gitignore, add hit “Save.” see disappear GitHub top right window. don’t see changes, hit Refresh button top right corner. Since ’ve updated .gitignore file, now good time commit push changes GitHub repository.","code":""},{"path":"tools.html","id":"how-to-get-help","chapter":"Tools","heading":"How to get help","text":"best data science superpower knowing ask question. – Mara Averick","code":""},{"path":"tools.html","id":"searching-for-help-with-r","chapter":"Tools","heading":"Searching for Help with R","text":"Google best friend. question something R, someone probably question someone else probably answered online. Stack Overflow RStudio Community two best forums finding asking questions/solutions. Adding “R” /“tidyverse” keyword search helps find relevant results exact question. specific possible wording question!","code":""},{"path":"tools.html","id":"reproducible-examples","chapter":"Tools","heading":"Reproducible Examples","text":"don’t find answer question ’re still stuck, ask question forums! order get best response, sharing reproducible example community allows others easily start left . reprex package allows easily.First, install reprex package. , load package.Let’s now look problematic code. data set “murders” package “dslabs” provides murder statistics, well population counts, states. Suppose want calculate rate murders state per 100k residents (number murders/population * 10^6). However, code misspelled “population,” resulting error:use reprex, highlight code necessary packages. copy highlighted code pressing Ctrl/Cmd + c. code now clipboard. Console, type reprex() hit Enter/Return. reprex automatically creates reproducible example places clipboard.Now now go favorite R forum paste reproducible example question! example post Slack.post RStudio community.","code":"\ninstall.packages(\"reprex\")\nlibrary(reprex)"},{"path":"tools.html","id":"how-to-make-a-table","chapter":"Tools","heading":"How to make a table","text":"gt R package creating elegant tables. First, ’ll create gt summary table observations data. Second, ’ll run regression display outcome using gtsummary, companion package gt specializes presenting results statistical models.want learn gt check fantastic guide. Go official gt package website. See extensive guide gtsummary.Load necessary libraries.set message=FALSE code chunk avoid showing ugly notes libraries loaded.Let’s pull data use table:Create simplest table gt(), key command: Now let’s make professional. gt offers variety functions add features like these6:can add title subtitle using tab_header(): default, titles text can formatted. want formatting, must wrap character string call md(), md stands (M)ark(d). example, bolded title. can change column names using cols_label(): Use tab_source_note() cite source data create caption. function exclusively providing source — though ’s handy way — can used display text ’d like: Using md() , can italicize name Enos study caption: Now table structure looks good, want format numbers . Let’s add dollar signs income column using fmt_currency(). function also adds commas (want commas without dollar signs use fmt_number()). vars() within fmt_currency() denotes variable formatted currency: Note line return title “Intergroup” “Contact” effect break title displayed md().","code":"\nlibrary(tidyverse)\nlibrary(PPBDS.data)\nlibrary(gt)\nx <- trains %>%\n  select(gender, income, att_end) %>%\n  slice(1:5)\nx## # A tibble: 5 x 3\n##   gender income att_end\n##   <chr>   <dbl>   <dbl>\n## 1 Female 135000      11\n## 2 Female 105000      10\n## 3 Male   135000       5\n## 4 Male   300000      11\n## 5 Male   135000       5\nx %>% \n  gt()\nx %>% \n  gt() %>%\n   tab_header(title = \"Enos Data Observations\", \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx %>% \n  gt()%>%\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") %>% \n  tab_source_note(\"Source: Ryan Enos\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") %>% \n  tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup Contact on Exclusionary Attitudes*\"))\nx %>% \n  gt() %>%\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\")%>%\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\") %>% \n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                       Contact on Exclusionary Attitudes*\")) %>%\n    fmt_currency(columns = vars(income), \n                 decimals = 0) "},{"path":"tools.html","id":"regression-tables","chapter":"Tools","heading":"Regression tables","text":"can making gt table stan_glm() regression object. Key gtsummary package tbl_regression() function.\n          1\n          \n           \n          CI = Confidence Interval\n           ","code":"\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\nfit2 <- stan_glm(att_end ~ party, data = trains, refresh = 0)\n\ntbl_regression(fit2, intercept = TRUE) %>%\n  as_gt() %>%\n    tab_header(title = \"Regression of Attitudes about Immigration\", \n               subtitle = \"The Effect of Party on End Attitude\") %>%\n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                        Contact on Exclusionary Attitudes*\"))"},{"path":"tools.html","id":"references","chapter":"Tools","heading":"References","text":"","code":""},{"path":"shiny.html","id":"shiny","chapter":"Shiny","heading":"Shiny","text":"","code":""},{"path":"shiny.html","id":"helpful-resources","chapter":"Shiny","heading":"Helpful Resources","text":"number resources Shiny get started, definitely take look following materials addition using guide:Shiny Video Tutorials comprehensive video tutorial goes basics building Shiny app adding customization. 2 half hours long, individual chapters can found link walk throughs smaller topics. first 40 minutes tutorial particularly helpful getting basic app running available shinyapps.io.Shiny Video Tutorials comprehensive video tutorial goes basics building Shiny app adding customization. 2 half hours long, individual chapters can found link walk throughs smaller topics. first 40 minutes tutorial particularly helpful getting basic app running available shinyapps.io.Shiny Written Tutorials also quite comprehensive, walks templates example Shiny document. breaks set UI, control widgets, reactive output (like dropdown variables), using R scripts data, using reactive expressions, sharing app.Shiny Written Tutorials also quite comprehensive, walks templates example Shiny document. breaks set UI, control widgets, reactive output (like dropdown variables), using R scripts data, using reactive expressions, sharing app.Mastering Shiny Hadley Wickham really nice guide using Shiny best practices higher-quality apps, including reducing code duplication.Mastering Shiny Hadley Wickham really nice guide using Shiny best practices higher-quality apps, including reducing code duplication.","code":""},{"path":"shiny.html","id":"set-up-and-getting-started","chapter":"Shiny","heading":"Set Up and Getting Started","text":"Sign Shiny account https://www.shinyapps.io/.Sign Shiny account https://www.shinyapps.io/.Install Shiny package : install.packages(“shiny”)Install Shiny package : install.packages(“shiny”)Create Shiny app RStudio : File > New File > Shiny Web App…Create Shiny app RStudio : File > New File > Shiny Web App…popup, choose Single File (app.R) - doesn’t particularly matter, way Preceptor taught class, way example set . prefer two separate files ui server, also valid might allow cleaner code.popup, choose Single File (app.R) - doesn’t particularly matter, way Preceptor taught class, way example set . prefer two separate files ui server, also valid might allow cleaner code.now functioning example Shiny App - click Run App use keyboard shortcut Cmd/Ctrl + Shift + Enter see app action! can view new window, external web browser (see dropdown arrow next Run App options). Notice example includes slider allows viewer change number bins histogram .now functioning example Shiny App - click Run App use keyboard shortcut Cmd/Ctrl + Shift + Enter see app action! can view new window, external web browser (see dropdown arrow next Run App options). Notice example includes slider allows viewer change number bins histogram .","code":""},{"path":"shiny.html","id":"building-your-basic-app","chapter":"Shiny","heading":"Building Your Basic App","text":"Now functioning Shiny App running, let’s take closer look files directories. Notice app.R file created within directory. important remember files within directory accessible app online. Taking look app.R file, can see four necessary elements file create working app:First, calls library(shiny), defines user interface ui <- fluidPage(…)also defines server logic, takes input UI, produces output based input, defined server <- function(input, output) {…}Finally, calls shinyApp(ui, server) run app.","code":""},{"path":"shiny.html","id":"setting-up-the-basic-ui","chapter":"Shiny","heading":"Setting Up the Basic UI","text":"According Preceptor, Gov 1005 Final Projects formatted three tabs, well embedded video. basic set might something like :can run example app repo see code action. Breaking code , created navigation bar page three tabs. first tab display model, two potential input options, Option Option B. can customized based model, number inputs desired. Note wrapped inside fluidPage(), simply creates basic page layout can support rows columns desired. Breaking tab , sidebarLayout() defining layout page sidebar panel main panel. Within sidebar panel, inserted selectInput, allow us reactive pages.Note used HTML formatting Discussion pages.","code":"ui <- navbarPage(\n  \"Final Project Title\",\n  tabPanel(\"Model\",\n           fluidPage(\n              titlePanel(\"Model Title\"),\n              sidebarLayout(\n                  sidebarPanel(\n                      selectInput(\n                          \"plot_type\",\n                          \"Plot Type\",\n                          c(\"Option A\" = \"a\", \"Option B\" = \"b\")\n                      )),\n                  mainPanel(plotOutput(\"line_plot\")))\n             )),\n  tabPanel(\"Discussion\",\n           titlePanel(\"Discussion Title\"),\n           p(\"Tour of the modeling choices you made and \n              an explanation of why you made them\")),\n  tabPanel(\"About\", \n          titlePanel(\"About\"),\n          h3(\"Project Background and Motivations\"),\n          p(\"Hello, this is where I talk about my project.\"),\n          h3(\"About Me\"),\n          p(\"My name is ______ and I study ______. \n             You can reach me at ______@college.harvard.edu.\")))"},{"path":"shiny.html","id":"setting-up-the-server","chapter":"Shiny","heading":"Setting up the Server","text":"server function perform backend logic app. can put R code create plots tables needed. inputs come UI definitions, like selectInput() sliderInput(). UI example, used select input named “plot_type,” two options: B. outputs defined within server function, rendered defined UI. UI example, want render plotOutput named “line_plot.”server function ideally use plot_type input create line_plot. example basic server function might something like :","code":"server <- function(input, output) {\n    output$line_plot <- renderPlot({\n        # Generate type based on input$plot_type from ui\n        \n        ifelse(\n            input$plot_type == \"a\",\n            \n            # If input$plot_type is \"a\", plot histogram of \"waiting\" column \n            # from the faithful dataframe\n            \n            x   <- faithful[, 2],\n            \n            # If input$plot_type is \"b\", plot histogram of \"eruptions\" column\n            # from the faithful dataframe\n            \n            x   <- faithful[, 1]\n        )\n        \n        # Draw the histogram with the specified number of bins\n        \n        hist(x, col = 'darkgray', border = 'white')\n    })\n}"},{"path":"shiny.html","id":"organization","chapter":"Shiny","heading":"Organization","text":"Now basic app running, let’s talk organization. One common problem shows lot people tend throw code app.R file, causing grow hundreds even thousands lines code long. happens easily can backend logic written server function, additionally entire HTML markdown pages can formatted UI section. ways stop app.R file becoming incredibly messy.First, can put large blocks text separate files. example, take HTML formatting “” tab section put separate .html file, similarly format section using markdown. simply use shiny function includeHTML() includeMarkdown() insert file contents.Second, put much prep work possible separate (multiple separate) R file. helpful readability, generate small files, also helpful app run heavy prep calculations ’s trying load. , tip perform heavy operations document titled prep-shiny.R, example, save helpful outputs graphic images gifs, rds files can loaded app.R file.Third, keep repository clean. save unneccessary raw data files like repository, may just create clutter. Make sure update .gitignore file anything like Github repository Shiny App. includes .Rproj file, raw data files, cache files.","code":""},{"path":"shiny.html","id":"customizations","chapter":"Shiny","heading":"Customizations","text":"Finally, can take look simple way customize look Shiny App. package shinythemes can easily added give app theme. shinythemes website displays various themes quite nicely, can use theme selector click different themes.Shiny gallery helpful browsing different layouts types visualization inspiration.","code":""},{"path":"maps.html","id":"maps","chapter":"Maps","heading":"Maps","text":"order make maps, first need data.7","code":""},{"path":"maps.html","id":"tidycensus","chapter":"Maps","heading":"Tidycensus","text":"tidycensus package great way load interesting data mapping. Note use package, ’ll need Census API key. can request key . key, ’ll need following two commands use package:\"API KEY\" replaced API key.8How can retrieve state’s population 2010 Census? get data decennial census, can use function get_decennial().9 takes three arguments get data want:geography determines unit analysis. , use “state,” many geographies use, “us” entire country, “county” counties, .variables selects Census variables want. Unfortunately, rather opaque names. didn’t know “P001001” variable name population, couple options:\nUse load_variables() function tidycensus generate tibble variable names (described ).\nSearch data.census.gov variable want download results search; variable name .csv file.\nUse load_variables() function tidycensus generate tibble variable names (described ).Search data.census.gov variable want download results search; variable name .csv file.year year. get_decennial() can obtain data 1990, 2000, 2010 Census.output tibble four columns:GEOID part FIPS code, short Federal Information Processing Standard. ’s standardized way identify states, counties, census tracts, etc. instance ’s two digits wide. specific get Census boundaries, longer number gets.NAME generic name get_decennial() gives unit selected geography; , state names. Note 52 observations; “state” geography includes D.C. Puerto Rico along 50 states.variable name variable selected.value value variable selected (, population).wanted select one variable? default, get_decennial() stack variables top , identifying variable column. let’s say wanted know proportion population state lives rural areas. select two variables (total population rural population) receive tibble 104 observations, state appearing per variable. may helpful way receive data, depending purposes. (faceting, may want data long format like , ’ll see .) can request data wide format instead using option output = \"wide\"., created tibble states rows total population (“P001001”) rural population (“P002005”) columns. plot proportion state’s population lives rural areas now simple application tidyverse functions know love. First, let’s create variable rural population proportion order states variable:Next, let’s plot using ggplot():kind plot great want visualize states rural states least. see immediately Maine Vermont rural D.C. entirely urban (one expect, given city). wanted sense proportion rural residents varied geographically? figure plot like one know something U.S. geography. much easier, however, see information directly map United States. R makes easy .","code":"\nlibrary(tidycensus)\n\ncensus_api_key(\"API KEY\")\nlibrary(tidyverse)\n\npop <- get_decennial(geography = \"state\",\n                     variables = \"P001001\",\n                     year = 2010)\n\nglimpse(pop)## Rows: 52\n## Columns: 4\n## $ GEOID    <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\", …\n## $ NAME     <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"L…\n## $ variable <chr> \"P001001\", \"P001001\", \"P001001\", \"P001001\", \"P001001\", \"P001…\n## $ value    <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e+0…\nrural <- get_decennial(geography = \"state\",\n                       variables = c(\"P001001\", \"P002005\"),\n                       year = 2010,\n                       output = \"wide\")\nglimpse(rural)## Rows: 52\n## Columns: 4\n## $ GEOID   <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\", \"…\n## $ NAME    <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Lo…\n## $ P001001 <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e+06…\n## $ P002005 <dbl> 1957932, 241338, 651358, 1278329, 1880350, 1215567, 1806024, …\nrural <- rural %>%\n  rename(state = NAME) %>%\n  mutate(prop_rural = P002005/P001001,\n         state = reorder(state, prop_rural))\nrural %>%\n  ggplot(aes(x = prop_rural, y = state)) +\n  geom_point() +\n  ylab(\"\") +\n  xlab(\"Rural Population Proportion\")"},{"path":"maps.html","id":"conceptual-introduction-to-mapping","chapter":"Maps","heading":"Conceptual introduction to mapping","text":"two underlying important pieces information spatial data: coordinates object coordinates relate physical location Earth, also known coordinate reference system CRS.coordinates familiar geography. CRS uses three-dimensional model earth define specific locations surface grid. object can defined relation longitude (East/West) latitude (North/South).gets complicated attempting create projection. projection translation three-dimensional grid onto two-dimensional plane. animation demonstrates process.Thus, CRS determines geometric object look displayed two-dimensional screen. see, usually don’t need select CRS working tidycensus, good know concept ever work spatial data.","code":""},{"path":"maps.html","id":"vector-versus-spatial-data","chapter":"Maps","heading":"Vector versus spatial data","text":"Spatial data defined CRS can either vector raster data. Vector data based points can connected form lines polygons. located within coordinate reference system. example road map.Raster data, however, values within grid system, satellite imagery. book, dealing vector data, format get data tidycensus package.","code":""},{"path":"maps.html","id":"sf-vs-sp","chapter":"Maps","heading":"sf vs sp","text":"older package, sp, lets user handle vector raster data. book focus vector data sf package. main differences sp sf packages store CRS information. sp uses spatial sub classes, sf stores data data frames, allowing interact dplyr methods ’ve learned far.","code":""},{"path":"maps.html","id":"shapefiles","chapter":"Maps","heading":"Shapefiles","text":"R can handle importing different kinds file formats spatial data, including KML geojson. ’ll focus shapefiles, created Esri 1990s. Though refer “shapefile” singular, ’s actually collection least three basic files:.shp - lists shape vertices.shx - index offsets.dbf - relationship file geometry attributes (data)files must present directory named (except file extension) import correctly. Thankfully, tidycensus grab geometric information Census shapefile .","code":""},{"path":"maps.html","id":"mapping-with-tidycensus-and-geom_sf","chapter":"Maps","heading":"Mapping with tidycensus and geom_sf()","text":"order start mapping R, need get little data tidycensus package. can use get_decennial() function , time adding argument geometry = TRUE.just like tibble , except now funky “multipolygon” called geometry.10 geometry column contains information ggplot() needs create map.order create map using ggplot(), need new geom: geom_sf(). works much like geoms seen , geom_point() geom_line(), except works spatial data. Let’s see happens run geom_sf() arguments:Well, ’s interesting. boundaries state, including Alaska Hawaii. problems. ggplot2 best fit everything one image, taxing system. can’t see particular state well, map zoomed far . Also, colors didn’t fill data., let’s create new map geom_sf() fill prop_rural. ’ll filter Alaska, Hawaii, Puerto Rico now.Now something usable! already lot ’d want map—notably, states shaded based variable interest, helping us see patterns data. use bit makeover, ’ll give next section.","code":"\nrural <- get_decennial(geography = \"state\",\n                       variables = c(\"P001001\", \"P002005\"),\n                       year = 2010,\n                       output = \"wide\",\n                       geometry = TRUE) %>%\n  rename(state = NAME) %>%\n  mutate(prop_rural = P002005/P001001,\n         state = reorder(state, prop_rural))\n\nglimpse(rural)## Rows: 52\n## Columns: 6\n## $ GEOID      <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\"…\n## $ state      <fct> Alabama, Alaska, Arizona, Arkansas, California, Louisiana,…\n## $ P001001    <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e…\n## $ P002005    <dbl> 1957932, 241338, 651358, 1278329, 1880350, 1215567, 180602…\n## $ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-85 31, -85..., MULTIPOLYGON …\n## $ prop_rural <dbl> 0.410, 0.340, 0.102, 0.438, 0.050, 0.268, 0.416, 0.138, 0.…\nrural %>%\n  ggplot() +\n  geom_sf()\nrural %>%\n  filter(! state %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")) %>%\n  ggplot(aes(fill = prop_rural)) +\n  geom_sf()"},{"path":"maps.html","id":"making-maps-pretty","chapter":"Maps","heading":"Making maps pretty","text":"ways can aesthetically improve map:Make fill colors easier distinguishMake darker colors map onto higher values prop_ruralRemove gray backgroundGive legend informative title add title captionA great function providing fill colors maps scale_fill_viridis_c(). different color palettes can selected option argument, easily distinguishable displayed black white people common forms colorblindness. can also reverse default order colors direction = -1 option. function continuous variables prop_rural; discrete variable, can use analogous scale_fill_viridis_d().’ll also use theme_void(), great theme maps gets rid gray background. Finally, ’ll use labs() give legend title “Percent Rural” (multiply values variable 100) add overall title caption.map, clear rural states concentrated Great Plains, South, parts New England, (South)west Northeast less rural.","code":"\nrural %>%\n  filter(! state %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")) %>%\n  ggplot(aes(fill = prop_rural * 100)) +\n  geom_sf() + \n  scale_fill_viridis_c(option = \"plasma\",\n                       direction = -1) +\n  labs(title = \"Rural geography of the United States\",\n       caption = \"Source: Census 2010\",\n       fill = \"Percent Rural\") +\n  theme_void()"},{"path":"maps.html","id":"adding-back-alaska-and-hawaii","chapter":"Maps","heading":"Adding back Alaska and Hawaii","text":"Alaska Hawaii? want display map without zoom , can take advantage argument get_decennial(), shift_geo = TRUE:Now, Alaska Hawaii can displayed near lower 48 states. Note option removes Puerto Rico tibble altogether, good option want show data Puerto Rico.","code":"\nrural_shifted <- get_decennial(geography = \"state\",\n                               variables = c(\"P001001\", \"P002005\"),\n                               year = 2010,\n                               output = \"wide\",\n                               geometry = TRUE,\n                               shift_geo = TRUE) %>%\n  rename(state = NAME) %>%\n  mutate(prop_rural = P002005/P001001,\n         state = reorder(state, prop_rural))\nrural_shifted %>%\n  ggplot(aes(fill = prop_rural * 100)) +\n  geom_sf() + \n  scale_fill_viridis_c(option = \"plasma\",\n                       direction = -1) +\n  labs(title = \"Rural geography of the United States\",\n       caption = \"Source: Census 2010\",\n       fill = \"Percent Rural\") +\n  theme_void()"},{"path":"maps.html","id":"faceting-maps","chapter":"Maps","heading":"Faceting maps","text":"powerful tool ggplot2 use maps faceting. Let’s grab data ACS population Harris County, Texas census tracts race:code similar ’ve used , except retrieving data American Community Survey using get_acs() instead decennial census. new features worth pointing :year get_acs() last year five year sample. Thus, data 2014–2018. can choose years 2009–2018.Since geography “tract,” specifying state county.obtaining data long format, makes faceting easier.added summary_var, “B02001_001,” total population. ’ll see, appears separate column, helpful us. (exercise, try going back code created rural see long format summary_var.)Let’s take look harris:similar ’ve seen . Note now moe summary_moe columns, stand “margin error.” , unlike decennial census, ACS survey thus values get estimates true value.11","code":"\nracevars <- c(White = \"B02001_002\", \n              Black = \"B02001_003\", \n              Asian = \"B02001_005\",\n              Hispanic = \"B03003_003\")\nharris <- get_acs(geography = \"tract\",\n                  variables = racevars, \n                  year = 2018,\n                  state = \"TX\",\n                  county = \"Harris County\",\n                  geometry = TRUE,\n                  summary_var = \"B02001_001\") \nglimpse(harris)## Rows: 3,144\n## Columns: 8\n## $ GEOID       <chr> \"48201100000\", \"48201100000\", \"48201100000\", \"48201100000…\n## $ NAME        <chr> \"Census Tract 1000, Harris County, Texas\", \"Census Tract …\n## $ variable    <chr> \"White\", \"Black\", \"Asian\", \"Hispanic\", \"White\", \"Black\", …\n## $ estimate    <dbl> 3426, 1045, 230, 892, 2936, 3591, 7, 2119, 2973, 885, 0, …\n## $ moe         <dbl> 390, 308, 106, 241, 1358, 2196, 14, 1013, 430, 242, 13, 4…\n## $ summary_est <dbl> 5063, 5063, 5063, 5063, 6820, 6820, 6820, 6820, 4403, 440…\n## $ summary_moe <dbl> 478, 478, 478, 478, 3685, 3685, 3685, 3685, 502, 502, 502…\n## $ geometry    <MULTIPOLYGON [°]> MULTIPOLYGON (((-95 30, -95..., MULTIPOLYGON…"},{"path":"maps.html","id":"transforming-and-mapping-the-data","chapter":"Maps","heading":"Transforming and mapping the data","text":"Now can use facet_wrap() look race variables side--side:Note easy create percentages using summary_est. also used color = Percent scale_color_viridis_c() avoid annoying borders around census tracts. Otherwise, doesn’t differ much code , yet much easier make comparisons across variables. Faceting powerful tool use maps.","code":"\nharris %>%\n  mutate(Percent = 100 * (estimate / summary_est)) %>%\n  ggplot(aes(fill = Percent, color = Percent)) +\n  facet_wrap(~ variable) +\n  geom_sf() +\n  scale_fill_viridis_c(direction = -1) +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"Racial geography of Harris County, Texas\",\n       caption = \"Source: American Community Survey 2014-2018\") +\n  theme_void()"},{"path":"maps.html","id":"want-to-explore-further","chapter":"Maps","heading":"Want to explore further?","text":"Take look tidycensus website.shapefiles place tidycensus, can read using st_read() sf package, join data using dplyr functions, map geom_sf() shown .\nmay look using coord_sf() trouble displaying data.\nmay look using coord_sf() trouble displaying data.Want add interactivity maps? Check leaflet package. ’s good introduction using leaflet tidycensus.Practice skills Andrew Tran’s case study slides, can replicate graphic Washington Post. Note: involves packages haven’t shown book, follow along step step able see used.","code":""},{"path":"animation.html","id":"animation","chapter":"Animation","heading":"Animation","text":"gganimate package creating animated ggplots. provides range new functionality can added plot object order customize change time.Key features gganimate:transitions: want data changeviews: want viewpoint changeshadows: want animation memoryMany thanks Alboukadel Kassambara allowing us use tutorial section.","code":""},{"path":"animation.html","id":"set-up","chapter":"Animation","heading":"Set Up","text":"Load required packages set default ggplot2 theme theme_bw():","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(gganimate)\ntheme_set(theme_bw())\nhead(gapminder)## # A tibble: 6 x 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786."},{"path":"animation.html","id":"transition-through-distinct-states-in-time","chapter":"Animation","heading":"Transition through distinct states in time","text":"Begin static plot:","code":"\np <- ggplot(gapminder,\n            aes(x = gdpPercap, y=lifeExp, size = pop, colour = country)) +\n      geom_point(show.legend = FALSE, alpha = 0.7) +\n      scale_color_viridis_d() +\n      scale_size(range = c(2, 12)) +\n      scale_x_log10() +\n      labs(x = \"GDP per capita\", y = \"Life expectancy\")\np"},{"path":"animation.html","id":"basics","chapter":"Animation","heading":"Basics","text":"Key R function: transition_time(). transition length states set correspond actual time difference .Label variables: frame_time. Gives time current frame corresponds .","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"},{"path":"animation.html","id":"create-facets-by-continent","chapter":"Animation","heading":"12.0.1 Create facets by continent","text":"","code":"\np + facet_wrap(~continent) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"},{"path":"animation.html","id":"let-the-view-follow-the-data-in-each-frame","chapter":"Animation","heading":"12.0.2 Let the view follow the data in each frame","text":"","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  view_follow(fixed_y = TRUE)"},{"path":"animation.html","id":"show-preceding-frames-with-gradual-falloff","chapter":"Animation","heading":"12.0.3 Show preceding frames with gradual falloff","text":"shadow meant draw small wake data showing latest frames current. can choose gradually diminish size /opacity shadow. length wake given absolute frames make animation susceptible changes framerate. Instead given proportion total length animation.","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_wake(wake_length = 0.1, alpha = FALSE)"},{"path":"animation.html","id":"show-the-original-data-as-background-marks","chapter":"Animation","heading":"12.0.4 Show the original data as background marks","text":"shadow lets show raw data behind current frame. past /future raw data can shown styled want.","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_mark(alpha = 0.3, size = 0.5)"},{"path":"animation.html","id":"reveal-data-along-a-given-dimension","chapter":"Animation","heading":"Reveal data along a given dimension","text":"transition allows let data gradually appear, based given time dimension. Start static plot:","code":"\np <- ggplot(airquality,\n            aes(Day, Temp, group = Month, color = factor(Month))) +\n      geom_line() +\n      scale_color_viridis_d() +\n      labs(x = \"Day of Month\", y = \"Temperature\") +\n      theme(legend.position = \"top\")\np"},{"path":"animation.html","id":"let-data-gradually-appear","chapter":"Animation","heading":"12.0.5 Let data gradually appear","text":"Reveal day (x-axis)Show points:Points can kept giving unique group:","code":"\np + transition_reveal(Day)\np + \n  geom_point() +\n  transition_reveal(Day)\np + \n  geom_point(aes(group = seq_along(Day))) +\n  transition_reveal(Day)"},{"path":"animation.html","id":"transition-between-several-distinct-stages-of-the-data","chapter":"Animation","heading":"Transition between several distinct stages of the data","text":"Data preparation:Create bar plot mean temperature:transition_states():enter_grow() + enter_fade()","code":"\nmean.temp <- airquality %>%\n  group_by(Month) %>%\n  summarise(Temp = mean(Temp), .groups = \"drop_last\")\nmean.temp## # A tibble: 5 x 2\n##   Month  Temp\n##   <int> <dbl>\n## 1     5  65.5\n## 2     6  79.1\n## 3     7  83.9\n## 4     8  84.0\n## 5     9  76.9\np <- ggplot(mean.temp, \n            aes(Month, Temp, fill = Temp)) +\n      geom_col() +\n      scale_fill_distiller(palette = \"Reds\", direction = 1) +\n      theme_minimal() +\n      theme(panel.grid = element_blank(),\n            panel.grid.major.y = element_line(color = \"white\"),\n            panel.ontop = TRUE)\np\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark()\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark() +\n  enter_grow() +\n  enter_fade()"},{"path":"animation.html","id":"save-your-animation","chapter":"Animation","heading":"Save your animation","text":"code create animations can take long time run. created animation, ’ll want save somewhere can display without run code.key function use anim_save(), similar saving static plots using ggsave(). save animation gif. first argument filename want give animation second animation object, animation object called p wanted save file called “p.gif,” save like :don’t supply second argument, anim_save() default saving recent animation rendered. anim_save(\"animation.gif\") save recent animation “animation.gif.”don’t want save gif current directory, can specify directory using path argument. Let’s say subdirectory working directory called “gifs.” can thus save “animation.gif” “gifs” anim_save(\"animation.gif\", path = \"gifs\").created gif, can post online. can post Facebook selecting “Photo/Video” Facebook status Twitter clicking photo icon.","code":"\nanim_save(\"p.gif\", p)"},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
